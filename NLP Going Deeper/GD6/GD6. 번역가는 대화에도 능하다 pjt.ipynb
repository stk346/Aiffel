{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "engaging-fairy",
   "metadata": {},
   "source": [
    "## 번역 모델 만들기\n",
    "실습에선 접근성이 좋은 영어-스페인어 데이터를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "imported-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "martial-jewelry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-level",
   "metadata": {},
   "source": [
    "두 언어가 단어 사전을 공유하도록 하겠습니다. 영어와 스페인어 모두 알파벳으로 이뤄지는 데다가 같은 인도유럽어족이기 때문에 기대할 수 있는 효과가 많아요! 후에 챗봇을 만들 때에도 질문과 답변이 모두 한글로 이루어져 있기 때문에 Embedding 층을 공유하는 것이 성능에 도움이 됩니다.  \n",
    "  \n",
    "토큰화에는 Sentencepiece를 사용할 것이고 단어 사전 수는 20,000으로 설정하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-setting",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "중복데이터를 set 데이터형을 활용해 제거한 후 Sentencepiece 기반의 토크나이저를 생성해 주는 gernerate_tokenizer() 함수를 정의하여 토크나이저를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "apparent-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "colonial-airport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus)) \n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-handy",
   "metadata": {},
   "source": [
    "위에서 두 언어 사이에 단어 사전을 공유하기로 하였으므로, 따라서 Encoder와 Decoder의 전용 토크나이저를 만들지 않고, 방금 만들어진 토크나이저를 두 언어 사이에서 공유하게 됩니다. \n",
    "  \n",
    "토크나이저가 준비되었으니 본격적으로 데이터를 토큰화하도록 하겠습니다. 문장부호와 대소문자 등을 정제하는 preprocess_sentence() 함수를 정의해 데이터를 정제하고, 정제된 데이터가 50개 이상의 토큰을 갖는 경우 제거하도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "intermediate-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bound-prophet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d368df4ea724bfa84fa6250ad040977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))   # encode_ad_ids() 는 문자열을 숫자로 분할합니다.\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-cincinnati",
   "metadata": {},
   "source": [
    "pad_sequences()를 이용해 패딩을 진행하겠습니다. 데이터의 1%는 테스트 셋으로 빼놓겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "metric-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-smith",
   "metadata": {},
   "source": [
    "### 트랜스포머 구현하기\n",
    "Encoder와 Decoder 각각의 Embedding과 출력층의 Linear, 총 3개의 레이어가 Weight를 공유할 수 있게 하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-exploration",
   "metadata": {},
   "source": [
    "**Positional Encoding**  \n",
    "$PE(pos, 2i) = Sin(pos/10000^{\\frac{2i}{d_{model}}})$  \n",
    "$PE(pos, 2i+1) = Cos(pos/10000^{\\frac{2i+1}{d_{model}}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "handled-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-density",
   "metadata": {},
   "source": [
    "**Generate Padding Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "stable-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-difficulty",
   "metadata": {},
   "source": [
    "**Multi-head Attention**\n",
    "$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adjacent-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-version",
   "metadata": {},
   "source": [
    "**Position-wise Feed Forward Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sound-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-stock",
   "metadata": {},
   "source": [
    "**Encoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "unavailable-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-reliance",
   "metadata": {},
   "source": [
    "**Decoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "narrative-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-share",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "generic-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-session",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecological-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-prior",
   "metadata": {},
   "source": [
    "**Transformer 전체 모델 조립**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "studied-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-swing",
   "metadata": {},
   "source": [
    "**모델 인스턴스 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "higher-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-breach",
   "metadata": {},
   "source": [
    "모델을 만들었으니 학습을 시켜봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-homeless",
   "metadata": {},
   "source": [
    "**Learning Rate Scheduler**  \n",
    "트랜스포머 학습률(learning rate)은 고정된 값을 유지하는 것이 아니라 학습 경과에 따라 변하도록 설게하였습니다. 공식은 아래와 같습니다.  \n",
    "$lrate = d_{model}^{-0.5}*min(step\\_num^{-0.5}, step\\_num * warmup\\_steps^{-1.5})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "shaped-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-stand",
   "metadata": {},
   "source": [
    "**Learning Rate & Optimizer 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "sharp-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-grave",
   "metadata": {},
   "source": [
    "**Loss Function 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cordless-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-plymouth",
   "metadata": {},
   "source": [
    "**Train Step 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "literary-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-commerce",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "outside-genre",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71d7ff09249433587d913a7e5fc3aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffcdaa3d61f426994396b751e281fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d673a3e4743441ab93e9aa2eb3fac34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-murder",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (1) BLEU Score\n",
    "라이브러리를 활용해서 간단하게 BLEU Score를 실습해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-branch",
   "metadata": {},
   "source": [
    "### NLTK를 활용한 BLEU Score\n",
    "NLTK는 Natural Language Tool Kit 의 준말로 이름부터 자연어 처리에 큰 도움이 될 것 같은 라이브러리입니다. BLEU Score를 지원하니 이를 활용하도록 합시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "patent-pension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-commodity",
   "metadata": {},
   "source": [
    "BLEU Score는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많습니다.  \n",
    "  \n",
    "BLEU Score가 50점을 넘는다는 것은 정말 멋진 번역을 생성했다는 의미예요. 보통 논문에서 제시하는 BLEU Score는 20점에서 높으면 40점을 바라보는 정도거든요! 하지만 방금 나온 점수는 사실상 0점이라고 해야 하겠네요. 그렇게까지 엉망진창인 번역이 된 것일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-consultancy",
   "metadata": {},
   "source": [
    "BLEU Score의 정의로 돌아가 한번 따져봅시다. BLEU Score가 N-gram으로 점수를 측정한다는 것을 기억하실 거예요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-maker",
   "metadata": {},
   "source": [
    "$(\\prod_{i=1}^{4}precision_i)^\\frac{1}{4} = ($1-gram x 2-gram x 3-gram x 4-gram$)^\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-bolivia",
   "metadata": {},
   "source": [
    "1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면$(\\frac{1}{4})$ BLEU Score가 된답니다. 진정 멋진 번역이라면, 모든 N-gram에 대해서 높은 점수를 얻었을 거예요. 그렇다면 위에서 살펴본 예시에서는 각 N-gram이 점수를 얼마나 얻었는지 확인해 보도록 합시다. weights의 디폴트 값은 [0.25, 0.25, 0.25, 0.25]로 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 주는 것이지만, 만약 이 값을 [1, 0, 0, 0]으로 바꿔주면 BLEU Score에 1-gram의 점수만 반영하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "sunset-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-interim",
   "metadata": {},
   "source": [
    "0점에 가까운 BLEU Score가 나오는 원인을 알 수 있겠네요. 바로 3-gram와 4-gram에서 거의 0점을 받았기 때문인데요, 위 예시에서 번역문 문장 중 어느 3-gram도 원문의 3-gram과 일치하는 것이 없기 때문입니다. 2-gram이 0.18이 나오는 것은 원문의 11개 2-gram 중에 2개만이 번역문에서 재현되었기 때문입니다. (를 선호) (선호 한다)  \n",
    "  \n",
    "하지만 만약 nltk의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.  \n",
    "  \n",
    " 예전 버전에서는 위 수식에서 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸해버리기 때문에 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현되어 있었습니다. 하지만 1.0 은 모든 번역을 완벽히 재현했음을 의미하기 때문에 총점이 의도치 않게 높아질 수 있어요! 그럴 경우에는 BLEU Score가 바람직하지 못할 것(Undesirable)이라는 경고문이 추가되긴 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-rates",
   "metadata": {},
   "source": [
    "### SmoothingFunction()으로 BLEU Score 보정하기\n",
    "그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다. Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요.  \n",
    "  \n",
    "진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해 봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dressed-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1) # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-mediterranean",
   "metadata": {},
   "source": [
    "SmoothingFunction()로 BLEU score를 보정한 결과, 새로운 BLEU 점수는 무려, 5점으로 올라갔습니다. 거의 의미 없는 번역이라는 냉정한 평가를 받게 되는군요.  \n",
    "  \n",
    "여기서 BLEU-4가 BLEU-3보다 조금이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해 보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-harrison",
   "metadata": {},
   "source": [
    "### 트랜스포머 모델의 번역 성능 알아보기\n",
    "위 예시를 조금만 응용하면 우리가 훈련한 모델이 얼마나 번역을 잘하는지 평가할 수 있습니다! 아까 1%의 데이터를 테스트셋으로 빼 둔 것을 기억하시죠? 테스트셋으로 모델의 BLEU Score를 측정하는 함수 eval_bleu() 를 구현해보도록 합시다!  \n",
    "  \n",
    "먼저 번역을 생성하기 위해 evaluate() 함수와 translate() 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "otherwise-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence) # 문자열을 token으로 분할합니다.\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence) # 문자열을 숫자로 분할합니다.\n",
    "    \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                    maxlen=enc_train.shape[-1], padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장합니다.\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids) # 숫자를 문자열로 복원합니다.\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "        result = tgt_tokenizer.decode_ids(ids)\n",
    "        return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "unlike-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-james",
   "metadata": {},
   "source": [
    "이제 함수 eval_bleu() 를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bright-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score=0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "    \n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "        \n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                        smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Source Sentence:\", src_sentence)\n",
    "            print(\"Model Prediction:\", candidate)\n",
    "            print(\"Real:\", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "            \n",
    "        print(\"Num of Sample:\", sample_size)\n",
    "        print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-geology",
   "metadata": {},
   "source": [
    "주어진 병렬 말뭉치 src_corpus 와 tgt_corpus 를 인덱스 순으로 살피며 소스 토큰과 타겟 토큰을 각각 원문으로 Decoding 하고, 소스 문장을 translate() 함수를 통해 번역한 후 생성된 번역문과 타겟 문장의 BLEU Score를 측정합니다. 측정된 score 는 total_score 에 합산되어 최종적으로 주어진 병렬 말뭉치의 평균 BLEU Score를 출력하죠!  \n",
    "\n",
    "verbose 변수를 True 로 주면 번역문과 원문, 매 스텝의 점수를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "frozen-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3878d017d91449be981b91016a1ec18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: tom just shrugged .........................................\n",
      "Model Prediction: []\n",
      "Real: ['tom', 'solo', 'se', 'encogi', 'de', 'hombros', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n",
      "Source Sentence: this is the most beautiful sight that i have ever seen ..................................\n",
      "Model Prediction: []\n",
      "Real: ['este', 'es', 'el', 'paisaje', 'que', 'he', 'visto', 'hasta', 'ahora', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n",
      "Source Sentence: i was fond of cake ........................................\n",
      "Model Prediction: []\n",
      "Real: ['me', 'gustaba', 'mucho', 'la', 'tarta', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-toilet",
   "metadata": {},
   "source": [
    "3 Epoch밖에 학습하지 않아서인지 성능이 좋지 않네요. 표본이 적은 것일 수도 있으니 좀 더 많은 데이터로 측정해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "native-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e515f90b4b742bbae145ebf84ead279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n",
      "Num of Sample: 119\n",
      "Total Score: 4.6909707642663925e-20\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-dylan",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (2) Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ethical-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]] # 생성된 문장과 점수를 저장\n",
    "    \n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "                \n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "convinced-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for word in seq:\n",
    "        sentence += vocab[word]+ \" \"\n",
    "        \n",
    "    print(sentence, \"//Score: %.4f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "incorrect-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 6, 4, 9, 5, 1, 0, 0, 0, 0], 42.52430434762729],\n",
       " [[2, 6, 3, 9, 5, 1, 0, 0, 0, 0], 28.013461367181293],\n",
       " [[3, 6, 4, 9, 5, 1, 0, 0, 0, 0], 17.89828068648655]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-apollo",
   "metadata": {},
   "source": [
    "사실 이 예시는 Beam Search를 설명하는 데에는 더없이 적당하지만 실제로 모델이 문장을 생성하는 과정과는 거리가 멉니다. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 prob_seq 처럼 확률을 정의할 수 없겠다는 생각이 머리를 스치죠. 각 단어에 대한 확률은 prob_seq 처럼 한 번에 정의가 되지 않고 이전 스텝까지의 단어에 따라서 결정되기 때문입니다!  \n",
    "  \n",
    "간단한 예시로, Beam Size가 2이고 Time-step이 2인 순간의 두 문장이 나는 밥을 , 나는 커피를 이라고 한다면 세 번째 단어로 먹는다 , 마신다 를 고려할 수 있습니다. 이때, 전자에서 마신다 에 할당하는 확률과 후자에서 마신다 에 할당하는 확률은 각각 이전 단어들인 나는 밥을 , 나는 커피를 에 따라서 결정되기 때문에 서로 독립적인 확률을 갖습니다. 예컨대 후자가 마신다 에 더 높은 확률을 할당할 것을 알 수 있죠! 위 소스에서처럼 \"3번째 단어는 항상 [마신다: 0.3, 먹는다:0.5, ...] 의 확률을 가진다!\" 라고는 할 수 없다는 겁니다.  \n",
    "  \n",
    "따라서 Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 합니다. Beam Size가 5라고 가정하면 맨 첫 단어로 적합한 5개의 단어를 생성하고, 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률)를 가지고 있으니 각각의 순위를 매길 수 있겠죠? 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.  \n",
    "  \n",
    "위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-launch",
   "metadata": {},
   "source": [
    "### Beam Search Decoder 작성 및 평가하기\n",
    "각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "mounted-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "    \n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    model(src_ids, tgt_ids, enc_padding_mask, conbined_mask, dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
