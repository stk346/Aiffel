{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f980b26b",
   "metadata": {
    "id": "6Hgw3GTXLLw0"
   },
   "source": [
    "## ü§ó Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32de77",
   "metadata": {
    "id": "W7-QHmRiAMB9"
   },
   "source": [
    "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
    "\n",
    "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
    "\n",
    "We will leverage ü§ó`Seq2SeqTrainer`, gradient checkpointing and as usual ü§ó`datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd59a64",
   "metadata": {
    "id": "0B19PhgrCHM1"
   },
   "source": [
    "First, let's try to get a GPU with at least 15GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661029",
   "metadata": {
    "id": "6z18zG2N192Z"
   },
   "source": [
    "To check that we are having enough RAM we can run the following command.\n",
    "If the randomely allocated GPU is too small, the above cells can be run \n",
    "to crash the notebook hoping to get a better GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0b19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ded7d85",
   "metadata": {
    "id": "P59lSzY4192Z"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install datasets==1.2.1\n",
    "# !pip install transformers==4.2.0\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a533c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface datasets load\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_train_df/\")\n",
    "val_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_val_df/\")\n",
    "test_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_test_df/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1583374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': ' from outstanding performance in 2011 12, burberry began the year cautiously optimistic , our long-range objectives ensuring clarity of the luxury brand message , enabling sustainable growth and being a great company firmly in sight . angela ahrendts chief executive officer this combination of optimism and determination , fuelled by the brand‚Äôs wealth of opportunity , suggested continued pursuit of the investment-oriented strategic agenda in the year ahead . at the same time , this pre-disposition was tempered by uncertainties in the macro environment and the goal to deliver near-term financial performance . in the final analysis , the result was a balance of dynamic management , core execution and strategic investment . challenging context following standout growth in 2011 relative to the range of consumer sectors , luxury slowed dramatically in 2012. the ongoing economic crisis in the eurozone and a continued sluggish us weighed on all areas of consumer spending . although most of asia remained relatively healthy , the chinese consumer which accounts for a majority of luxury consumption growth was subdued by a secularly decelerating economy complicated by government transition . industry experts estimate that luxury sector growth declined from 13 percent in 2011 to 5 percent in 2012. within that , ready-to-wear brands and businesses were disproportionately affected . for burberry , this climate manifested itself in a decline in store traffic and greater weekly sales volatility . dynamic management internally , we talk about managing the business dynamically and focusing on the things we can control . 2012 13‚Äôs external environment tested the team on these dimensions . supported by investments in information systems and business intelligence expertise during the past few years which enable monitoring , analysis and implementation we set out to do exactly that . heightened conversion the traffic decline placed greater emphasis on converting consumers entering the stores to customers . this effort included service initiatives that maximised time on the selling floor of our most skilled associates , increased inventory availability and improved selling skills and product knowledge . in terms of product , we expedited fashion assortments targeted at core luxury customers and refined monthly floorset execution to enhance the flow of fresh merchandise . 8 chief executive officer‚Äôs letter striking the balance targeted marketing aided by deeper consumer insight , we retargeted marketing activities in keeping with changing consumer spending patterns . the team increased the brand‚Äôs presence in high-profile outdoor and travel-oriented locations and experimented with new digital venues some of which were contracted on a real-time basis . for festive periods , a cross-functional group refined programmes across product , visual and advertising to better highlight specific gift-giving opportunities . tactical efficiency the group also acted to enhance near-term efficiency . discretionary expenses were tightly controlled and inventory was closely managed at all stages of the process in keeping with softer sales . core execution articulated by the five strategic themes , burberry‚Äôs core strategy has been consistently executed over the past seven years , and actions to navigate immediate conditions did not sway us from this course . blurring the physical and digital given a world of increasingly ubiquitous mobile internet access , we expect dissolution of the boundaries separating physical and digital channels . consumers will see a single , continuous space in which to interact with a brand . through a range of activities , we are working to integrate the benefits of the physical and digital spheres . ultimately , the vision is to serve completely any consumer on any platform in any geography . in this regard , the opening of the london flagship at 121 regent street in september is our most ambitious effort to date . housed in a period building restored in partnership with traditional british craftsmen , the store expresses burberry .com in tangible space complete product assortment , rfid technology to trigger targeted multimedia content , omnipresent digital screens continuously projecting brand imagery . burberry world live , a store of the future , blending heritage and innovation , online with offline . additional integration initiatives included the expanded use of ipads to enhance inventory availability , continued upgrading of retail theatre throughout the store base to ensure synchronised delivery of brand content to consumers , and experimentation with new payment systems to streamline the purchase experience . enhancing the product proposition at the heart of the burberry brand , product was a key area of strategic activity . elevation of the product offering is an ongoing process . in the year , we exited selected opening price points in heritage rainwear and leather goods categories . exacting a cost in terms of sales , this is consistent with the brand‚Äôs positioning within the current luxury context . similarly responding to consumer demand , we continued to invest in the upper tiers of our product pyramid , the prorsum and london labels . these labels increased their share of retail sales during the year . although founded as a mens brand , burberry is underpenetrated in mens . during the year , outerwear benefited from greater emphasis on innovation and design . tailoring developed with broader assortments and expanded distribution . closer attention to in-store timing of seasonal merchandise enhanced relevance of the offering . and the first fully dedicated burberry mens store was opened in london in october 2012. achieving a 13 percent revenue increase , mens was the fastest growing product division in the year . with mens integration only two years old , burberry is in the early stages of capitalising on this heritage . october saw the launch of the britain watch for women and men . the britain , featuring an advanced swiss-made mechanical movement and more sophisticated design , is an important step in realigning burberry‚Äôs watch business with the brand‚Äôs luxury positioning . engaging the chinese consumer globally given their importance , efforts to better understand and serve chinese consumers are an ongoing priority . during the year , burberry conducted proprietary research , leveraging the results across functions to extend product sizing and fit , to train sales associates in high-travel markets , to formulate occasion-specific marketing campaigns citing a few examples . in the year , greater china represented burberry‚Äôs fastest growing major market and this consumer was prominent throughout the retail network . 9 chief executive officer‚Äôs letter developing growth markets outside asia , the group continued to develop other growth markets . in the directly operated markets of india , latin america and the middle east , the group opened net six mainline stores during the year . while some offer below average profitability today , we believe these markets represent important components of future growth . in regions operated through franchise partners , including turkey , russia and eastern europe , eight stores were opened with expansion to five new markets , including georgia and jordan . new franchise agreements were signed for colombia and chile . expanding the retail presence in addition to london , the year included flagship store openings in chicago , hong kong and milan . while contributing to sales , these brand statements in gateway cities present the complete burberry to diverse groups of relevant consumers , many of them new to the brand . in total , the group opened net 14 mainline stores , six concessions and five outlets during the year , and completed seven major renovations . average selling space increased 13 percent . refining the wholesale presence efforts to align the quality of the brand‚Äôs wholesale presence with that of retail are ongoing . in both the americas and europe , we continued to concentrate on luxury-oriented department and specialty stores with emphasis on dedicated real estate while exiting legacy doors inconsistent with the brand‚Äôs positioning . this activity , in combination with the channel‚Äôs exposure to soft geographies , resulted in 1 percent underlying wholesale revenue growth for the year . as part of the brand proposition , burberry looks to be a leader in consumers‚Äô digital interaction with brands , in both innovation and capability . in marketing , the s s13 campaign generated record awareness through social media . total burberry youtube video views reached over ten million during the year . rfid-enabled personalised content was introduced with the a w13 runway show . experimenting with emerging digital platforms , burberry streamed live images of london weather to prominent outdoor sites in london , paris , hong kong , los angeles and new york during the olympic period . in commerce , burberry .com added spanish and korean languages and tested new fulfilment options . strategic investment while executing currently , the team invested in strategic initiatives with longer-term horizons . integrating beauty among the most exciting strategic investments in recent years , the transition of burberry‚Äôs fragrance and make-up business from a licensed to a directly operated business began in the year . offering luxury‚Äôs opening price point and broadest distribution , fragrance is the most widely encountered expression of the burberry brand . the category also accounts for a large percentage of global brand media spend . as a result , direct operation will assist in optimising brand presence in every market , further enable burberry to capitalise on the synergistic relationship with fashion , and better align the product offering with brand architecture . integration elevates this business to true core activity , allowing burberry to capture the full opportunity . in terms of opportunity , despite burberry‚Äôs position among the largest luxury apparel and accessories brands globally , it is undersized in fragrance . growth has been slow , with fragrance significantly underperforming the rest of the group over the past five years . in make-up , the brand has only just started . from the decision to integrate in october , the team moved quickly across functions , leveraging existing skills and resources while adding external category-specific talent and capability . as of 1 april , beauty had been successfully integrated and commenced operating . burberry‚Äôs fifth product division , beauty is a growth platform of the future . evolving customer dialogue in a landscape of big data and continuous communication , we believe information-intensive , deep customer relationships which allow an individualised customer dialogue will be critical to future success in luxury . as part of this , the group began developing new tools to provide an integrated view of a customer‚Äôs interaction with the brand across all burberry platforms , with initial piloting of a clienteling application commencing at year end . development work to enhance consumers‚Äô ability to engage the brand through mobile devices also progressed in the year . 10 chief executive officer‚Äôs letter transitioning the japan legacy in japan , transition from the legacy licensed business to global integration continued . as part of this , burberry‚Äôs early stage retail operation achieved strong growth at existing stores and concessions , opened a concession , added a third store and planned additional openings in 2013 14 while the effect of licence terminations continued to reduce legacy royalty income . reinforcing the supply chain to accommodate future growth objectives , the group reinforced the supply chain . in logistics , burberry added distribution capacity , upgraded existing facilities and increased network efficiency . in sourcing , additional resources were committed to further develop in-house outerwear manufacturing capability and improve raw material management . strong financial results this balance of activity delivered record financial results in 2012 13 while positioning burberry well for years ahead . total revenue increased 8 percent underlying to pound 2bn . retail revenue grew 12 percent driven by new space and a 5 percent comparable store gain . soft european markets particularly weighed on wholesale , resulting in a 1 percent underlying revenue increase . the 1 percent underlying decline in licensing revenue was a product of double-digit growth among global licences more than offset by declining legacy japan royalties . adjusted operating profit increased 14 percent to pound 428m , with the core retail wholesale segment increasing 17 percent on 8 percent revenue growth retail wholesale operating margin also reached a record 17.8 percent . capital expenditure totalled pound 176m and the group ended the year with pound 297m in net cash . powerful culture burberry‚Äôs culture is a key ingredient to this success . rooted in the brand‚Äôs core values and fuelled by a creative-thinking , entrepreneurial spirit , our connected , united culture creates an energy that enables innovation , coordination and agility . these characteristics are evident in the year‚Äôs accomplishments . this distinctive culture is also expressed externally through ethical trade and sustainability efforts , employee engagement with local communities and the burberry foundation , which contributes both human and capital resources to encourage youth to realise their dreams through the power of creativity . we continued to invest in this powerful culture throughout the organisation communication initiatives , operating structures , reward programmes and celebrations . the bigger the business becomes , the more connected we will need to be . a great community through the efforts of this great team we largely realised our objectives . so i thank them , as well as burberry‚Äôs extended community of franchise and licensing partners , customers and suppliers , for their passion , commitment and hard work during the year . looking forward , this team provides confidence for markets favourable or not . kpi growth in adjusted diluted eps year to 31 march is a key valuation metric for burberry‚Äôs shareholders . 70.0p 14 percent 2013 2012 2011 2010 2009 70.0 14 percent 61.6 26 percent 48.9 39 percent 35.1 16 percent 30.2 -4 percent adjusted diluted eps is stated before exceptional items . reported diluted eps 57.0p 2012 59.3p . 11 chief executive officer‚Äôs letter ',\n",
       " 'abstract': 'from outstanding performance in 2011 12, burberry began the year cautiously optimistic , our long-range objectives ensuring clarity of the luxury brand message , enabling sustainable growth and being a great company firmly in sight .angela ahrendts chief executive officer this combination of optimism and determination , fuelled by the brand‚Äôs wealth of opportunity , suggested continued pursuit of the investment-oriented strategic agenda in the year ahead .at the same time , this pre-disposition was tempered by uncertainties in the macro environment and the goal to deliver near-term financial performance .in the final analysis , the result was a balance of dynamic management , core execution and strategic investment .challenging context following standout growth in 2011 relative to the range of consumer sectors , luxury slowed dramatically in 2012. the ongoing economic crisis in the eurozone and a continued sluggish us weighed on all areas of consumer spending .although most of asia remained relatively healthy , the chinese consumer which accounts for a majority of luxury consumption growth was subdued by a secularly decelerating economy complicated by government transition .industry experts estimate that luxury sector growth declined from 13 percent in 2011 to 5 percent in 2012. within that , ready-to-wear brands and businesses were disproportionately affected .additional integration initiatives included the expanded use of ipads to enhance inventory availability , continued upgrading of retail theatre throughout the store base to ensure synchronised delivery of brand content to consumers , and experimentation with new payment systems to streamline the purchase experience .and the first fully dedicated burberry mens store was opened in london in october 2012. achieving a 13 percent revenue increase , mens was the fastest growing product division in the year .strategic investment while executing currently , the team invested in strategic initiatives with longer-term horizons .soft european markets particularly weighed on wholesale , resulting in a 1 percent underlying revenue increase .capital expenditure totalled pound 176m and the group ended the year with pound 297m in net cash .',\n",
       " '__index_level_0__': 1592}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe607ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2445\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2e40",
   "metadata": {
    "id": "APGpqABk192Z"
   },
   "source": [
    "Next, we install ü§óTransformers, ü§óDatasets, and `rouge_score`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfb3f8",
   "metadata": {
    "id": "p3Me6n0o192Z"
   },
   "source": [
    "Let's start by loading and preprocessing the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b551ed",
   "metadata": {
    "id": "CzjKRBib192Z"
   },
   "source": [
    "Next, we download the pubmed train and validation dataset ([click to see on ü§óDatasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **‚òï** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5b249",
   "metadata": {
    "id": "Uyn-TDeB192a"
   },
   "source": [
    "It's always a good idea to take a look at some data samples. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febc7268",
   "metadata": {
    "id": "5QkyLfIy192a"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=4):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4813ec",
   "metadata": {
    "id": "wx3iiUgy192a"
   },
   "source": [
    "We can see that the input data is the `article` - a scientific report and the target data is the `abstract` - a concise summary of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69f001",
   "metadata": {
    "id": "xs7zKqy-192a"
   },
   "source": [
    "Cool! Having downloaded the dataset, let's tokenize it.\n",
    "We'll import the convenient `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc06d3",
   "metadata": {
    "id": "Ifg19ED9192a"
   },
   "source": [
    "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6514b0e",
   "metadata": {
    "id": "w_s6Z_ni192a"
   },
   "source": [
    "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.${}^1$. \n",
    "\n",
    "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
    "\n",
    "In this notebook, we are only able to train on `batch_size=2` to prevent out-of-memory errors.\n",
    "\n",
    "---\n",
    "${}^1$ The data is taken from page 11 of [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c0360d",
   "metadata": {
    "id": "jk7tS_xN192b"
   },
   "outputs": [],
   "source": [
    "max_input_length = 4096\n",
    "max_output_length = 512\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fd76",
   "metadata": {
    "id": "2493L-dt192b"
   },
   "source": [
    "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
    "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
    "\n",
    "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b28121",
   "metadata": {
    "id": "2_UzG6Ek192b"
   },
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8da054",
   "metadata": {
    "id": "q-V7N0L-192b"
   },
   "source": [
    "For the sake of this notebook, we will reduce the training and validation data \n",
    "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b172c6",
   "metadata": {
    "id": "OeJ7dOpi192b"
   },
   "source": [
    "Great, having defined the mapping function, let's preprocess the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9c5b8",
   "metadata": {
    "id": "WSUZxqMX192d"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1a6168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  9 10:38:29 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2643f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2871ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff5f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb969c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµÏùÑ ÏßÑÌñâÌïòÎäî Í∏∞Í∏∞: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('ÌïôÏäµÏùÑ ÏßÑÌñâÌïòÎäî Í∏∞Í∏∞:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ecf870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "print(torch.__version__) # torch version Ï∂úÎ†•\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efbab5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be5d18da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "msB7BC1d192d",
    "outputId": "06060f81-6742-4124-9f4c-d8900a1cf808",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb76c80a3a4462498ae46600251549d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2445.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d27d9dc471405ea019812867cebb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2445\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 915\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstk346\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/stk346/huggingface/runs/18j7cpyg\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/stk346/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 1:07:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=915, training_loss=0.5458936263954705, metrics={'train_runtime': 4036.8547, 'train_samples_per_second': 1.817, 'train_steps_per_second': 0.227, 'total_flos': 1.979250143920128e+16, 'train_loss': 0.5458936263954705, 'epoch': 3.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# load pubmed\n",
    "# pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n",
    "# pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n",
    "pubmed_train = train_dataset\n",
    "pubmed_val = val_dataset\n",
    "\n",
    "# comment out following lines for a test run\n",
    "# pubmed_train = pubmed_train.select(range(32))\n",
    "# pubmed_val = pubmed_val.select(range(32))\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "\n",
    "# max encoder length is 8192 for PubMed\n",
    "encoder_max_length = 4096\n",
    "decoder_max_length = 512\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=decoder_max_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# map train data\n",
    "pubmed_train = pubmed_train.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    ")\n",
    "\n",
    "# map val data\n",
    "pubmed_val = pubmed_val.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    ")\n",
    "\n",
    "# set Python list to PyTorch tensor\n",
    "pubmed_train.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# set Python list to PyTorch tensor\n",
    "pubmed_val.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# enable fp16 apex training  ## ## name 'amp' is not defined Î¨∏Ï†úÎ°ú Ï£ºÏÑùÏ≤òÎ¶¨\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=False, ## default = True\n",
    "    fp16_backend=\"auto\", ## default = apex\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=250,\n",
    "    eval_steps=5000,\n",
    "    save_steps=500,\n",
    "    warmup_steps=1500,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "\n",
    "# compute Rouge score during validation\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# load model + enable gradient checkpointing & disable cache for checkpointing\n",
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# set generate hyperparameters\n",
    "led.config.num_beams = 4\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3\n",
    "\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=led,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args, ## optional\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=pubmed_train,\n",
    "    eval_dataset=pubmed_val,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a057d4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84381dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2167\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "507c1950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'global_attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 280\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdbc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_dataset.select(range(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244cf104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abstract', 'article'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91c7cfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/transformers/generation_utils.py:1632: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b323850092da4ba1b0eedf28bccde97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: Score(precision=0.39738487689689733, recall=0.3347771925156876, fmeasure=0.3585959433230117)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "# load pubmed\n",
    "pubmed_test = test_df\n",
    "\n",
    "# load tokenizer\n",
    "model_path = \"/home/aiffelsummabot/LED/checkpoint-500/\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_path)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_path).to(\"cuda\").half()\n",
    "\n",
    "\n",
    "def generate_answer(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    # put global attention on <s> token\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "\n",
    "result = pubmed_test.map(generate_answer, batched=True, batch_size=4)\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2742c980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>article</th>\n",
       "      <th>predicted_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25695 19 march 2018 3 29 pm proof 7 02 s .c .h...</td>\n",
       "      <td>25695 19 march 2018 3 29 pm proof 7 02 s . c ...</td>\n",
       "      <td>25695 19 march 2018 3 29 pm proof 7 02 s.c.har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strategic report chief executive‚Äôs statement 1...</td>\n",
       "      <td>strategic report chief executive‚Äôs statement ...</td>\n",
       "      <td>strategic report chief executive‚Äôs statement 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary our dedication to providing our client...</td>\n",
       "      <td>summary our dedication to providing our clien...</td>\n",
       "      <td>summary our dedication to providing our client...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q a with ceo , david miles 92 percent of tenan...</td>\n",
       "      <td>q a with ceo , david miles 92 percent of tena...</td>\n",
       "      <td>q a with ceo, david miles 92 percent of tenant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the spring , we launched our walk in wins‚Äô ...</td>\n",
       "      <td>strategic report domino‚Äôs pizza group plc ann...</td>\n",
       "      <td>strategy across all of our markets remains sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strategic report q a with interim group chief ...</td>\n",
       "      <td>strategic report q a with interim group chief...</td>\n",
       "      <td>the b ga market is looking positive.it showed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>having reduced investment in the national acci...</td>\n",
       "      <td>18 nahl group plc annual report and accounts ...</td>\n",
       "      <td>18 nahl group plc annual report and accounts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>importantly , we are pleased to see that our i...</td>\n",
       "      <td>2017 has been another strong year for taylor ...</td>\n",
       "      <td>2017 has been another strong year for taylor w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  25695 19 march 2018 3 29 pm proof 7 02 s .c .h...   \n",
       "1  strategic report chief executive‚Äôs statement 1...   \n",
       "2  summary our dedication to providing our client...   \n",
       "3  q a with ceo , david miles 92 percent of tenan...   \n",
       "4  in the spring , we launched our walk in wins‚Äô ...   \n",
       "5  strategic report q a with interim group chief ...   \n",
       "6  having reduced investment in the national acci...   \n",
       "7  importantly , we are pleased to see that our i...   \n",
       "\n",
       "                                             article  \\\n",
       "0   25695 19 march 2018 3 29 pm proof 7 02 s . c ...   \n",
       "1   strategic report chief executive‚Äôs statement ...   \n",
       "2   summary our dedication to providing our clien...   \n",
       "3   q a with ceo , david miles 92 percent of tena...   \n",
       "4   strategic report domino‚Äôs pizza group plc ann...   \n",
       "5   strategic report q a with interim group chief...   \n",
       "6   18 nahl group plc annual report and accounts ...   \n",
       "7   2017 has been another strong year for taylor ...   \n",
       "\n",
       "                                  predicted_abstract  \n",
       "0  25695 19 march 2018 3 29 pm proof 7 02 s.c.har...  \n",
       "1  strategic report chief executive‚Äôs statement 1...  \n",
       "2  summary our dedication to providing our client...  \n",
       "3  q a with ceo, david miles 92 percent of tenant...  \n",
       "4  strategy across all of our markets remains sim...  \n",
       "5  the b ga market is looking positive.it showed ...  \n",
       "6  18 nahl group plc annual report and accounts 2...  \n",
       "7  2017 has been another strong year for taylor w...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pd = pd.DataFrame({'abstract':result['abstract'],\n",
    "                                        'article': result['article'],\n",
    "                                        'predicted_abstract': result['predicted_abstract']})\n",
    "result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "335d8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd.to_csv(\"LED_base_4098_512_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ca3064f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abstract', 'article', 'predicted_abstract'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c2b757e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cef8edfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'strategic report chief executive‚Äôs statement 14 operational review 2017 marked a strong year of growth for alpha , both in revenue and in our investment in sta and infrastructure .during the year , we increased our client numbers by 39 percent , bringing our total number of clients to 310. pleasingly , the fact that our revenue has grown at a higher percentage is a re ection of the larger trades that we are doing and the increasing size of our clients .we will continue to focus on growing our client base by penetrating our existing corporate marketplace in the uk , alongside continued expansion into the institutional marketplace and overseas sectors .europe in particular presents a very exciting area of expansion for us .during the nancial year we have recruited sta for our london o ce who are uent in foreign languages which has enabled us to steadily expand into select european territories .as a result , we successfully onboarded our rst european clients in the second half of the nancial year .further operational progress was made with the expansion of our fx analysis team and through obtaining regulatory approval to sell derivative products .as well as providing additional bene ts to our existing clients , both developments will undoubtedly increase our appeal to prospective clients .as such , we have created a team with a wealth of experience in institutional fx and tailored our technology and service solutions to meet investment managers‚Äô speci c and increasingly complex needs .we believe that our bespoke platform o ers customers a greater insight into their hedging activities than our competitors and has become a key di erentiator in a short space of time , helping us with both customer acquisition and retention , as well as margin protection .of these new recruits , 11 joined the front o ce team to continue the rate of client acquisition in the uk corporate market , as well as penetrating overseas territories from our o ce in london .as always , we expect sta contributions to increase incrementally as individuals develop along the learning curve , and i am pleased that yet again , new starters in fy17 outperformed those in fy16 - a trend i expect to continue due to the compounding e ect of surrounding great people with more great people , alongside our evolving service and infrastructure and the growing reputation of alpha in our target markets .',\n",
       " 'article': ' strategic report chief executive‚Äôs statement 14 operational review 2017 marked a strong year of growth for alpha , both in revenue and in our investment in sta and infrastructure . during the year , we increased our client numbers by 39 percent , bringing our total number of clients to 310. pleasingly , the fact that our revenue has grown at a higher percentage is a re ection of the larger trades that we are doing and the increasing size of our clients . we will continue to focus on growing our client base by penetrating our existing corporate marketplace in the uk , alongside continued expansion into the institutional marketplace and overseas sectors . europe in particular presents a very exciting area of expansion for us . during the nancial year we have recruited sta for our london o ce who are uent in foreign languages which has enabled us to steadily expand into select european territories . as a result , we successfully onboarded our rst european clients in the second half of the nancial year . whilst we are constantly looking for ways to acquire new clients , we remain dedicated to providing the very best levels of service to our existing client base . to ensure this , my co-founder jon currie has recently moved into a role focused on enhancing our service delivery and is working collaboratively with our clients to improve our customer insights and perpetuate our high levels of client retention . further operational progress was made with the expansion of our fx analysis team and through obtaining regulatory approval to sell derivative products . as well as providing additional bene ts to our existing clients , both developments will undoubtedly increase our appeal to prospective clients . aside from penetrating our traditional corporate marketplace , we have targeted the institutional and funds marketplace , which presents a valuable growth opportunity for alpha . as such , we have created a team with a wealth of experience in institutional fx and tailored our technology and service solutions to meet investment managers‚Äô speci c and increasingly complex needs . throughout the course of this year we have also continued to invest in the ongoing development of our technology platform which has proved to be an important element of our advisory service o ering and this will continue to be a focus moving forward . our online currency management platform has been developed in-house and we believe , a ords us a strong chief executive‚Äôs statement everyone at alpha passionately believes our ipo marked the start of our journey , not the end . we took alpha public to avoid the destination mentality‚Äô and now have the opportunity to continue developing a very special business . 248225 alpha fx pp12-pp26.qxp 03 04 2018 15 07 page 14 strategic report chief executive‚Äôs statement 15 competitive advantage . it provides our clients with a tailored and comprehensive overview of their currency exposures and is subsequently allowing them to manage and report on these exposures more e ectively , whilst reducing the time and costs involved in doing so . we believe that our bespoke platform o ers customers a greater insight into their hedging activities than our competitors and has become a key di erentiator in a short space of time , helping us with both customer acquisition and retention , as well as margin protection . in light of this , we made the important decision to migrate to a new cloud application platform in february 2018 which o ers richer functionality , greater customisation and increased scalability . technology is an important cornerstone of the business and we have a strong technology development roadmap in place , and further functionality will be introduced during 2018 to enhance the platform . a re ection of the importance we place on our technology is the fact that our team of developers has grown from 2 at the start of the year to 9. in december , we relocated our head o ce to london to bring all our team under one roof in order to support the unique culture which ultimately underpins our success . we have already seen the bene ts of a single , larger o ce , with wider skill sharing , stronger relationships being forged and a positive atmosphere of camaraderie and collaboration . people when we listed alpha in april , we underestimated how in uential it was going to be in helping us acquire exceptional talent . our plc status has served as a testament to alpha‚Äôs success and future ambitions , enabling us to attract high calibre individuals in a relatively short space of time , and with a noticeable shift in the volume and quality of inbound applications . as planned , total headcount increased from 30 to 51 during the course of the year . of these new recruits , 11 joined the front o ce team to continue the rate of client acquisition in the uk corporate market , as well as penetrating overseas territories from our o ce in london . as always , we expect sta contributions to increase incrementally as individuals develop along the learning curve , and i am pleased that yet again , new starters in fy17 outperformed those in fy16 - a trend i expect to continue due to the compounding e ect of surrounding great people with more great people , alongside our evolving service and infrastructure and the growing reputation of alpha in our target markets . the majority of remaining new team members joined our technology team , spearheading new product development and technological innovation , with a particular focus on evolving alpha‚Äôs online currency management platform and payment functionality , as well as developing custom platform solutions for our clients . thank you the bedrock of our business has always been our people , and i would like to thank everyone who has got alpha to this stage in its journey . your commitment and vision is helping to build a business that is driving change within the industry , as well as a culture i feel incredibly privileged to be a part of . last but by no means least , i would like to thank you , our shareholders who believe in our vision and are helping us to take alpha to new heights , whilst accelerating positive changes within our industry . outlook everyone at alpha passionately believes our ipo marked the start of our journey , not the end . we took alpha public to enable our growth ambitions whilst ensuring we could maintain the entrepreneurial culture that got us here in the rst place . we now have the opportunity to create a very special business , with a clear vision and growth strategy . i look forward to being able to report back on further progress achieved in the current year . morgan tillbrook chief executive o cer 248225 alpha fx pp12-pp26.qxp 03 04 2018 15 07 page 15 ',\n",
       " 'predicted_abstract': 'strategic report chief executive‚Äôs statement 14 operational review 2017 marked a strong year of growth for alpha, both in revenue and in our investment in sta and infrastructure.during the year, we increased our client numbers by 39 percent, bringing our total number of clients to 310. pleasingly, the fact that our revenue has grown at a higher percentage is a re ection of the larger trades that we are doing and the increasing size of our clients.we will continue to focus on growing our client base by penetrating our existing corporate marketplace in the uk, alongside continued expansion into the institutional marketplace and overseas sectors.europe in particular presents a very exciting area of expansion for us.overall, we are constantly looking for ways to acquire new clients, we remain dedicated to providing the very best levels of service to our existing client base.as such, we have created a team with a wealth of experience in institutional fx and tailored our technology and service solutions to meet investment managers‚Äô speci c and increasingly complex needs.in light of this, we made the important decision to migrate to a new cloud application platform in february 2018 which o ers richer functionality, greater customisation and increased scalability.we took alpha public to enable our growth ambitions whilst ensuring we could maintain the entrepreneurial culture that got us here in the rst place.we now have the opportunity to create a very special business, with a clear vision and growth strategy.i look forward to being able to report back on further progress achieved in the current year.morgan tillbrook chief executive o cer 248225 alpha fx pp12-pp26.qxp 03 04 2018 15 07 page 15 '}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cd5ac",
   "metadata": {},
   "source": [
    "evaluationÍπåÏßÄ ÏôÑÎ£å.  \n",
    "Í∑∏ÎÉ• pretrain Ïù¥Îûë fine-tuningÎêú Î™®Îç∏Ïù¥Îûë predict Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥ Î≥¥ÎãàÍπå fine-tuningÎêú Î™®Îç∏Ïùò Í≤∞Í≥ºÍ∞Ä Ìõ®Ïî¨ Ï¢ãÏïòÎã§. Î¨∏Ïû•Ïù¥ Îß§ÎÅÑÎüΩÍ≤å Ïù¥Ïñ¥ÏßÄÎ©∞ ÌëúÌòÑÎ†• ÎòêÌïú Ï¢ãÏïòÍ∏∞ ÎïåÎ¨∏Ïóê Ï†ÄÏû•Îêú checkpointÎ°ú evaluationÏùÑ ÏßÑÌñâÌï† ÏòàÏ†ïÏù¥Îã§."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
