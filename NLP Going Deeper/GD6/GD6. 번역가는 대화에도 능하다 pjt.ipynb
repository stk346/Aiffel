{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "middle-davis",
   "metadata": {},
   "source": [
    "## 번역 모델 만들기\n",
    "실습에선 접근성이 좋은 영어-스페인어 데이터를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfactory-focus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-spell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n",
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-roads",
   "metadata": {},
   "source": [
    "두 언어가 단어 사전을 공유하도록 하겠습니다. 영어와 스페인어 모두 알파벳으로 이뤄지는 데다가 같은 인도유럽어족이기 때문에 기대할 수 있는 효과가 많아요! 후에 챗봇을 만들 때에도 질문과 답변이 모두 한글로 이루어져 있기 때문에 Embedding 층을 공유하는 것이 성능에 도움이 됩니다.  \n",
    "  \n",
    "토큰화에는 Sentencepiece를 사용할 것이고 단어 사전 수는 20,000으로 설정하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-detective",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "중복데이터를 set 데이터형을 활용해 제거한 후 Sentencepiece 기반의 토크나이저를 생성해 주는 gernerate_tokenizer() 함수를 정의하여 토크나이저를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "induced-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "static-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus)) \n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-being",
   "metadata": {},
   "source": [
    "위에서 두 언어 사이에 단어 사전을 공유하기로 하였으므로, 따라서 Encoder와 Decoder의 전용 토크나이저를 만들지 않고, 방금 만들어진 토크나이저를 두 언어 사이에서 공유하게 됩니다. \n",
    "  \n",
    "토크나이저가 준비되었으니 본격적으로 데이터를 토큰화하도록 하겠습니다. 문장부호와 대소문자 등을 정제하는 preprocess_sentence() 함수를 정의해 데이터를 정제하고, 정제된 데이터가 50개 이상의 토큰을 갖는 경우 제거하도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recreational-prerequisite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6360f289dae4fa4ae3a3788b1fd6f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))   # encode_ad_ids() 는 문자열을 숫자로 분할합니다.\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-joseph",
   "metadata": {},
   "source": [
    "pad_sequences()를 이용해 패딩을 진행하겠습니다. 데이터의 1%는 테스트 셋으로 빼놓겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "discrete-priority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-morgan",
   "metadata": {},
   "source": [
    "### 트랜스포머 구현하기\n",
    "Encoder와 Decoder 각각의 Embedding과 출력층의 Linear, 총 3개의 레이어가 Weight를 공유할 수 있게 하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-maryland",
   "metadata": {},
   "source": [
    "**Positional Encoding**  \n",
    "$PE(pos, 2i) = Sin(pos/10000^{\\frac{2i}{d_{model}}})$  \n",
    "$PE(pos, 2i+1) = Cos(pos/10000^{\\frac{2i+1}{d_{model}}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interior-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-infrastructure",
   "metadata": {},
   "source": [
    "**Generate Padding Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accomplished-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-nature",
   "metadata": {},
   "source": [
    "**Multi-head Attention**\n",
    "$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "involved-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-luxury",
   "metadata": {},
   "source": [
    "**Position-wise Feed Forward Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dietary-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-beaver",
   "metadata": {},
   "source": [
    "**Encoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prime-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-advocate",
   "metadata": {},
   "source": [
    "**Decoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "after-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-holder",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reported-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-google",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "facial-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-outside",
   "metadata": {},
   "source": [
    "**Transformer 전체 모델 조립**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parallel-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-brazil",
   "metadata": {},
   "source": [
    "**모델 인스턴스 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "august-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-manor",
   "metadata": {},
   "source": [
    "모델을 만들었으니 학습을 시켜봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-portable",
   "metadata": {},
   "source": [
    "**Learning Rate Scheduler**  \n",
    "트랜스포머 학습률(learning rate)은 고정된 값을 유지하는 것이 아니라 학습 경과에 따라 변하도록 설게하였습니다. 공식은 아래와 같습니다.  \n",
    "$lrate = d_{model}^{-0.5}*min(step\\_num^{-0.5}, step\\_num * warmup\\_steps^{-1.5})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "partial-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-proposal",
   "metadata": {},
   "source": [
    "**Learning Rate & Optimizer 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "medium-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-validation",
   "metadata": {},
   "source": [
    "**Loss Function 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effective-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-notice",
   "metadata": {},
   "source": [
    "**Train Step 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "settled-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-logging",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "concerned-frederick",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07063541a55242adbf66e837936cdda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ca5bf192f246c384848de40cedb5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8e01058c214005a52f1693a6818419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-ability",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (1) BLEU Score\n",
    "라이브러리를 활용해서 간단하게 BLEU Score를 실습해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-twist",
   "metadata": {},
   "source": [
    "### NLTK를 활용한 BLEU Score\n",
    "NLTK는 Natural Language Tool Kit 의 준말로 이름부터 자연어 처리에 큰 도움이 될 것 같은 라이브러리입니다. BLEU Score를 지원하니 이를 활용하도록 합시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intelligent-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-contributor",
   "metadata": {},
   "source": [
    "BLEU Score는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많습니다.  \n",
    "  \n",
    "BLEU Score가 50점을 넘는다는 것은 정말 멋진 번역을 생성했다는 의미예요. 보통 논문에서 제시하는 BLEU Score는 20점에서 높으면 40점을 바라보는 정도거든요! 하지만 방금 나온 점수는 사실상 0점이라고 해야 하겠네요. 그렇게까지 엉망진창인 번역이 된 것일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-creativity",
   "metadata": {},
   "source": [
    "BLEU Score의 정의로 돌아가 한번 따져봅시다. BLEU Score가 N-gram으로 점수를 측정한다는 것을 기억하실 거예요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-croatia",
   "metadata": {},
   "source": [
    "$(\\prod_{i=1}^{4}precision_i)^\\frac{1}{4} = ($1-gram x 2-gram x 3-gram x 4-gram$)^\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-patch",
   "metadata": {},
   "source": [
    "1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면$(\\frac{1}{4})$ BLEU Score가 된답니다. 진정 멋진 번역이라면, 모든 N-gram에 대해서 높은 점수를 얻었을 거예요. 그렇다면 위에서 살펴본 예시에서는 각 N-gram이 점수를 얼마나 얻었는지 확인해 보도록 합시다. weights의 디폴트 값은 [0.25, 0.25, 0.25, 0.25]로 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 주는 것이지만, 만약 이 값을 [1, 0, 0, 0]으로 바꿔주면 BLEU Score에 1-gram의 점수만 반영하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "owned-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-innocent",
   "metadata": {},
   "source": [
    "0점에 가까운 BLEU Score가 나오는 원인을 알 수 있겠네요. 바로 3-gram와 4-gram에서 거의 0점을 받았기 때문인데요, 위 예시에서 번역문 문장 중 어느 3-gram도 원문의 3-gram과 일치하는 것이 없기 때문입니다. 2-gram이 0.18이 나오는 것은 원문의 11개 2-gram 중에 2개만이 번역문에서 재현되었기 때문입니다. (를 선호) (선호 한다)  \n",
    "  \n",
    "하지만 만약 nltk의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.  \n",
    "  \n",
    " 예전 버전에서는 위 수식에서 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸해버리기 때문에 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현되어 있었습니다. 하지만 1.0 은 모든 번역을 완벽히 재현했음을 의미하기 때문에 총점이 의도치 않게 높아질 수 있어요! 그럴 경우에는 BLEU Score가 바람직하지 못할 것(Undesirable)이라는 경고문이 추가되긴 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-volume",
   "metadata": {},
   "source": [
    "### SmoothingFunction()으로 BLEU Score 보정하기\n",
    "그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다. Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요.  \n",
    "  \n",
    "진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해 봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ranking-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1) # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-alliance",
   "metadata": {},
   "source": [
    "SmoothingFunction()로 BLEU score를 보정한 결과, 새로운 BLEU 점수는 무려, 5점으로 올라갔습니다. 거의 의미 없는 번역이라는 냉정한 평가를 받게 되는군요.  \n",
    "  \n",
    "여기서 BLEU-4가 BLEU-3보다 조금이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해 보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-killing",
   "metadata": {},
   "source": [
    "### 트랜스포머 모델의 번역 성능 알아보기\n",
    "위 예시를 조금만 응용하면 우리가 훈련한 모델이 얼마나 번역을 잘하는지 평가할 수 있습니다! 아까 1%의 데이터를 테스트셋으로 빼 둔 것을 기억하시죠? 테스트셋으로 모델의 BLEU Score를 측정하는 함수 eval_bleu() 를 구현해보도록 합시다!  \n",
    "  \n",
    "먼저 번역을 생성하기 위해 evaluate() 함수와 translate() 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ranking-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence) # 문자열을 token으로 분할합니다.\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence) # 문자열을 숫자로 분할합니다.\n",
    "    \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                    maxlen=enc_train.shape[-1], padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장합니다.\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids) # 숫자를 문자열로 복원합니다.\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "        result = tgt_tokenizer.decode_ids(ids)\n",
    "        return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "undefined-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-kernel",
   "metadata": {},
   "source": [
    "이제 함수 eval_bleu() 를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "italian-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score=0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "    \n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "        \n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                        smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Source Sentence:\", src_sentence)\n",
    "            print(\"Model Prediction:\", candidate)\n",
    "            print(\"Real:\", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "            \n",
    "        print(\"Num of Sample:\", sample_size)\n",
    "        print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-station",
   "metadata": {},
   "source": [
    "주어진 병렬 말뭉치 src_corpus 와 tgt_corpus 를 인덱스 순으로 살피며 소스 토큰과 타겟 토큰을 각각 원문으로 Decoding 하고, 소스 문장을 translate() 함수를 통해 번역한 후 생성된 번역문과 타겟 문장의 BLEU Score를 측정합니다. 측정된 score 는 total_score 에 합산되어 최종적으로 주어진 병렬 말뭉치의 평균 BLEU Score를 출력하죠!  \n",
    "\n",
    "verbose 변수를 True 로 주면 번역문과 원문, 매 스텝의 점수를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "natural-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956f5c630c124c5692ff2b43be0cd0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: can you give him first aid ?......................................\n",
      "Model Prediction: []\n",
      "Real: ['¿', 'puedes', 'darle', 'primeros', 'auxilios', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n",
      "Source Sentence: we re your neighbors .........................................\n",
      "Model Prediction: []\n",
      "Real: ['somos', 'tus', 'vecinos', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n",
      "Source Sentence: you shouldn t keep them waiting so long ...................................\n",
      "Model Prediction: []\n",
      "Real: ['no', 'deber', 'as', 'dejarlos', 'esperando', 'tanto', 'tiempo', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-television",
   "metadata": {},
   "source": [
    "3 Epoch밖에 학습하지 않아서인지 성능이 좋지 않네요. 표본이 적은 것일 수도 있으니 좀 더 많은 데이터로 측정해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "frank-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba225fc8af4e4305b89d9b375322c569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n",
      "Num of Sample: 119\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-admission",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (2) Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "entire-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]] # 생성된 문장과 점수를 저장\n",
    "    \n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "                \n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "recreational-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  //Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for word in seq:\n",
    "        sentence += vocab[word]+ \" \"\n",
    "        \n",
    "    print(sentence, \"//Score: %.4f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unknown-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 6, 4, 9, 5, 1, 0, 0, 0, 0], 42.52430434762729],\n",
       " [[2, 6, 3, 9, 5, 1, 0, 0, 0, 0], 28.013461367181293],\n",
       " [[3, 6, 4, 9, 5, 1, 0, 0, 0, 0], 17.89828068648655]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-sheffield",
   "metadata": {},
   "source": [
    "사실 이 예시는 Beam Search를 설명하는 데에는 더없이 적당하지만 실제로 모델이 문장을 생성하는 과정과는 거리가 멉니다. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 prob_seq 처럼 확률을 정의할 수 없겠다는 생각이 머리를 스치죠. 각 단어에 대한 확률은 prob_seq 처럼 한 번에 정의가 되지 않고 이전 스텝까지의 단어에 따라서 결정되기 때문입니다!  \n",
    "  \n",
    "간단한 예시로, Beam Size가 2이고 Time-step이 2인 순간의 두 문장이 나는 밥을 , 나는 커피를 이라고 한다면 세 번째 단어로 먹는다 , 마신다 를 고려할 수 있습니다. 이때, 전자에서 마신다 에 할당하는 확률과 후자에서 마신다 에 할당하는 확률은 각각 이전 단어들인 나는 밥을 , 나는 커피를 에 따라서 결정되기 때문에 서로 독립적인 확률을 갖습니다. 예컨대 후자가 마신다 에 더 높은 확률을 할당할 것을 알 수 있죠! 위 소스에서처럼 \"3번째 단어는 항상 [마신다: 0.3, 먹는다:0.5, ...] 의 확률을 가진다!\" 라고는 할 수 없다는 겁니다.  \n",
    "  \n",
    "따라서 Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 합니다. Beam Size가 5라고 가정하면 맨 첫 단어로 적합한 5개의 단어를 생성하고, 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률)를 가지고 있으니 각각의 순위를 매길 수 있겠죠? 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.  \n",
    "  \n",
    "위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-aerospace",
   "metadata": {},
   "source": [
    "### Beam Search Decoder 작성 및 평가하기\n",
    "각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "moved-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "    \n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    model(src_ids, tgt_ids, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-concord",
   "metadata": {},
   "source": [
    "np.long  \n",
    "보통 int는 4바이트, long long은 8바이트를 할당받는 데이터 타입이다. 즉, int 데이터 타입을 가지는 변수는 2진수로 총 32자리(8비트 * 4바이트)까지 표현 가능하고, long long 데이터는 64자리까지 표현 가능하다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "available-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "excess-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "    \n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score - calculate_bleu(reference, candidate)\n",
    "        \n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "        \n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-amount",
   "metadata": {},
   "source": [
    "구현 후 다음과 같이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bizarre-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['sigamos', 'al', 'habla', '...........................................']\n",
      "Candidate: []\n",
      "BLEU: 0\n",
      "Reference: ['sigamos', 'al', 'habla', '...........................................']\n",
      "Candidate: ['t']\n",
      "BLEU: 0\n",
      "Reference: ['sigamos', 'al', 'habla', '...........................................']\n",
      "Candidate: ['t']\n",
      "BLEU: 0\n",
      "Reference: ['sigamos', 'al', 'habla', '...........................................']\n",
      "Candidate: ['t']\n",
      "BLEU: 0\n",
      "Reference: ['sigamos', 'al', 'habla', '...........................................']\n",
      "Candidate: ['tt']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-process",
   "metadata": {},
   "source": [
    "## 데이터 부풀리기\n",
    "이번 스텝에서는 Data Augmentation, 그중에서도 Embedding을 활용한 Lexical Substitution을 구현해 볼 거예요.  \n",
    "  \n",
    "gensim 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.  \n",
    "1) 직접 모델을 다운로드해 load 하는 방법  \n",
    "2) gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법  \n",
    "  \n",
    "한국어는 gensim 에서 지원하지 않으므로 두 번째 방법을 사용할 수 없지만, 영어라면 얘기가 달라지죠! 아래 웹페이지의 Available data → Model 부분에서 공개된 모델의 종류를 확인할 수 있습니다.  \n",
    "  \n",
    "대표적으로 사용되는 Embedding 모델은 word2vec-google-news-300 이지만 용량이 커서 다운로드에 많은 시간이 소요되므로 이번 실습엔 적합하지 않습니다. 우리는 적당한 사이즈의 모델인 glove-wiki-gigaword-300 을 사용할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "three-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-parliament",
   "metadata": {},
   "source": [
    "불러온 모델은 아래와 같이 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cutting-invention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-reliance",
   "metadata": {},
   "source": [
    "주어진 데이터를 토큰 단위로 분리한 후, 랜덤하게 하나를 선정하여 해당 토큰과 가장 유사한 단어를 찾아 대치하면 그것으로 Lexical Substitution은 완성되겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ordered-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know? all you need is attention.\n",
      "To: you know? all you need now attention. \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"you know? all you need is attention.\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0]+\" \"\n",
    "        \n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "        \n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "previous-model",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7094072a727f46088fb4dcf190a1b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' she looked as though she had seeing a ghost . ', 'she looked as though she had seen a ghost .', ' i took over the business back my father . ', 'i took over the business from my father .', ' may i use the toilet you ', 'may i use the toilet ?', ' i j see you saturday . ', 'i ll see you saturday .', ' show me the money , ', 'show me the money .']\n"
     ]
    }
   ],
   "source": [
    "### Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "    \n",
    "    res = \" \"\n",
    "    toks = sentence.split()\n",
    "    \n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except: # 단어장에 없는 단어\n",
    "        return None\n",
    "    \n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "            \n",
    "    return res\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "    \n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "    \n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "        \n",
    "    new_corpus.append(old_src)\n",
    "    \n",
    "print(new_corpus[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
