{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f980b26b",
   "metadata": {
    "id": "6Hgw3GTXLLw0"
   },
   "source": [
    "## 🤗 Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32de77",
   "metadata": {
    "id": "W7-QHmRiAMB9"
   },
   "source": [
    "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
    "\n",
    "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
    "\n",
    "We will leverage 🤗`Seq2SeqTrainer`, gradient checkpointing and as usual 🤗`datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd59a64",
   "metadata": {
    "id": "0B19PhgrCHM1"
   },
   "source": [
    "First, let's try to get a GPU with at least 15GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661029",
   "metadata": {
    "id": "6z18zG2N192Z"
   },
   "source": [
    "To check that we are having enough RAM we can run the following command.\n",
    "If the randomely allocated GPU is too small, the above cells can be run \n",
    "to crash the notebook hoping to get a better GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0b19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ded7d85",
   "metadata": {
    "id": "P59lSzY4192Z"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install datasets==1.2.1\n",
    "# !pip install transformers==4.2.0\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a533c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface datasets load\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_train_df/\")\n",
    "val_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_val_df/\")\n",
    "test_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_test_df/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1583374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': ' 25229.04 9 march 2017 5 13 pm proof 5 post self editing chief executive’s review our outlook is positive and we remain optimistic that we are well placed to take advantage of an upturn in our markets . s .c . harris l group chief executive overview reported revenues for the group were up 6 percent in 2016. however , the significant weakness in the oil gas market and the knock-on effect on general industrial demand continued to exert considerable downward pressure on group results . group revenue on a like- for-like basis 1 was down 3.5 percent . like-for-like sales to the energy markets were down 27 percent . excluding the impact of these falling energy revenues , the group had flat sales year on year . it should be noted that like-for-like revenues at the half year were down 6 percent . the improved performance for the full year was due to a notable pickup in activity in the fourth quarter . civil aerospace in western europe was strong , particularly in the second half , while the north american business was more mixed as the supply chains for this sector continue to go through adjustment associated with the changeover of aircraft and engine platforms . in automotive , car and light trucks built on the strong start to 2016, accelerating through the year such that the second half growth rate was 6 percent on a like-for-like basis . while the background market demand in the car and light truck market has moderated over recent months , group revenues continue to grow on the back of new programme wins . specialist technologies performed well overall , increasing their contribution to group headline operating profit to 42 percent . while two of these technologies , surface technology and hip product fabrication , continue to be hard hit by low levels of activity in the oil gas sector , the remaining technologies once again showed good growth . margins continued to exceed 30 percent in specialist technologies . the group’s headline operating margin 2 was resilient at 16.6 percent . headline operating profit declined 2 percent from pound 102.1m to pound 99.6m 13 percent at constant exchange rates . the robust margin performance was partly helped by favourable currency translation . however , the bodycote margin model is helping to drive improvements and this , together with the improved flexibility the group now has in its cost base and the favourable impact on mix coming from specialist technologies , combined to deliver margin resilience that would not have been possible in past downturns . 04 bodycote plc annual report for the year ended 31 december 2016 25229.04 9 march 2017 5 13 pm proof 5 post self editing we completed the acquisition of five plants in 2016, in line with our bolt-on strategy in classical heat treatment . in some cases these acquisitions brought new capability into the group , but in all cases they strengthened our network and enhanced our local cluster strength . they were all completed in the second half of the year with the majority in the final quarter and , correspondingly , had an immaterial impact on our group result . they will , on the other hand , provide us with a small impetus as we enter 2017 . annualised sales from the acquisitions will be around pound 20m , with average group margins expected to be achieved in 2017 . we have a good acquisition pipeline and fully intend to continue to execute these acquisition opportunities provided we are confident that they will create further shareholder value . basic headline earnings per share were 37 .0p , a decline of 6 percent , principally reflecting a higher tax rate in 2016. cash generation has remained strong , with 92 percent of headline operating profit turned into cash 3 2015 80 percent . indeed , even after spending pound 30m on acquisitions , capital investment of pound 63m corresponding to 1 .1 times depreciation , restructuring costs and pound 48m of dividends , the group’s year end net cash position only reduced by pound 11 .2m to pound 1 .1m 2015 pound 12.3m . the group continues to be in a strong financial position , with plenty of available financial headroom . strategic progress we continued our strategy of investing in areas where we see high-growth potential but are always mindful of the group’s minimum 20 percent hurdle rate for return on capital employed . expenditure in the year included new facilities in the usa , mexico , poland and france , as well as continued expansion of our specialist technologies’ capacities and capabilities , and further deployment of the group’s erp programme . the strategy of preferential investment in specialist technologies continues to benefit the group . the group’s strategy remains relevant and unaltered . the drive for operational efficiency in the more mature parts of the business , expansion of the group’s footprint in rapid-growth countries and the focus on growth in the higher value-added businesses , particularly specialist technologies , are all designed to increase the quality of the group’s earnings and create significant value . we have made further progress during this challenging year and remain readier than ever to respond to developments in our markets . summary and outlook the group delivered a robust performance in 2016 despite significant headwinds in some key business sectors . the speed and effectiveness of management’s actions , in addition to the continued focus on improved mix , resulted in resilient margins . while our business , by its nature , has limited forward visibility , we continue to demonstrate that we are capable of adapting with great agility to changes in market conditions . our outlook is positive and we remain optimistic that we are well placed to take advantage of an upturn in our markets . the board is confident that management’s continued focus on business improvements and execution of the group strategy will generate good returns through the cycle . s .c . harris group chief executive 28 february 2017 1. like-for-like year-on-year revenue growth rates are at constant exchange rates and exclude acquisitions , closed sites and the impact of the disposal of businesses . 2. headline operating margin is defined as headline operating profit as a percentage of revenue . 3. cash conversion is defined as headline operating cash as a percentage of headline operating profit . strategic report governance financial statements additional information 05 www .bodycote .com stock code boy ',\n",
       " 'abstract': ' 25229.04 9 march 2017 5 13 pm proof 5 post self editing chairman’s statement we remain well placed to take advantage of our strong position , whichever way our markets develop , and the long- term prospects for the group are excellent . a .m . thomson l chairman overview 2016 was another challenging year for the group . the oil gas and other resources’ downturn persisted , with its knock-on impact on the capital goods market more generally . this resulted in a decline in revenue at constant exchange rates , partially offset by continued growth in bodycote’s automotive and aerospace business . the group , under the stewardship of stephen harris and his executive team , has once again successfully navigated the business through these tough conditions and we remain in a strong financial position . dividend the board considers the dividend to be an important component of shareholder returns and is proposing a final ordinary dividend of 10.8p , an increase of 4.9 percent , which will be paid on 2 june 2017 , subject to shareholder approval at the 2017 annual general meeting agm . this brings the total ordinary dividend for 2016 to 15.8p 2015 15.1p costing pound 20.5m which represents a year-on- year increase of 4.6 percent . the board is not recommending a special dividend this year , noting that , on top of the bolt-on acquisitions completed in 2016, there is a pipeline of further potential transactions , as well as other investments to support growth , which the board believes will deliver superior returns for shareholders . governance and reporting one of my key responsibilities as chairman is to promote effective governance across the group thus ensuring that we remain a successful and sustainable entity with good governance procedures practised across all 23 countries in which the group operates . in order to enable shareholders to understand how this goal is achieved , we have provided a corporate governance statement on page 38 of this annual report . this describes how the governance structure underpins the delivery of the group’s business strategy . on page 24 we have also outlined the principal risks that may prevent the business from achieving its objectives and the actions being taken to overcome these potential obstacles . during the year we introduced a new directors’ remuneration policy , following approval at the group’s agm in may 2016. details of the arrangements for directors’ remuneration can be found in the board report on remuneration on pages 54 to 73 of this annual report . 02 bodycote plc annual report for the year ended 31 december 2016 25229.04 9 march 2017 5 13 pm proof 5 post self editing board matters it is the responsibility of every board to ensure that there is an appropriate succession plan in place across the business , including for the board of directors . 2016 marked a number of changes to our board , testing and proving the efficacy of our planning . it was with great sadness that we learned of raj rajagopal’s passing in november 2016. raj , our senior independent director , served on the board from september 2008 until his retirement after the 2016 agm . he was succeeded as senior independent director by ian duncan , who also chairs the group’s audit committee and is a member of the board’s other committees . i was delighted to welcome pat larmon to our board in september . pat joins us with a strong background in packaging products , originally as a business owner , and more recently as president of bunzl’s north america business since 2003. his experience in running complex multi-site operations , as well as in completing and integrating multiple bolt-on acquisitions , will serve bodycote very well . david landless , who demonstrated his dedication to bodycote by agreeing to stay on as group finance director in order to effect a smooth handover to his successor , retired from the business at the end of 2016. i would like to thank david for his service and dedication during his 17 years at the group . i was also pleased to welcome dominique yates to the board in november as david’s successor . dominique joins us with ten years of experience as chief financial officer , with quoted company experience at symrise ag and , most recently , regus plc . dominique assumed the role of chief financial officer at the beginning of january 2017 . people bodycote is a service business , but first class service is delivered by passionate and professional people , who understand their customers’ needs and meet their demanding requirements time after time . we will continue to invest in training and developing our employees to ensure that our talented workforce remains one of our competitive advantages . as noted above , 2016 has not been an easy year for the group and , once again , i would like to thank all of our employees for their dedication . shareholders during the year i met with a number of bodycote’s largest shareholders and received positive feedback from them on their views of the group . in the coming year i will maintain this valuable dialogue . i also look forward to meeting as many shareholders as possible at this year’s agm in may 2017 , when there will be an opportunity to discuss the group’s business and future prospects with board members . summary the performance of the group was creditable in 2016 through another challenging year . decisive cost actions and measured investments have helped mitigate the negative impacts of weak market conditions . we have a strong , high-performance culture serving a wide range of international customers , with a committed workforce and absolute integrity in our operating procedures . we remain well placed to take advantage of our strong position , whichever way our markets develop , and the long-term prospects for the group are excellent . i remain confident that these should ensure an attractive return for both our employees and our shareholders over the coming years . a .m . thomson chairman 28 february 2017 strategic report governance financial statements additional information 03 www .bodycote .com stock code boy ',\n",
       " '__index_level_0__': 568}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe607ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2167\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19146d6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMQWD3wL192Z",
    "outputId": "80096c0e-b26b-488f-ac55-d194e2579d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  7 08:05:23 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   65C    P0    50W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2e40",
   "metadata": {
    "id": "APGpqABk192Z"
   },
   "source": [
    "Next, we install 🤗Transformers, 🤗Datasets, and `rouge_score`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfb3f8",
   "metadata": {
    "id": "p3Me6n0o192Z"
   },
   "source": [
    "Let's start by loading and preprocessing the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b551ed",
   "metadata": {
    "id": "CzjKRBib192Z"
   },
   "source": [
    "Next, we download the pubmed train and validation dataset ([click to see on 🤗Datasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **☕** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5b249",
   "metadata": {
    "id": "Uyn-TDeB192a"
   },
   "source": [
    "It's always a good idea to take a look at some data samples. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febc7268",
   "metadata": {
    "id": "5QkyLfIy192a"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=4):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4813ec",
   "metadata": {
    "id": "wx3iiUgy192a"
   },
   "source": [
    "We can see that the input data is the `article` - a scientific report and the target data is the `abstract` - a concise summary of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bcd4a",
   "metadata": {
    "id": "xs7zKqy-192a"
   },
   "source": [
    "Cool! Having downloaded the dataset, let's tokenize it.\n",
    "We'll import the convenient `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0e1ef7",
   "metadata": {
    "id": "k7wczXZP192a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a2d36",
   "metadata": {
    "id": "Ifg19ED9192a"
   },
   "source": [
    "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a786b30",
   "metadata": {
    "id": "w_s6Z_ni192a"
   },
   "source": [
    "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.${}^1$. \n",
    "\n",
    "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
    "\n",
    "In this notebook, we are only able to train on `batch_size=2` to prevent out-of-memory errors.\n",
    "\n",
    "---\n",
    "${}^1$ The data is taken from page 11 of [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62abf2ee",
   "metadata": {
    "id": "jk7tS_xN192b"
   },
   "outputs": [],
   "source": [
    "max_input_length = 4096\n",
    "max_output_length = 1024\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fd76",
   "metadata": {
    "id": "2493L-dt192b"
   },
   "source": [
    "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
    "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
    "\n",
    "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b28121",
   "metadata": {
    "id": "2_UzG6Ek192b"
   },
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8da054",
   "metadata": {
    "id": "q-V7N0L-192b"
   },
   "source": [
    "For the sake of this notebook, we will reduce the training and validation data \n",
    "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b172c6",
   "metadata": {
    "id": "OeJ7dOpi192b"
   },
   "source": [
    "Great, having defined the mapping function, let's preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3971ec4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a7fe6f9ae61b4cfbadaba2c0bfc50e1a",
      "f3b8ce93ce314a4d80b6e7b9f935600c",
      "58609f6a301741c788d9e190e49601bc",
      "727b33b4d0ba48d3be3b76040bfb0ead",
      "ea44f4c51d334e3aa3ed03acbb795242",
      "591785faa90742dfb12a3436c2dee0e2",
      "8156f721d27246b5a5357fac577b7d82",
      "415c082418fd4c3b94408e00bbcda82a"
     ]
    },
    "id": "wcaN0IJD192b",
    "outputId": "952499e3-265a-4694-ec89-7f848d19f2a6"
   },
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da93a5",
   "metadata": {
    "id": "8gunSLWj192b"
   },
   "source": [
    "and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6f650bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "aa16af7563554384be6e5513c6afe79a",
      "3eb2a25aab6141bfa1cf5a76bed6c5b2",
      "be015dcb6cef4b41911e69fbc5af1a59",
      "a62d68c2e4914f0ba08ede851cf47280",
      "f2c219f0099d411caf4ff5e831673875",
      "0e6d376e4a394a2aa025e2edc3898ec8",
      "628aa2531c944e4e8efeecf6fca40993",
      "78482e48010a412385fbe008d146d728"
     ]
    },
    "id": "FkrEujTX192b",
    "outputId": "56576f0b-29df-4fde-84e5-c82f81de6b81"
   },
   "outputs": [],
   "source": [
    "# val_dataset = val_dataset.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd6638",
   "metadata": {
    "id": "2I_QUpgAyOJ7"
   },
   "source": [
    "**train_dataset**  \n",
    "attention mask  \n",
    "global_attention_mask  \n",
    "input_ids  \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e3997",
   "metadata": {
    "id": "abdbZ4eW192b"
   },
   "source": [
    "Finally, the datasets should be converted into the PyTorch format as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd6b651",
   "metadata": {
    "id": "fC9arodU192b"
   },
   "outputs": [],
   "source": [
    "# train_dataset.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )\n",
    "# val_dataset.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b245fe",
   "metadata": {
    "id": "A4j2pUM_192c"
   },
   "source": [
    "Alright, we're almost ready to start training. Let's load the model via the `AutoModelForSeq2SeqLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c263a05",
   "metadata": {
    "id": "V6wz7S7b192c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e567e",
   "metadata": {
    "id": "EpOKd662192c"
   },
   "source": [
    "We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e5d609",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "642d8f4c64474cccbec943413240c027",
      "d2ad50bf2a3b4b77947a29b72ec2bc8c",
      "2dcf5d79460d4554940dd259ccca44a4",
      "1960516a36a3417c8a3c345d2cc101ec",
      "519ced1cfc774ca2a671cfe3903623a8",
      "2074f2221bef4523ba6f7364db39651c",
      "b395732993124de480c2da0b8bb63032",
      "f2e83c9cec9b4236afb6a7227ebf5db9"
     ]
    },
    "id": "vHw7_nMQ192c",
    "outputId": "38090c2b-bfad-4469-cf87-613f75634229"
   },
   "outputs": [],
   "source": [
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6df40",
   "metadata": {
    "id": "sTDTLQV9192c"
   },
   "source": [
    "During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0a0009",
   "metadata": {
    "id": "vSt8x6mu192c"
   },
   "outputs": [],
   "source": [
    "# set generate hyperparameters\n",
    "led.config.num_beams = 2\n",
    "led.config.max_length = 1024\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d21319",
   "metadata": {
    "id": "gqqcuezU192c"
   },
   "source": [
    "Next, we also have to define the function the will compute the `\"rouge\"` score during evalution.\n",
    "\n",
    "Let's load the `\"rouge\"` metric from 🤗datasets and define the `compute_metrics(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac749d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5e605dd890d741a5a89dd1fd04183e9c",
      "b7bfd05045ef495bad3d30e6a2ad3f9b",
      "dea7c32f1e71438d9e30d808be8ff585",
      "8d22ef0d5edb458781eccf7e0037cbcd",
      "20c130370f4249a68c2a73fc4369f527",
      "d94589c558cf4f928bfdbfc1c182c806",
      "f177d781255f4db783b7394229d8f730",
      "fc1e239059a945139662bc1dd6b0b2b5"
     ]
    },
    "id": "B1gU2BXJ192c",
    "outputId": "eade9532-4546-4359-b29f-599916726da5"
   },
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1aa0a",
   "metadata": {
    "id": "TG0mPNbp192c"
   },
   "source": [
    "The compute metrics function expects the generation output, called `pred.predictions` as well as the gold label, called `pred.label_ids`.\n",
    "\n",
    "Those tokens are decoded and consequently, the rouge score can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf8c935",
   "metadata": {
    "id": "z9o3v3O9192c"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ee710",
   "metadata": {
    "id": "77JgIa9o192c"
   },
   "source": [
    "Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b403e79e",
   "metadata": {
    "id": "tfV29f1L192d"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca01ff",
   "metadata": {
    "id": "51TY_eN-192d"
   },
   "source": [
    "In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n",
    "\n",
    "Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8236db2f",
   "metadata": {
    "id": "s9TBR1Fa192d"
   },
   "outputs": [],
   "source": [
    "# # enable fp16 apex training\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     fp16=True,\n",
    "#     output_dir=\"./\",\n",
    "#     logging_steps=5,\n",
    "#     eval_steps=10,\n",
    "#     save_steps=10,\n",
    "#     save_total_limit=2,\n",
    "#     gradient_accumulation_steps=4, ## 줄일 수 있는 parameter인듯. default=4\n",
    "#     num_train_epochs=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1f466",
   "metadata": {
    "id": "nHgxs9s6192d"
   },
   "source": [
    "The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aba0e45",
   "metadata": {
    "id": "BVddmiYv192d"
   },
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=led,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6f5e0",
   "metadata": {
    "id": "PSa9ed3l192d"
   },
   "source": [
    "and we can start training. This will take about ~35min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c42e21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t5Jkoai2OOY",
    "outputId": "4d3bb483-edb4-46a1-9bbb-f7bae0026a13"
   },
   "outputs": [],
   "source": [
    "# ! pip install apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a156a574",
   "metadata": {
    "id": "CigrFIc4192d"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbb384",
   "metadata": {
    "id": "omQfzdvO192d"
   },
   "source": [
    "This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n",
    "\n",
    "In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9c5b8",
   "metadata": {
    "id": "WSUZxqMX192d"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863eaf02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmnWQn0dtz6C",
    "outputId": "8d8b73db-997e-4074-c2aa-23186d5a8beb"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install amp-atomistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df9efa68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfe1ztPu5fZT",
    "outputId": "e1e2325f-c4f3-47ad-883e-205744cc09cd"
   },
   "outputs": [],
   "source": [
    "# # 시간 오래 걸림. 30분 정도 apex나 amp 사용 안하면 설치할 필요 없음\n",
    "\n",
    "# !git clone https://github.com/NVIDIA/apex.git\n",
    "# %cd apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2230e4df",
   "metadata": {
    "id": "oO-ro4dXuBcC"
   },
   "outputs": [],
   "source": [
    "# import amp\n",
    "# from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a1a6168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  7 08:05:30 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   65C    P0    50W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2643f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2871ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff5f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb969c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('학습을 진행하는 기기:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36ecf870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "print(torch.__version__) # torch version 출력\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efbab5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be5d18da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "msB7BC1d192d",
    "outputId": "06060f81-6742-4124-9f4c-d8900a1cf808",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# from datasets import load_dataset, load_metric\n",
    "# from transformers import (\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "# )\n",
    "\n",
    "# # load rouge\n",
    "# rouge = load_metric(\"rouge\")\n",
    "\n",
    "# # load pubmed\n",
    "# # pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n",
    "# # pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n",
    "# pubmed_train = train_dataset\n",
    "# pubmed_val = val_dataset\n",
    "\n",
    "# # comment out following lines for a test run\n",
    "# # pubmed_train = pubmed_train.select(range(32))\n",
    "# # pubmed_val = pubmed_val.select(range(32))\n",
    "\n",
    "# # load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "\n",
    "# # max encoder length is 8192 for PubMed\n",
    "# encoder_max_length = 4096\n",
    "# decoder_max_length = 1024\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# def process_data_to_model_inputs(batch):\n",
    "#     # tokenize the inputs and labels\n",
    "#     inputs = tokenizer(\n",
    "#         batch[\"article\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=encoder_max_length,\n",
    "#     )\n",
    "#     outputs = tokenizer(\n",
    "#         batch[\"abstract\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=decoder_max_length,\n",
    "#     )\n",
    "\n",
    "#     batch[\"input_ids\"] = inputs.input_ids\n",
    "#     batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "#     # create 0 global_attention_mask lists\n",
    "#     batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "#         [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "#     ]\n",
    "\n",
    "#     # since above lists are references, the following line changes the 0 index for all samples\n",
    "#     batch[\"global_attention_mask\"][0][0] = 1\n",
    "#     batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "#     # We have to make sure that the PAD token is ignored\n",
    "#     batch[\"labels\"] = [\n",
    "#         [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "#         for labels in batch[\"labels\"]\n",
    "#     ]\n",
    "\n",
    "#     return batch\n",
    "\n",
    "\n",
    "# # map train data\n",
    "# pubmed_train = pubmed_train.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )\n",
    "\n",
    "# # map val data\n",
    "# pubmed_val = pubmed_val.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )\n",
    "\n",
    "# # set Python list to PyTorch tensor\n",
    "# pubmed_train.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )\n",
    "\n",
    "# # set Python list to PyTorch tensor\n",
    "# pubmed_val.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )\n",
    "\n",
    "# # enable fp16 apex training  ## ## name 'amp' is not defined 문제로 주석처리\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     fp16=False, ## default = True\n",
    "#     fp16_backend=\"auto\", ## default = apex\n",
    "#     output_dir=\"./\",\n",
    "#     logging_steps=250,\n",
    "#     eval_steps=5000,\n",
    "#     save_steps=500,\n",
    "#     warmup_steps=1500,\n",
    "#     save_total_limit=2,\n",
    "#     gradient_accumulation_steps=4,\n",
    "# )\n",
    "\n",
    "\n",
    "# # compute Rouge score during validation\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(\n",
    "#         predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "#     )[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }\n",
    "\n",
    "\n",
    "# # load model + enable gradient checkpointing & disable cache for checkpointing\n",
    "# led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# # set generate hyperparameters\n",
    "# led.config.num_beams = 4\n",
    "# led.config.max_length = 1024\n",
    "# led.config.min_length = 100\n",
    "# led.config.length_penalty = 2.0\n",
    "# led.config.early_stopping = True\n",
    "# led.config.no_repeat_ngram_size = 3\n",
    "\n",
    "\n",
    "# # instantiate trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=led,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args, ## optional\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=pubmed_train,\n",
    "#     eval_dataset=pubmed_val,\n",
    "# )\n",
    "\n",
    "# # start training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a057d4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c7cfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66e1bf24f064e4a9acfe12d00a015fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "# load pubmed\n",
    "pubmed_test = test_dataset\n",
    "\n",
    "# load tokenizer\n",
    "model_path = \"/home/aiffelsummabot/LED/checkpoint-500/\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_path)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_path).to(\"cuda\").half()\n",
    "\n",
    "\n",
    "def generate_answer(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    # put global attention on <s> token\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "\n",
    "result = pubmed_test.map(generate_answer, batched=True, batch_size=4)\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca653a1f",
   "metadata": {},
   "source": [
    "P100 GPU 두 개를 붙여서 학습을 진행하니까 한 시간 내외로 빠르게 훈련이 가능했다. 하지만 evaluation을 진행할 때 gpu가 돌지 않고 별 다른 반응이 없다. 왜 그런건지 다시 살펴 봐야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
