{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigBird_Pretrain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKs56dOakJw2"
      },
      "source": [
        "# 참고\n",
        "# https://github.com/google-research/bigbird/blob/master/bigbird/pretrain/run_pretraining.py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C86lLq8kUoK",
        "outputId": "00feb9ff-62b3-4ae4-b918-f25cc625810d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc-IHEOvkSnY",
        "outputId": "01ddc56a-50a3-43fa-faad-10b9ae401014"
      },
      "source": [
        "# colab pro 더 빠른 GPU 사용\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoXBeyOLkbvf",
        "outputId": "750b9762-97f1-4f6c-8f70-1822cc287f1c"
      },
      "source": [
        "# colab pro 추가 메모리\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVOoMc60tM9I",
        "outputId": "5c871f2a-bf99-4d02-c573-bb9697c73cde"
      },
      "source": [
        "! pip install absl-py\n",
        "! pip install natsort\n",
        "! pip install numpy\n",
        "! pip install rouge-score\n",
        "! pip install sentencepiece\n",
        "! pip install tensorflow\n",
        "! pip install \n",
        "! pip install \n",
        "! pip install \n",
        "! pip install \n",
        "! pip install "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py) (1.15.0)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdgip0vhw2Je",
        "outputId": "db068cd4-b156-44cc-ac04-fd69948a3a1f"
      },
      "source": [
        "! pip install -r /content/bigbird/requirements.txt"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from -r /content/bigbird/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from -r /content/bigbird/requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/bigbird/requirements.txt (line 3)) (1.19.5)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r /content/bigbird/requirements.txt (line 6)) (2.7.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.4.0.dev202111210106-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->-r /content/bigbird/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score->-r /content/bigbird/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.22.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (12.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.42.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r /content/bigbird/requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->-r /content/bigbird/requirements.txt (line 7)) (0.12.0)\n",
            "Collecting gevent\n",
            "  Downloading gevent-21.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (4.1.3)\n",
            "Collecting pypng\n",
            "  Downloading pypng-0.0.21-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dopamine-rl in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.0.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (4.0.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.12.8)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (4.62.3)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.17.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.1.4)\n",
            "Collecting mesh-tensorflow\n",
            "  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
            "\u001b[K     |████████████████████████████████| 366 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->-r /content/bigbird/requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->-r /content/bigbird/requirements.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->-r /content/bigbird/requirements.txt (line 9)) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->-r /content/bigbird/requirements.txt (line 9)) (0.3.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (3.0.6)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 60.5 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (1.2.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (2.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (0.1.6)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor->-r /content/bigbird/requirements.txt (line 8)) (21.2.0)\n",
            "Building wheels for collected packages: bz2file\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6883 sha256=a12e721c2460ce95eb73236fcb4dc000e492c9b3e9df57cb53c2c5b540861020\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/ce/8d/b5f76b602b16a8a39f2ded74189cf5f09fc4a87bea16c54a8b\n",
            "Successfully built bz2file\n",
            "Installing collected packages: zope.interface, zope.event, tensorflow-probability, tf-slim, tensorflow-gan, tensorflow-addons, pypng, mesh-tensorflow, kfac, gunicorn, gevent, bz2file, tfds-nightly, tensorflow-text, tensor2tensor, sentencepiece, rouge-score\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.15.0\n",
            "    Uninstalling tensorflow-probability-0.15.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.15.0\n",
            "Successfully installed bz2file-0.98 gevent-21.8.0 gunicorn-20.1.0 kfac-0.2.0 mesh-tensorflow-0.1.19 pypng-0.0.21 rouge-score-0.0.4 sentencepiece-0.1.96 tensor2tensor-1.15.7 tensorflow-addons-0.15.0 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 tensorflow-text-2.7.3 tf-slim-1.1.0 tfds-nightly-4.4.0.dev202111210106 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "0flKIC0hv1um",
        "outputId": "a8808635-b950-4364-de34-894a6f134630"
      },
      "source": [
        "# setup, requirement 수동으로 설치해 주었으니 코드 따로 실행 안해도 됨.\n",
        "\n",
        "# # Copyright 2021 The BigBird Authors.\n",
        "# #\n",
        "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# # you may not use this file except in compliance with the License.\n",
        "# # You may obtain a copy of the License at\n",
        "# #\n",
        "# #     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# #\n",
        "# # Unless required by applicable law or agreed to in writing, software\n",
        "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# # See the License for the specific language governing permissions and\n",
        "# # limitations under the License.\n",
        "\n",
        "# \"\"\"Install BigBird.\"\"\"\n",
        "\n",
        "# import setuptools\n",
        "\n",
        "# # Get install requirements from the REQUIREMENTS file.\n",
        "# with open('/content/bigbird/requirements.txt') as fp:\n",
        "#   _REQUIREMENTS = fp.read().splitlines()\n",
        "\n",
        "# # Get the long description from the README file.\n",
        "# with open('/content/bigbird/README.md') as fp:\n",
        "#   _LONG_DESCRIPTION = fp.read()\n",
        "\n",
        "# setuptools.setup(\n",
        "#     name='bigbird',\n",
        "#     version='0.0.1',\n",
        "#     description='Big Bird: Transformers for Long Sequences',\n",
        "#     long_description=_LONG_DESCRIPTION,\n",
        "#     long_description_content_type='text/markdown',\n",
        "#     author='Google Inc.',\n",
        "#     author_email='no-reply@google.com',\n",
        "#     url='http://github.com/google-research/bigbird',\n",
        "#     license='Apache 2.0',\n",
        "#     packages=[\n",
        "#         'bigbird', 'bigbird.core', 'bigbird.classifier',\n",
        "#         'bigbird.pretrain', 'bigbird.summarization'\n",
        "#     ],\n",
        "#     package_data={'bigbird': ['vocab/*']},\n",
        "#     scripts=[],\n",
        "#     install_requires=_REQUIREMENTS,\n",
        "#     keywords='deeplearning machinelearning nlp classifier qa summarization transformer pretraining',\n",
        "# )\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option -f not recognized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvKpKDV6qibI",
        "outputId": "b8bc281c-14dd-46aa-8ff0-8cccd5a0ada7"
      },
      "source": [
        "! git clone https://github.com/google-research/bigbird.git\n",
        "! cd /content/bigbird\n",
        "! pip3 install -e ."
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /content\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mt4NE3KydX2"
      },
      "source": [
        "# 시스템 경로 임의로 지정\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/bigbird/')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7BoQlHl0N4-",
        "outputId": "88f71ae2-3849-48db-99bc-d94f215bb7a0"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigbird  drive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dRD7zLNkdE4"
      },
      "source": [
        "\"\"\"Run masked LM/next sentence pre-training for BigBird.\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from absl import app\n",
        "from absl import logging\n",
        "from bigbird.core import flags\n",
        "from bigbird.core import modeling\n",
        "from bigbird.core import optimization\n",
        "from bigbird.core import utils\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tft\n",
        "\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pGSzZ_3kgUg"
      },
      "source": [
        "def input_fn_builder(data_dir, vocab_model_file, masked_lm_prob,\n",
        "                     max_encoder_length, max_predictions_per_seq,\n",
        "                     preprocessed_data, substitute_newline, is_training,\n",
        "                     tmp_dir=None):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  sp_model = spm.SentencePieceProcessor()\n",
        "  sp_proto = tf.io.gfile.GFile(vocab_model_file, \"rb\").read()\n",
        "  sp_model.LoadFromSerializedProto(sp_proto)\n",
        "  vocab_size = sp_model.GetPieceSize()\n",
        "  word_start_subtoken = np.array(\n",
        "      [sp_model.IdToPiece(i)[0] == \"▁\" for i in range(vocab_size)])\n",
        "\n",
        "  feature_shapes = {\n",
        "      \"input_ids\": [max_encoder_length],\n",
        "      \"segment_ids\": [max_encoder_length],\n",
        "      \"masked_lm_positions\": [max_predictions_per_seq],\n",
        "      \"masked_lm_ids\": [max_predictions_per_seq],\n",
        "      \"masked_lm_weights\": [max_predictions_per_seq],\n",
        "      \"next_sentence_labels\": [1]\n",
        "  }\n",
        "\n",
        "  def _decode_record(record):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    name_to_features = {\n",
        "        \"input_ids\":\n",
        "            tf.io.FixedLenFeature([max_encoder_length], tf.int64),\n",
        "        \"segment_ids\":\n",
        "            tf.io.FixedLenFeature([max_encoder_length], tf.int64),\n",
        "        \"masked_lm_positions\":\n",
        "            tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
        "        \"masked_lm_ids\":\n",
        "            tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
        "        \"masked_lm_weights\":\n",
        "            tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
        "        \"next_sentence_labels\":\n",
        "            tf.io.FixedLenFeature([1], tf.int64),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.cast(t, tf.int32)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def do_masking(example):\n",
        "    text = example[\"text\"]\n",
        "    tokenizer = tft.SentencepieceTokenizer(\n",
        "        model=tf.io.gfile.GFile(vocab_model_file, \"rb\").read())\n",
        "    if substitute_newline:\n",
        "      text = tf.strings.regex_replace(text, \"\\n\", substitute_newline)\n",
        "    subtokens = tokenizer.tokenize(text)\n",
        "    (subtokens, masked_lm_positions, masked_lm_ids,\n",
        "     masked_lm_weights) = tf.compat.v1.py_func(\n",
        "         numpy_masking, [subtokens], [tf.int32, tf.int32, tf.int32, tf.float32],\n",
        "         stateful=False)\n",
        "    features = {\n",
        "        \"input_ids\": subtokens,\n",
        "        \"segment_ids\": tf.zeros_like(subtokens),\n",
        "        \"masked_lm_positions\": masked_lm_positions,\n",
        "        \"masked_lm_ids\": masked_lm_ids,\n",
        "        \"masked_lm_weights\": masked_lm_weights,\n",
        "        \"next_sentence_labels\": tf.zeros([1], dtype=tf.int64),\n",
        "    }\n",
        "    return features\n",
        "\n",
        "  def numpy_masking(subtokens):\n",
        "    # Find a random span in text\n",
        "    end_pos = max_encoder_length - 2 + np.random.randint(\n",
        "        max(1, len(subtokens) - max_encoder_length - 2))\n",
        "    start_pos = max(0, end_pos - max_encoder_length + 2)\n",
        "    subtokens = subtokens[start_pos:end_pos]\n",
        "\n",
        "    # The start might be inside a word so fix it\n",
        "    # such that span always starts at a word\n",
        "    word_begin_mark = word_start_subtoken[subtokens]\n",
        "    word_begins_pos = np.flatnonzero(word_begin_mark).astype(np.int32)\n",
        "    if word_begins_pos.size == 0:\n",
        "      # if no word boundary present, we do not do whole word masking\n",
        "      # and we fall back to random masking.\n",
        "      word_begins_pos = np.arange(len(subtokens), dtype=np.int32)\n",
        "      word_begin_mark = np.logical_not(word_begin_mark)\n",
        "      print(subtokens, start_pos, end_pos, word_begin_mark)\n",
        "    correct_start_pos = word_begins_pos[0]\n",
        "    subtokens = subtokens[correct_start_pos:]\n",
        "    word_begin_mark = word_begin_mark[correct_start_pos:]\n",
        "    word_begins_pos = word_begins_pos - correct_start_pos\n",
        "    num_tokens = len(subtokens)\n",
        "\n",
        "    # @'e' want to do whole word masking so split by word boundary\n",
        "    words = np.split(np.arange(num_tokens, dtype=np.int32), word_begins_pos)[1:]\n",
        "    assert len(words) == len(word_begins_pos)\n",
        "\n",
        "    # Decide elements to mask\n",
        "    num_to_predict = min(\n",
        "        max_predictions_per_seq,\n",
        "        max(1, int(round(len(word_begins_pos) * masked_lm_prob))))\n",
        "    masked_lm_positions = np.concatenate(np.random.choice(\n",
        "        np.array([[]] + words, dtype=np.object)[1:],\n",
        "        num_to_predict, replace=False), 0)\n",
        "    # but this might have excess subtokens than max_predictions_per_seq\n",
        "    if len(masked_lm_positions) > max_predictions_per_seq:\n",
        "      masked_lm_positions = masked_lm_positions[:max_predictions_per_seq+1]\n",
        "      # however last word can cross word boundaries, remove crossing words\n",
        "      truncate_masking_at = np.flatnonzero(\n",
        "          word_begin_mark[masked_lm_positions])[-1]\n",
        "      masked_lm_positions = masked_lm_positions[:truncate_masking_at]\n",
        "\n",
        "    # sort masking positions\n",
        "    masked_lm_positions = np.sort(masked_lm_positions)\n",
        "    masked_lm_ids = subtokens[masked_lm_positions]\n",
        "\n",
        "    # replance input token with [MASK] 80%, random 10%, or leave it as it is.\n",
        "    randomness = np.random.rand(len(masked_lm_positions))\n",
        "    mask_index = masked_lm_positions[randomness < 0.8]\n",
        "    random_index = masked_lm_positions[randomness > 0.9]\n",
        "\n",
        "    subtokens[mask_index] = 67  # id of masked token\n",
        "    subtokens[random_index] = np.random.randint(  # ignore special tokens\n",
        "        101, vocab_size, len(random_index), dtype=np.int32)\n",
        "\n",
        "    # add [CLS] (65) and [SEP] (66) tokens\n",
        "    subtokens = np.concatenate([\n",
        "        np.array([65], dtype=np.int32), subtokens,\n",
        "        np.array([66], dtype=np.int32)\n",
        "    ])\n",
        "\n",
        "    # pad everything to correct shape\n",
        "    pad_inp = max_encoder_length - num_tokens - 2\n",
        "    subtokens = np.pad(subtokens, [0, pad_inp], \"constant\")\n",
        "\n",
        "    pad_out = max_predictions_per_seq - len(masked_lm_positions)\n",
        "    masked_lm_weights = np.pad(\n",
        "        np.ones_like(masked_lm_positions, dtype=np.float32),\n",
        "        [0, pad_out], \"constant\")\n",
        "    masked_lm_positions = np.pad(\n",
        "        masked_lm_positions + 1, [0, pad_out], \"constant\")\n",
        "    masked_lm_ids = np.pad(masked_lm_ids, [0, pad_out], \"constant\")\n",
        "\n",
        "    return subtokens, masked_lm_positions, masked_lm_ids, masked_lm_weights\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # Load dataset and handle tfds separately\n",
        "    split = \"train\" if is_training else \"test\"\n",
        "    if \"tfds://\" == data_dir[:7]:\n",
        "      d = tfds.load(data_dir[7:], split=split,\n",
        "                    shuffle_files=is_training,\n",
        "                    data_dir=tmp_dir)\n",
        "    else:\n",
        "      input_files = tf.io.gfile.glob(\n",
        "          os.path.join(data_dir, \"{}.tfrecord*\".format(split)))\n",
        "\n",
        "      # For training, we want a lot of parallel reading and shuffling.\n",
        "      # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "      if is_training:\n",
        "        d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n",
        "        d = d.shuffle(buffer_size=len(input_files))\n",
        "\n",
        "        # Non deterministic mode means that the interleaving is not exact.\n",
        "        # This adds even more randomness to the training pipeline.\n",
        "        d = d.interleave(tf.data.TFRecordDataset,\n",
        "                         deterministic=False,\n",
        "                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "      else:\n",
        "        d = tf.data.TFRecordDataset(input_files)\n",
        "\n",
        "    if preprocessed_data:\n",
        "      d = d.map(_decode_record,\n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    else:\n",
        "      d = d.map(do_masking,\n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if is_training:\n",
        "      d = d.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
        "      d = d.repeat()\n",
        "\n",
        "    d = d.padded_batch(batch_size, feature_shapes,\n",
        "                       drop_remainder=True)  # For static shape\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ4SqPgH2K38"
      },
      "source": [
        "def serving_input_fn_builder(batch_size, max_encoder_length,\n",
        "                             vocab_model_file, substitute_newline):\n",
        "  \"\"\"Creates an `input_fn` closure for exported SavedModel.\"\"\"\n",
        "  def dynamic_padding(inp, min_size):\n",
        "    pad_size = tf.maximum(min_size - tf.shape(inp)[1], 0)\n",
        "    paddings = [[0, 0], [0, pad_size]]\n",
        "    return tf.pad(inp, paddings)\n",
        "\n",
        "  def input_fn():\n",
        "    # text input\n",
        "    text = tf.compat.v1.placeholder(tf.string, [batch_size], name=\"input_text\")\n",
        "\n",
        "    # text tokenize\n",
        "    tokenizer = tft.SentencepieceTokenizer(\n",
        "        model=tf.io.gfile.GFile(vocab_model_file, \"rb\").read())\n",
        "    if substitute_newline:\n",
        "      text = tf.strings.regex_replace(text, \"\\n\", substitute_newline)\n",
        "    ids = tokenizer.tokenize(text)\n",
        "    if isinstance(ids, tf.RaggedTensor):\n",
        "      ids = ids.to_tensor(0)\n",
        "\n",
        "    # text padding: Pad only if necessary and reshape properly\n",
        "    padded_ids = dynamic_padding(ids, max_encoder_length)\n",
        "    ids = tf.slice(padded_ids, [0, 0], [batch_size, max_encoder_length])\n",
        "\n",
        "    receiver_tensors = {\"input\": text}\n",
        "    features = {\"input_ids\": tf.cast(ids, tf.int32, name=\"input_ids\")}\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        features=features, receiver_tensors=receiver_tensors)\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUVsbqVc3Rzl"
      },
      "source": [
        "def model_fn_builder(bert_config):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    model = modeling.BertModel(bert_config)\n",
        "    masked_lm = MaskedLMLayer(\n",
        "        bert_config[\"hidden_size\"], bert_config[\"vocab_size\"], model.embeder,\n",
        "        initializer=utils.create_initializer(bert_config[\"initializer_range\"]),\n",
        "        activation_fn=utils.get_activation(bert_config[\"hidden_act\"]))\n",
        "    next_sentence = NSPLayer(\n",
        "        bert_config[\"hidden_size\"],\n",
        "        initializer=utils.create_initializer(bert_config[\"initializer_range\"]))\n",
        "\n",
        "    sequence_output, pooled_output = model(\n",
        "        features[\"input_ids\"], training=is_training,\n",
        "        token_type_ids=features.get(\"segment_ids\"))\n",
        "\n",
        "    masked_lm_loss, masked_lm_log_probs = masked_lm(\n",
        "        sequence_output,\n",
        "        label_ids=features.get(\"masked_lm_ids\"),\n",
        "        label_weights=features.get(\"masked_lm_weights\"),\n",
        "        masked_lm_positions=features.get(\"masked_lm_positions\"))\n",
        "\n",
        "    next_sentence_loss, next_sentence_log_probs = next_sentence(\n",
        "        pooled_output, features.get(\"next_sentence_labels\"))\n",
        "\n",
        "    total_loss = masked_lm_loss\n",
        "    if bert_config[\"use_nsp\"]:\n",
        "      total_loss += next_sentence_loss\n",
        "\n",
        "    tvars = tf.compat.v1.trainable_variables()\n",
        "    utils.log_variables(tvars, bert_config[\"ckpt_var_list\"])\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      learning_rate = optimization.get_linear_warmup_linear_decay_lr(\n",
        "          init_lr=bert_config[\"learning_rate\"],\n",
        "          num_train_steps=bert_config[\"num_train_steps\"],\n",
        "          num_warmup_steps=bert_config[\"num_warmup_steps\"])\n",
        "\n",
        "      optimizer = optimization.get_optimizer(bert_config, learning_rate)\n",
        "\n",
        "      global_step = tf.compat.v1.train.get_global_step()\n",
        "\n",
        "      gradients = optimizer.compute_gradients(total_loss, tvars)\n",
        "      train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
        "\n",
        "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          host_call=utils.add_scalars_to_summary(\n",
        "              bert_config[\"output_dir\"], {\"learning_rate\": learning_rate}))\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(masked_lm_loss_value, masked_lm_log_probs, masked_lm_ids,\n",
        "                    masked_lm_weights, next_sentence_loss_value,\n",
        "                    next_sentence_log_probs, next_sentence_labels):\n",
        "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
        "        masked_lm_predictions = tf.argmax(\n",
        "            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
        "        masked_lm_accuracy = tf.compat.v1.metrics.accuracy(\n",
        "            labels=masked_lm_ids,\n",
        "            predictions=masked_lm_predictions,\n",
        "            weights=masked_lm_weights)\n",
        "        masked_lm_mean_loss = tf.compat.v1.metrics.mean(\n",
        "            values=masked_lm_loss_value)\n",
        "\n",
        "        next_sentence_predictions = tf.argmax(\n",
        "            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
        "        next_sentence_accuracy = tf.compat.v1.metrics.accuracy(\n",
        "            labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
        "        next_sentence_mean_loss = tf.compat.v1.metrics.mean(\n",
        "            values=next_sentence_loss_value)\n",
        "\n",
        "        return {\n",
        "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
        "            \"masked_lm_loss\": masked_lm_mean_loss,\n",
        "            \"next_sentence_accuracy\": next_sentence_accuracy,\n",
        "            \"next_sentence_loss\": next_sentence_mean_loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn, [\n",
        "          masked_lm_loss, masked_lm_log_probs, features[\"masked_lm_ids\"],\n",
        "          features[\"masked_lm_weights\"], next_sentence_loss,\n",
        "          next_sentence_log_probs, features[\"next_sentence_labels\"]\n",
        "      ])\n",
        "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics)\n",
        "    else:\n",
        "\n",
        "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\n",
        "              \"log-probabilities\": masked_lm_log_probs,\n",
        "              \"seq-embeddings\": sequence_output\n",
        "          })\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3fgTfCn3Yu0"
      },
      "source": [
        "class MaskedLMLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size,\n",
        "               vocab_size,\n",
        "               embeder,\n",
        "               initializer=None,\n",
        "               activation_fn=None,\n",
        "               name=\"cls/predictions\"):\n",
        "    super(MaskedLMLayer, self).__init__(name=name)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embeder = embeder\n",
        "\n",
        "    # We apply one more non-linear transformation before the output layer.\n",
        "    # This matrix is not used after pre-training.\n",
        "    self.extra_layer = utils.Dense2dLayer(\n",
        "        hidden_size, hidden_size, initializer,\n",
        "        activation_fn, \"transform\")\n",
        "    self.norm_layer = utils.NormLayer(hidden_size, name=\"transform\")\n",
        "\n",
        "    # The output weights are the same as the input embeddings, but there is\n",
        "    # an output-only bias for each token.\n",
        "    self.output_bias = tf.compat.v1.get_variable(\n",
        "        name+\"/output_bias\",\n",
        "        shape=[vocab_size],\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "  @property\n",
        "  def trainable_weights(self):\n",
        "    self._trainable_weights = (self.extra_layer.trainable_weights +\n",
        "                               self.norm_layer.trainable_weights +\n",
        "                               [self.output_bias])\n",
        "    return self._trainable_weights\n",
        "\n",
        "  def call(self, input_tensor,\n",
        "           label_ids=None,\n",
        "           label_weights=None,\n",
        "           masked_lm_positions=None):\n",
        "    if masked_lm_positions is not None:\n",
        "      input_tensor = tf.gather(input_tensor, masked_lm_positions, batch_dims=1)\n",
        "\n",
        "    # We apply one more non-linear transformation before the output layer.\n",
        "    # This matrix is not used after pre-training.\n",
        "    input_tensor = self.extra_layer(input_tensor)\n",
        "    input_tensor = self.norm_layer(input_tensor)\n",
        "\n",
        "    # The output weights are the same as the input embeddings, but there is\n",
        "    # an output-only bias for each token.\n",
        "    logits = self.embeder.linear(input_tensor)\n",
        "    logits = tf.nn.bias_add(logits, self.output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    if label_ids is not None:\n",
        "      one_hot_labels = tf.one_hot(\n",
        "          label_ids, depth=self.vocab_size, dtype=tf.float32)\n",
        "\n",
        "      # The `positions` tensor might be zero-padded (if the sequence is too\n",
        "      # short to have the maximum number of predictions). The `label_weights`\n",
        "      # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
        "      # padding predictions.\n",
        "      per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n",
        "      numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
        "      denominator = tf.reduce_sum(label_weights) + 1e-5\n",
        "      loss = numerator / denominator\n",
        "    else:\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "    return loss, log_probs"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkMGJmH03eOd"
      },
      "source": [
        "class NSPLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size,\n",
        "               initializer=None,\n",
        "               name=\"cls/seq_relationship\"):\n",
        "    super(NSPLayer, self).__init__(name=name)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n",
        "    # \"random sentence\". This weight matrix is not used after pre-training.\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.output_weights = tf.compat.v1.get_variable(\n",
        "          \"output_weights\",\n",
        "          shape=[2, hidden_size],\n",
        "          initializer=initializer)\n",
        "      self._trainable_weights.append(self.output_weights)\n",
        "      self.output_bias = tf.compat.v1.get_variable(\n",
        "          \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
        "      self._trainable_weights.append(self.output_bias)\n",
        "\n",
        "  def call(self, input_tensor, next_sentence_labels=None):\n",
        "    logits = tf.matmul(input_tensor, self.output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, self.output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    if next_sentence_labels is not None:\n",
        "      labels = tf.reshape(next_sentence_labels, [-1])\n",
        "      one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
        "      per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "      loss = tf.reduce_mean(per_example_loss)\n",
        "    else:\n",
        "      loss = tf.constant(0.0)\n",
        "    return loss, log_probs"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIAkI17H3hZ8"
      },
      "source": [
        "def main(_):\n",
        "\n",
        "  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_export:\n",
        "    raise ValueError(\n",
        "        \"At least one of `do_train`, `do_eval` must be True.\")\n",
        "\n",
        "  bert_config = flags.as_dictionary()\n",
        "\n",
        "  if FLAGS.max_encoder_length > bert_config[\"max_position_embeddings\"]:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (FLAGS.max_encoder_length, bert_config[\"max_position_embeddings\"]))\n",
        "\n",
        "  tf.io.gfile.makedirs(FLAGS.output_dir)\n",
        "  if FLAGS.do_train:\n",
        "    flags.save(os.path.join(FLAGS.output_dir, \"pretrain.config\"))\n",
        "\n",
        "  model_fn = model_fn_builder(bert_config)\n",
        "  estimator = utils.get_estimator(bert_config, model_fn)\n",
        "  tmp_data_dir = os.path.join(FLAGS.output_dir, \"tfds\")\n",
        "\n",
        "  if FLAGS.do_train:\n",
        "    logging.info(\"***** Running training *****\")\n",
        "    logging.info(\"  Batch size = %d\", estimator.train_batch_size)\n",
        "    logging.info(\"  Num steps = %d\", FLAGS.num_train_steps)\n",
        "    train_input_fn = input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        masked_lm_prob=FLAGS.masked_lm_prob,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        max_predictions_per_seq=FLAGS.max_predictions_per_seq,\n",
        "        preprocessed_data=FLAGS.preprocessed_data,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        tmp_dir=tmp_data_dir,\n",
        "        is_training=True)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n",
        "\n",
        "  if FLAGS.do_eval:\n",
        "    logging.info(\"***** Running evaluation *****\")\n",
        "    logging.info(\"  Batch size = %d\", estimator.eval_batch_size)\n",
        "\n",
        "    eval_input_fn = input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        masked_lm_prob=FLAGS.masked_lm_prob,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        max_predictions_per_seq=FLAGS.max_predictions_per_seq,\n",
        "        preprocessed_data=FLAGS.preprocessed_data,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        tmp_dir=tmp_data_dir,\n",
        "        is_training=False)\n",
        "\n",
        "    # Run continuous evaluation for latest checkpoint as training progresses.\n",
        "    last_evaluated = None\n",
        "    while True:\n",
        "      latest = tf.train.latest_checkpoint(FLAGS.output_dir)\n",
        "      if latest == last_evaluated:\n",
        "        if not latest:\n",
        "          logging.info(\"No checkpoints found yet.\")\n",
        "        else:\n",
        "          logging.info(\"Latest checkpoint %s already evaluated.\", latest)\n",
        "        time.sleep(300)\n",
        "        continue\n",
        "      else:\n",
        "        logging.info(\"Evaluating check point %s\", latest)\n",
        "        last_evaluated = latest\n",
        "\n",
        "        current_step = int(os.path.basename(latest).split(\"-\")[1])\n",
        "        output_eval_file = os.path.join(\n",
        "            FLAGS.output_dir, \"eval_results_{}.txt\".format(current_step))\n",
        "        result = estimator.evaluate(input_fn=eval_input_fn,\n",
        "                                    steps=FLAGS.max_eval_steps,\n",
        "                                    checkpoint_path=latest)\n",
        "\n",
        "        with tf.io.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "          logging.info(\"***** Eval results *****\")\n",
        "          for key in sorted(result.keys()):\n",
        "            logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "  if FLAGS.do_export:\n",
        "    logging.info(\"***** Running export *****\")\n",
        "\n",
        "    serving_input_fn = serving_input_fn_builder(\n",
        "        batch_size=FLAGS.eval_batch_size,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline)\n",
        "\n",
        "    estimator.export_saved_model(\n",
        "        os.path.join(FLAGS.output_dir, \"export\"), serving_input_fn)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Vx6pTQAd3rXs",
        "outputId": "1355af78-429b-4ce4-a29a-8c30522cd62c"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  tf.compat.v1.disable_v2_behavior()\n",
        "  tf.compat.v1.enable_resource_variables()\n",
        "  app.run(main)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FATAL Flags parsing error: Unknown command line flag 'f'\n",
            "Pass --helpshort or --helpfull to see help on flags.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFJCARLK3wlV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}