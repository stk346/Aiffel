{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sharp-mongolia",
   "metadata": {},
   "source": [
    "matplotlib 라이브러리는 기본 폰트로 한국어를 지원하지 않습니다. 따라서 올바른 Attention Map을 확인하기 위해 폰트를 변경해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distributed-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-shadow",
   "metadata": {},
   "source": [
    "## 내부 모듈 구현하기\n",
    "이번 코스는 트랜스포머를 완성하는 데에 필요한 모듈들을 하나하나 만든 후, 조립하여 완성하는 방식으로 진행할 겁니다.  \n",
    "Tensor로 변환한 입력 데이터가 주어졌다고 가정하고 흐름을 생각해 봅시다. 최초의 텍스트 입력 데이터는 [ batch_size x length ] 의 형태를 가지고 있겠죠? 번역이 끝나고 난 최종 출력 데이터는 [ batch_size x length x vocab_size ]의 형태를 가지게 됩니다. 번역 문제는 결국 매 스텝마다 vocab_size 만큼의 클래스 개수 중에 적당한 단어를 선택하는 작업을 length만큼 반복하는 것이니까요. 모델 구성하는 과정에서 레이어를 통과할 때마다 텐서의 shape가 어떻게 바뀌는지를 눈여겨봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-karaoke",
   "metadata": {},
   "source": [
    "1. 입력 데이터 > [ batch_size x length ]  \n",
    "2. Source & Target Embeddung > [batch_size x length x d_emb]  \n",
    "3. **Positional Encoding** 강의 노드에서 구현했듯이 2번 결과에 더해지므로 shape 변화는 없습니다.  \n",
    "4. **Multi-Head Attention** 아래와 같이 여러 개의 서브 모듈들이 존재합니다.  \n",
    "    1) **Split Heads** > [batch_size x length x heads x (d_emb / n_heads)]\n",
    "    2) **Masking for Masked Attention**\n",
    "    3) **Scaled Dot Product Attention**\n",
    "    4) **Combine Heads** > [batch_size x length x d_emb]\n",
    "5. Residual Connection\n",
    "6. Layer Normalization\n",
    "7. **Position-wise-Feed-Forward Netword** > [batch_size x length x d_ft]\n",
    "8. Output Linear Layer > [batch_size x length x vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-visibility",
   "metadata": {},
   "source": [
    " 굵게 표시된 모듈을 제외하면 나머지는 텐서플로우 내부에 이미 구현체가 포함되어 있어서 간단하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-elimination",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-retailer",
   "metadata": {},
   "source": [
    "수식  \n",
    "$PE(pos,2i) = sin(pos/10000^{2i/{d_{model}}})$  \n",
    "$PE(pos,2i+1) = cos(pos/10000^{2i/{d_{model}}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # 짝수 번째 자리수 position\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # 홀수 번째 자리수 position\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-light",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "Multi-Head Attention은 여러 개의 서브 모듈을 결합하여 완성됩니다.  \n",
    "* Embedding된 입력을 Head 수로 분할하는 split_heads()  \n",
    "* 분할된 입력으로부터 Attention 값을 구하는 scaled_dot_product_attention()  \n",
    "* 연산이 종료되고 분할된 Head를 다시 하나로 결합시켜주는 combine_heads()  \n",
    "  \n",
    "MultiHeadAttention 클래스를 정의하여 모두 포함시켜 주겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-victory",
   "metadata": {},
   "source": [
    "방금 설명에서는 Masking이 빠져있는데, 마스크의 형태를 결정하는 것은 모델 외부의 훈련 데이터입니다. 따라서 이를 생성하는 함수는 MultiHeadAttention 외부에 정의되어야 합니다. 하지만 생성된 마스크를 처리할 수 있도록  \n",
    "**scaled_dot_product_attention() 함수 내부에 if mask is not None: scaled_qk += (mask * -1e9) 코드를 포함시켜 주겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-charleston",
   "metadata": {},
   "source": [
    "수식  \n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})*V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animal-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1].tf.float32) # tf.cast(): tensor로 변환\n",
    "        \n",
    "        \"\"\"\n",
    "        Scaled QK 값 구하기\n",
    "        \"\"\"\n",
    "        QK = tf.matmul(Q, K, transpose_b=True) ## tf.matmul(a, b, transpose_a, transpose_b) \n",
    "        \n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "            \n",
    "        \"\"\"\n",
    "        1. Attention Weights 값 구하기 -> attentions\n",
    "        2. Attention 값을 V에 곱하기 -> out\n",
    "        \"\"\"\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1) # tf.nn: 원시 신경망(NN) 작업을 위한 래퍼\n",
    "        out = tf.matmul(attentions, V)\n",
    "        \n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 head의 수로 분할하는 함수\n",
    "        \n",
    "        x: [batch x length x emb]\n",
    "        return: [batch x length x heads x self.depth]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3]) # perm은 3차원 이상의 행렬을 뒤집을 때 유용하다.\n",
    "        \n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "        \n",
    "        x: [batch x length x heads x self.depth]\n",
    "        return [batch x length x emb]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "        \n",
    "        Step1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split) -> out, attention_weights\n",
    "        Step4: Combine Heads(out) -> out\n",
    "        Step5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q) # Demse layer 만들기\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ) # linear layer 통과\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(  # scaled-dot-product 연산\n",
    "            WQ_splits, WK_split, WV_splits, mask)\n",
    "        \n",
    "        out = self.combine_heads(out) # 다시 모아준 이후\n",
    "        out = self.linear(out) # linear 레이어 통과해 적절하게 분배\n",
    "        \n",
    "        return out, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-bruce",
   "metadata": {},
   "source": [
    "이론을 조금 이해한 상태에서 봤는데 이해가 잘 안된다. 행렬의 shape도 잘 모르겠고 perm에서 차원이 어떻게 변형되는지 등 세부적인 것들이 햇갈린다. 노드를 끝까지 본 이후에 다시 공부해봐야겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-yahoo",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "outstanding-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__() # 객체에 할당하면서 초기화, 오버라이드 될 수 있게 만들어줌. f1\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu') # 활성화함수 통과. f2.\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model) # linear layer 통과. f3\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-percentage",
   "metadata": {},
   "source": [
    "d_ff 는 논문의 설명대로라면 2048 일 거고, d_model 은 512 겠죠? [ batch x length x d_model ] 의 입력을 받아 w_1 이 2048차원으로 매핑하고 활성함수 ReLU를 적용한 후, 다시 w_2 를 통해 512차원으로 되돌리는 과정까지 수행했습니다.  \n",
    "* imput = 512, 은닉층 = 4, output layer = 512인것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-development",
   "metadata": {},
   "source": [
    "## 모듈 조립하기\n",
    "이 모든 모듈들을 가지고 트랜스포머를 완성할 수 있는데, 정확하게는 트랜스포머의 Encoder 한 층과 Decoder 한 층을 각각 완성할 수 있습니다!  \n",
    "transformer 논문의 층은 총 6개입니다. 측, 레이어 수를 원하는 만큼 쌓을 수 있어야 하죠. 이런 실험이 가능하게 하려면 모델이 동적으로 완성될 수 있게끔 해야 합니다.  \n",
    "방법은 단순합니다, 마치 텐서플로우의 Dense 레이어를 사용하듯이 EncoderLayer, DecoderLayer를 쓸 수 있게 tf.keras.layers.Layer 클래스를 상속받아 레이어 클래스로 정의해 주면 됩니다. MultiHeadAttention 이 그렇게 정의된 레이어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-discipline",
   "metadata": {},
   "source": [
    "N = 10  \n",
    "  \n",
    "**10개의 Linear Layer를 한 방에!**  \n",
    "linear_layers = [tf.keras.layers.Dense(30) for _ in range(N)]  \n",
    "  \n",
    "**10개의 Encoder Layer도 한 방에!**  \n",
    "enc_layers = [TransformerEncoderLayer(30) for _ in range(N)]    \n",
    "  \n",
    "이런 동적인 방식이 낯설다면 지금부터라도 익숙해지시길 강력하게 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-drain",
   "metadata": {},
   "source": [
    "### Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "offshore-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_self_attn = MuntiHeadAttention(d_model, n_heads) # MuntiHeadAttention class 상속\n",
    "        self.ffn = PosWiseFeedForwardNet(d_model, d_ff) # PoswiseFeedForwardNet class 상속\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6) # 인코더는 norm 레이어가 2개\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x # 잔차 할당\n",
    "        out = self.norm_1(x) # 첫 번째 norm layer\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask) # Multi-Head에 input 3개\n",
    "        out = self.dropout(out) # ?\n",
    "        out += residual # 잔차 더해주기\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out # 잔차 할당\n",
    "        out = self.norm_2(out) # 2번째 norm layer\n",
    "        out = self.ffn(out) # Feed Forward 통과\n",
    "        out = self.dropout(out) # ?\n",
    "        out += residual # 잔차 더해주기\n",
    "        \n",
    "        return out, enc_attn # decoder Multi-Head Attention으로 들어가는 2개 input 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-attitude",
   "metadata": {},
   "source": [
    "Transformer의 구현은 정말 많은데, 그중에서 Normalization Layer의 위치에 대한 논의가 종종 나온답니다. 실제 논문에서는 [Input] - [Module] - [Residual] - [Norm] (Module = MHA, FFN)으로 표현되어 있지만 정작 Official 구현인 구글의 Tensor2Tensor 에서는 [Input] - [Norm] - [Module] - [Residual] 방식을 사용했어요. \n",
    "  \n",
    "경험에 따르면 레이어가 많아질수록 후자가 약간 더 좋은 성능을 보였기에 필자는 논문 대신 Official 구현을 따르길 권장합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads) # MuntiHeadAttention class 상속\n",
    "        self.enc_dec_atten = MultiHeadAttention(d_model, n_heads) # 중간 단계에 들어가는 Attention\n",
    "        \n",
    "        self.ffn = PosWiseFeedForwardNet(d_model, d_ff) # PoswiseFeedForwardNet class 상속\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6) # 디코더는 norm 레이어가 3개\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "            \"\"\"\n",
    "            Masked-Multi-Head Attention\n",
    "            \"\"\"\n",
    "            residual = x # 잔차 할당\n",
    "            out = self.norm_1(x) # 첫 번째 norm layer\n",
    "            out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "            self.dropout(out)\n",
    "            out += residual # 잔차 더해주기\n",
    "            \n",
    "            \"\"\"\n",
    "            Multi-Head Attention\n",
    "            \"\"\"\n",
    "            residual = out\n",
    "            out = self.norm_2(out)\n",
    "            out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask) # 디코더 out, 인코더 out2개\n",
    "            out - self.dropout(out)\n",
    "            out += residual\n",
    "            \n",
    "            \"\"\"\n",
    "            Position-Wise Feed Forward Network\n",
    "            \"\"\"\n",
    "            residual = out\n",
    "            out = self.norm_3(out)\n",
    "            out = self.ffn(out)\n",
    "            out = self.dropout(out)\n",
    "            out += residual\n",
    "            \n",
    "            return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alone-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,  # 상속시마다 초기화 되는 파라미터\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                dropout):\n",
    "        super(Encoder, self).__init__() # 자기 자신을 오버라이딩?\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                          for _ in range(n_layers)] # EncoderLayer를 n_layer만큼 반복?\n",
    "        \n",
    "        def call(self, x, mask):\n",
    "            out = x\n",
    "            \n",
    "            enc_attns = list()\n",
    "            for i in range(self.n_layers):\n",
    "                out, enc_attn = self.enc_layers[i](out, mask)\n",
    "                enc_attns.append(enc_attn)\n",
    "                \n",
    "            return out, enc_attns # out은 누적되고 enc_attn은 list에 기록되는 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "technical-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                          for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            \n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "            \n",
    "        return out, dec_attens, dec_enc_attens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-contamination",
   "metadata": {},
   "source": [
    "### Transformer 완성하기\n",
    "정의된 Encoder 와 Decoder 를 가지고 최종적으로 트랜스포머를 완성합니다.  \n",
    "  \n",
    "1. shared 변수를 매개변수로 받아 True 일 경우 Decoder Embedding과 출력층 Linear의 Weight를 공유할 수 있게 하세요! Weight가 공유될 경우 Embedding 값에 sqrt(d_model)을 곱해줘야 하는 것, 잊지 않으셨죠? (참고: tf.keras.layers.Layer.set_weights())  \n",
    "  \n",
    "2. 우리가 정의한 positional_encoding 의 반환값 형태는 [ Length x d_model ] 인데, 이를 더해 줄 Embedding 값 형태가 [ Batch x Length x d_model ] 이라서 연산이 불가능합니다. 연산이 가능하도록 수정하세요! (참고: tf.expand_dims(), np.newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "purple-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                src_vocab_size,\n",
    "                tgt_vocab_size,\n",
    "                pos_len,\n",
    "                dropout=0.2,\n",
    "                shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        \"\"\"\n",
    "        1. Embedding Layer 정의\n",
    "        2. Positional Encoding 정의\n",
    "        3. Encoder / Decoder 정의\n",
    "        4. Output Linear 정의\n",
    "        5. Shared Weights\n",
    "        6. Dropout 정의\n",
    "        \"\"\"\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "    def embadding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding + Shared일 경우 Scaling 작업 포함\n",
    "        \n",
    "        x: [batch x length]\n",
    "        return: [batch x length x emb]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1] # (Pos * d_model) 이니까\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "            \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :] #  [ Length x d_model ] ->  [ Batch x Length x d_model ] 형태로 변환해줌\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요\n",
    "        \n",
    "        Step1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        Step2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        Step3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        Step4: Out Linear(dec_out) -> logits\n",
    "        \"\"\"\n",
    "        enc_in = self.embedding(self, enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self, dec_emb, dec_in)\n",
    "        \n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-honolulu",
   "metadata": {},
   "source": [
    "* shared가 정확히 뭘하는건지 모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-azerbaijan",
   "metadata": {},
   "source": [
    "## 모델 밖의 조력자들\n",
    "Masking을 살펴볼 시간이 다가왔습니다. 그리고 트랜스포머의 Learning Rate가 일반적이지 않다는 것도 기억하고 계실 거예요! 지금부터는 모델 외적인 부분을 정의해 주도록 하겠습니다. 이번 스텝에서는 데이터의 특성이나 학습 과정에 따라 달라지는 부분을 다루게 됩니다.  \n",
    "  \n",
    "먼저 Masking입니다. 이전 노드에서 배운 generate_causality_mask() 를 그대로 사용하면 되는데, 약간 추가할 내용이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "talented-behalf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 1, 25) (35, 25)\n",
      "(16, 1, 35, 35) (35, 35)\n"
     ]
    }
   ],
   "source": [
    "enc_mask, dec_enc_mask, dec_mask = generate_masks(sample_src, sample_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "variable-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0) \n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src,tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask) \n",
    "    print(enc_mask.shape, dec_enc_causality_mask.shape)\n",
    "    \n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "    print(dec_mask.shape, dec_causality_mask.shape)\n",
    "    \n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-waste",
   "metadata": {},
   "source": [
    "generate_padding_mask() 는 Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할을 합니다. 일전에 Sequence-to-Sequence 모델에서 Loss에 대한 Masking을 해줄 때도 위와 같은 방법으로 진행했죠? 한 배치의 데이터에서 <PAD> 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성합니다. 눈으로 직접 확인해보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dynamic-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "    print(enc_mask.shape, dec_enc_causality_mask.shape)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "    print(dec_mask.shape, dec_causality_mask.shape)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "going-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 1, 25) (35, 25)\n",
      "(16, 1, 35, 35) (35, 35)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 1, 25)\n",
      "(16, 1, 35, 25)\n",
      "(16, 1, 35, 35)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2') \n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(enc_mask.numpy().shape)\n",
    "print(dec_enc_mask.numpy().shape)\n",
    "print(dec_mask.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-newman",
   "metadata": {},
   "source": [
    "dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask) 에서 (16, 1, 1, 25), (35, 25)끼리 shape가 다른데 어떻게 연산이 되는건지 매우 궁금하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-reminder",
   "metadata": {},
   "source": [
    "첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태임을 알 수 있습니다. 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데... 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태입니다! 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹을 했던 것을 기억하시죠? 인과 관계를 가리는 것도 중요하지만 Decoder 역시 <PAD> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된답니다!  \n",
    "  \n",
    "또, 트랜스포머는 고정된 Learning Rate를 사용하지 않았었죠! 논문의 해당 부분을 Optimizer까지 포함하여 다시 한번 살펴봅시다. 이전 노드에서 Learning Rate를 numpy 로 간단히 구현을 했었는데, 이번엔 Tensorflow 상에서 잘 구동될 수 있도록 LearningRateSchedule 클래스를 상속받아 구현해보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "reported-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                    beta_1 = 0.9,\n",
    "                                    beta_2 = 0.98,\n",
    "                                    epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-equation",
   "metadata": {},
   "source": [
    "트랜스포머가 제안한 수식이 아니더라도 가변적인 Learning Rate를 사용하려면 위와 같이 구현을 하시면 됩니다. Optimizer는 논문에 정의된 대로 Adam Optimizer를 사용하며 세부 파라미터도 동일하게 맞춰 줍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
