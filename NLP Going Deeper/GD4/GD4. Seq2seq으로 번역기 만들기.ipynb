{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-passing",
   "metadata": {},
   "source": [
    "이전에 멋진 Sequence-to-Sequence 구조를 배웠습니다. 두 개의 RNN 모듈을 Encoder-Decoder 구조로 결합하여 사용하는 Seq2seq은 그야말로 혁신이었습니다.  \n",
    "Seq2seq이 번역기에 최적화되어 있는 만큼, 이번 코스에선 Seq2seq 기반 번역기를 직접 만들어보며 그 구조를 이해해보도록 하겠습니다. 또한 Attention 기법을 추가하여 성능을 높여보기도 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-eclipse",
   "metadata": {},
   "source": [
    "실습에서는 한국어가 포함된 말뭉치를 사용하므로, 한국어를 잘 시각화하기 위한 준비가 필요합니다. 다만 matplotlib 라이브러리의 기본 폰트는 한국어를 지원하지 않아요! 올바른 Attention Map을 확인하기 위해 한국어를 지원하는 폰트로 변경해 주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "occupied-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 터미널에서 나눔 글꼴을 설치합니다.\n",
    "# $ sudo apt -qq -y install fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "occupational-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-verification",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-encounter",
   "metadata": {},
   "source": [
    "### 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "hungry-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-omaha",
   "metadata": {},
   "source": [
    "데이터를 다운로드하는 데에는 텐서플로우에서 제공하는 tf.keras.utils.get_file() 함수를 사용할 겁니다. get_file()함수는 URL로부터 데이터를 다운받고, 압축된 형식일 경우 해제까지 알아서 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "thermal-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "professional-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "    \n",
    "    print(\"Data Size:\", len(raw))\n",
    "    print(\"Example:\")\n",
    "    \n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cultural-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVe.',\n",
       " 'Go.\\tVete.',\n",
       " 'Go.\\tVaya.',\n",
       " 'Go.\\tVáyase.',\n",
       " 'Hi.\\tHola.',\n",
       " 'Run!\\t¡Corre!',\n",
       " 'Run.\\tCorred.',\n",
       " 'Who?\\t¿Quién?',\n",
       " 'Fire!\\t¡Fuego!',\n",
       " 'Fire!\\t¡Incendio!',\n",
       " 'Fire!\\t¡Disparad!',\n",
       " 'Help!\\t¡Ayuda!',\n",
       " 'Help!\\t¡Socorro! ¡Auxilio!',\n",
       " 'Help!\\t¡Auxilio!',\n",
       " 'Jump!\\t¡Salta!',\n",
       " 'Jump.\\tSalte.',\n",
       " 'Stop!\\t¡Parad!',\n",
       " 'Stop!\\t¡Para!',\n",
       " 'Stop!\\t¡Pare!',\n",
       " 'Wait!\\t¡Espera!']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-anaheim",
   "metadata": {},
   "source": [
    "### 데이터 전처리: 정제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-chess",
   "metadata": {},
   "source": [
    "데이터는 \\t 기호를 기준으로 영어와 스페인어가 병렬 쌍을 이루고 있습니다. 고로 \\t 기호를 매개변수로 split() 함수를 호출하면 손쉽게 소스 문장과 타겟 문장을 분리할 수 있습니다. 또한 I와 같은 기호가 포함돼 있는데, 이러한 특수문자는 불필요한 노이즈로 작용할 수 있는 만큼 정제 과정에서 삭제하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "acquired-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+',\" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\",\" \",sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if s_token:\n",
    "        sentence = '<start> '+sentence\n",
    "        \n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-renaissance",
   "metadata": {},
   "source": [
    "전처리 과정에서 문장의 시작 문자 <start>, 종료 문자 <end> 를 붙여주게 됩니다. 이 작업은 Encoder에 들어갈 입력 문장의 전처리에는 굳이 필요하지 않지만, Decoder의 입력 문장과 라벨로 사용할 출력 문장에는 꼭 필요하게 됩니다. 이전 렉처 노드에서 살펴보았듯, Decoder는 첫 입력으로 사용할 시작 토큰과 문장생성 종료를 알리는 끝 토큰이 반드시 필요하기 때문입니다.  \n",
    "  \n",
    "원활한 학습을 위해 데이터는 상위 3만 개만 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "circular-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: go away !\n",
      "Spanish: <start> salga de aqu ! <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for pair in raw[:num_examples]:\n",
    "    eng, spa = pair.split(\"\\t\")\n",
    "    \n",
    "    enc_corpus.append(preprocess_sentence(eng))\n",
    "    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n",
    "    \n",
    "print(\"English:\", enc_corpus[100]) # go away !\n",
    "print(\"Spanish:\", dec_corpus[100]) # <start> sa!ga ed aqu ! <end>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-remark",
   "metadata": {},
   "source": [
    "### 데이터 전처리: 토큰화\n",
    "정제된 텍스트를 아래 tokenize() 함수를 사용해 토큰화하고 텐서로 변환합니다.  \n",
    "그리고 변환된 텐서를 80%의 훈련 데이터와 20%의 검증 데이터로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "solar-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "minimal-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(v) for v in enc_corpus]\n",
    "b = 0\n",
    "for i in a:\n",
    "    b += i\n",
    "mean_enc_corpus = b/len(a)\n",
    "\n",
    "c = [len(v) for v in dec_corpus]\n",
    "d = 0\n",
    "for i in c:\n",
    "    d += i\n",
    "mean_dec_corpus = d/len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "italian-metabolism",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 영어 코퍼스 길이: 25\n",
      "평균 영어 코퍼스 길이: 18.701433333333334\n",
      "최소 영어 코퍼스 길이: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"최대 영어 코퍼스 길이:\", max([len(v) for v in enc_corpus]))\n",
    "print(\"평균 영어 코퍼스 길이:\", mean_enc_corpus)\n",
    "print(\"최소 영어 코퍼스 길이:\", min([len(v) for v in enc_corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "graduate-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 4931\n",
      "Spanish Vocab Size: 8893\n"
     ]
    }
   ],
   "source": [
    "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "print(\"English Vocab Size:\", len(enc_tokenizer.index_word))\n",
    "print(\"Spanish Vocab Size:\", len(dec_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "acute-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 9)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-bradford",
   "metadata": {},
   "source": [
    "## 모델 설계\n",
    "각각 1개의 GRU을 갖는 Encoder-Decoder 구조를 설계할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-transport",
   "metadata": {},
   "source": [
    "Encoder는 모든 Time-Step의 Hidden State를 출력으로 갖고, Decoder는 Encoder의 출력과 Decoder의 t-1 Step의 Hidden State로 Attention을 취하여 t Step의 Hidden State를 만들어 냅니다.  \n",
    "Decoder에서 t Step의 단어로 예측된 것을 실제 정답과 대조해 Loss를 구하고, 생성된 t Step의 Hidden State는 t+1 Step의 Hidden State를 만들기 위해 다시 Decoder에 전달됩니다.  \n",
    "여기서 't=1 일 때의 Hidden State는 일반적으로 Encoder의 Final State를 Hidden State로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ancient-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "banned-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "prime-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "civilian-magnet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 8894)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word)+1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word)+1\n",
    "\n",
    "units = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                    sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-event",
   "metadata": {},
   "source": [
    "## 훈련하기 (1) Optimizer & Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-music",
   "metadata": {},
   "source": [
    "지금까지는 fit() 함수로 간편하게 학습을 진행했지만, Encoder-Decoder 구조의 경우 입출력이 단순하지 않아 학습 과정을 직접 정의해줘야 합니다.  \n",
    "낯선 함수들이 지나치게 많이 등장할 수 있으니, 이번 코스에선 직접 구현을 하기보단 구현체를 먼저 보고 이해하는 방향으로 공부하도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-puzzle",
   "metadata": {},
   "source": [
    "### Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "published-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-charlotte",
   "metadata": {},
   "source": [
    "Optimizer는 모델이 학습할 때에 정답을 찾아가는 방법 정도로 설명할 수 있는데, 일반적으로 Adam을 사용합니다.  \n",
    "  \n",
    "여기서 fit() 함수를 사용할 수 없는 이유는 바로 Loss 함수 때문입니다. 앞서 설명한 것처럼 Encoder-Decoder 구조는 학습 과정이 일반적이지 않으므로 직접 Loss를 커스텀해서 사용해야 하기 때문이죠.  \n",
    "  \n",
    "SparseCategoricalCrossentropy() 함수는 모델이 출력한 확률 분포와 (One-hot이 아닌) 정수 인덱스 답안을 비교해 Cross Entropy값을 구해줍니다. CategoricalCrossentropy()라면 [ 0.1, 0.2, 0.7 ] 과 One-hot 인코딩된 라벨 [0, 0, 1] 을 비교하여 점수를 채점하겠지만, SparseCategoricalCrossentropy() 함수라면 [ 0.1, 0.2, 0.7 ] 과 정수 인덱스 답안 2 를 비교하여 점수를 채점하는 거죠. from_logits 는 확률 분포가 Softmax를 거쳐서 들어오는지, 모델의 출력값 그대로 들어오는지를 결정합니다. True 로 줬으니 모델의 출력값을 그대로 전달하면 됩니다.  \n",
    "  \n",
    "pad_sequences()는 가장 긴 문장을 기준으로 패딩을 적용합니다. 만약 모델에게 <PAD> 토큰이 패딩을 위한 토큰이라고 명시하지 않으면 모델은 데이터의 굉장히 많은 부분이 <PAD> 로 이뤄져 있다고 생각하게 됩니다. 쉽게 말해 유난히 같은 답이 많은 객관식 시험이라고나 할까요...? 어떤 시험이 한 번호로만 찍어도 80점 이상을 받을 수 있다면 그 시험은 공부를 절대 하지 않겠죠! 모델은 심지어 10,000개의 정답이 넘게 있는 고난도의 문제를 풀고 있기 때문에 패딩에 대한 처리를 해주지 않으면 <PAD> 토큰만을 생성할 확률이 굉장히 높아집니다.  \n",
    "  \n",
    "이것은 종종 발생하는 문제입니다.  이 문제를 방지하기 위해 mask 가 사용됩니다! mask 는 정답지에서 <PAD> 토큰을 찾아내어 그 부분에 대한 Loss는 구하지 않도록 하는 역할을 해주죠. equal() 함수에 정확히는 0 이 아닌 <PAD> 토큰의 인덱스를 전달하는 것이 맞지만 대부분의 경우는 0으로 패딩되기 때문에 편의상 0을 전달하여 처리하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-cliff",
   "metadata": {},
   "source": [
    "## 훈련하기 (2) train_step 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "hollow-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-introduction",
   "metadata": {},
   "source": [
    "@tf.function 데코레이터는 훈련 외적인 텐서플로우 연산을 GPU에서 동작하게 해 훈련을 가속할 수 있도록 도와줍니다. 첫 번째 Epoch이 다른 Epoch보다 약간의 시간이 더 걸리는 것은 데코레이터가 붙은 함수를 GPU에 등록하는 과정이 포함되어 있기 때문입니다. 실제로 위 예제에서 @tf.function 를 제거할 경우, Epoch당 1.5~2배 이상 더 많은 시간이 소요됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-opportunity",
   "metadata": {},
   "source": [
    "tf.GradientTape()는 학습하며 발생한 모든 연산을 기록하는 테이프입니다. 이것은 모델이 각 스텝의 최종 단계에서 미분값을 구하는 데에 사용됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-treatment",
   "metadata": {},
   "source": [
    "train_step()의 학습 과정은 아래와 같습니다.  \n",
    "1. Encoder에 소스 문장을 전달해 컨텍스트 벡터인 enc_out 을 생성  \n",
    "2. t=0일 때, Decoder의 Hidden State는 Encoder의 Final State로 정의. h_dec = enc_out[:, -1]  \n",
    "3. Decoder에 입력으로 전달할 <start> 토큰 문장 생성  \n",
    "4. <start> 문장과 enc_out, Hidden State를 기반으로 다음 단어(t=1)를 예측. pred  \n",
    "5. 예측된 단어와 정답 간의 Loss를 구한 후, t=1의 정답 단어를 다음 입력으로 사용 (예측 단어 X)  \n",
    "6. 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-netscape",
   "metadata": {},
   "source": [
    "## 훈련하기 (3) 훈련 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "intellectual-bracelet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [03:50<00:00,  1.62it/s, Loss 1.3430]\n",
      "Epoch  2: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.8789]\n",
      "Epoch  3: 100%|██████████| 375/375 [03:27<00:00,  1.80it/s, Loss 0.6193]\n",
      "Epoch  4: 100%|██████████| 375/375 [03:27<00:00,  1.81it/s, Loss 0.4414]\n",
      "Epoch  5: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.3273]\n",
      "Epoch  6: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.2550]\n",
      "Epoch  7: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.2065]\n",
      "Epoch  8: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.1767]\n",
      "Epoch  9: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.1544]\n",
      "Epoch 10: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.1401]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-asset",
   "metadata": {},
   "source": [
    "tqdm은 훈련의 진행 과정을 한눈에 볼 수 있게 해주는 라이브러리입니다.  \n",
    "먼저 EPOCHS = 10 만큼 반복하는 루프에 진입한 후, 각 배치의 시작 인덱스를 idx_list 배열에 저장합니다. 그리고 모델이 학습을 원활하게 할 수 있도록 데이터를 섞어줘야 하는데요, 인덱스를 섞어서 처리할 겁니다. 순차적인 인덱스로 데이터를 불러오는 게 아닌 랜덤한 인덱스로 데이터를 불러오는 것은 데이터를 섞는 것과 동일한 효과를 가져옵니다.  \n",
    "그 후에 각 미니배치를 train_step() 함수에서 학습합니다. train_step()은 학습에 필요한 것은 모두 가져가 Loss를 계산한 후 반환하는 함수입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "written-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [03:27<00:00,  1.81it/s, Loss 0.1288]\n",
      "Test Epoch  1: 100%|██████████| 94/94 [00:32<00:00,  2.93it/s, Test Loss 0.6813]\n",
      "Epoch  2: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.1207]\n",
      "Test Epoch  2: 100%|██████████| 94/94 [00:19<00:00,  4.86it/s, Test Loss 0.6926]\n",
      "Epoch  3: 100%|██████████| 375/375 [03:22<00:00,  1.86it/s, Loss 0.1133]\n",
      "Test Epoch  3: 100%|██████████| 94/94 [00:15<00:00,  6.24it/s, Test Loss 0.7052]\n",
      "Epoch  4: 100%|██████████| 375/375 [03:26<00:00,  1.82it/s, Loss 0.1060]\n",
      "Test Epoch  4: 100%|██████████| 94/94 [00:19<00:00,  4.90it/s, Test Loss 0.7171]\n",
      "Epoch  5: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.1014]\n",
      "Test Epoch  5: 100%|██████████| 94/94 [00:19<00:00,  4.90it/s, Test Loss 0.7200]\n",
      "Epoch  6: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.0962]\n",
      "Test Epoch  6: 100%|██████████| 94/94 [00:19<00:00,  4.92it/s, Test Loss 0.7261]\n",
      "Epoch  7: 100%|██████████| 375/375 [03:27<00:00,  1.81it/s, Loss 0.0955]\n",
      "Test Epoch  7: 100%|██████████| 94/94 [00:19<00:00,  4.87it/s, Test Loss 0.7404]\n",
      "Epoch  8: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.0941]\n",
      "Test Epoch  8: 100%|██████████| 94/94 [00:19<00:00,  4.88it/s, Test Loss 0.7414]\n",
      "Epoch  9: 100%|██████████| 375/375 [03:28<00:00,  1.80it/s, Loss 0.0911]\n",
      "Test Epoch  9: 100%|██████████| 94/94 [00:19<00:00,  4.89it/s, Test Loss 0.7499]\n",
      "Epoch 10: 100%|██████████| 375/375 [03:29<00:00,  1.79it/s, Loss 0.0903]\n",
      "Test Epoch 10: 100%|██████████| 94/94 [00:19<00:00,  4.92it/s, Test Loss 0.7673]\n"
     ]
    }
   ],
   "source": [
    "# Define eval_step\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "sapphire-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: can i have some coffee ?\n",
      "Predicted translation: puedo dar un poco de acuerdo ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 608,
       "width": 481
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
