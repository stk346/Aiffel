{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of LED for Summarization on pubmed_test",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46f14117d9aa47d4ab43c0bd6a7fb5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_70f15365d468408a81c9f94ee6f6ef91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_330b166968d0438183c50585bc0e78ca",
              "IPY_MODEL_f6b1ee2e31b74e1ca0a71c1b06d638c2"
            ]
          }
        },
        "70f15365d468408a81c9f94ee6f6ef91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "330b166968d0438183c50585bc0e78ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8d798f71b32d4a629efd244d60b7b911",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1092,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1092,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b08e5513010d411796bfb4ef40a83b44"
          }
        },
        "f6b1ee2e31b74e1ca0a71c1b06d638c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c194a8d622054399bd42031ee2a76239",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.09k/1.09k [00:02&lt;00:00, 499B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_625a11f84c554bdd82353f21fefa69e2"
          }
        },
        "8d798f71b32d4a629efd244d60b7b911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b08e5513010d411796bfb4ef40a83b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c194a8d622054399bd42031ee2a76239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "625a11f84c554bdd82353f21fefa69e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8f922195e7c46afb6478784c0f27bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26963febd3fe4af29aff6a24f3a54af8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df441ff6d29b452d8931a292ab69f1e9",
              "IPY_MODEL_332cf270926249d5844df57843e34e31"
            ]
          }
        },
        "26963febd3fe4af29aff6a24f3a54af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df441ff6d29b452d8931a292ab69f1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6f4fcec70894267a6cb638404db24b2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ead94600ed8a45e1a7694a155e73ae02"
          }
        },
        "332cf270926249d5844df57843e34e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b4ae3ecf9a44f62aa5c21e8338164cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 950kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_548fe7ac70394b2e8642132dd0383056"
          }
        },
        "c6f4fcec70894267a6cb638404db24b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ead94600ed8a45e1a7694a155e73ae02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b4ae3ecf9a44f62aa5c21e8338164cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "548fe7ac70394b2e8642132dd0383056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1075ee6bea4548cba349f2292015a899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5316d1c93dd4da5abfb60a28ac03018",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1aed6c3e1a41483c82b63c101860b43f",
              "IPY_MODEL_bc6abdb1caa54f7d9dccfe2b34a873c3"
            ]
          }
        },
        "c5316d1c93dd4da5abfb60a28ac03018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aed6c3e1a41483c82b63c101860b43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5e7f73c371047ab9d246eb24a0c9eea",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fac27eaebd04f74941f9ffa1e5a21ae"
          }
        },
        "bc6abdb1caa54f7d9dccfe2b34a873c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_030113e4392f48f89eabccd0af09ef1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 720kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f01b86c980844eb0ac6bd6e8b9be6a20"
          }
        },
        "a5e7f73c371047ab9d246eb24a0c9eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fac27eaebd04f74941f9ffa1e5a21ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "030113e4392f48f89eabccd0af09ef1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f01b86c980844eb0ac6bd6e8b9be6a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7fe6f9ae61b4cfbadaba2c0bfc50e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3b8ce93ce314a4d80b6e7b9f935600c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58609f6a301741c788d9e190e49601bc",
              "IPY_MODEL_727b33b4d0ba48d3be3b76040bfb0ead"
            ]
          }
        },
        "f3b8ce93ce314a4d80b6e7b9f935600c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58609f6a301741c788d9e190e49601bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea44f4c51d334e3aa3ed03acbb795242",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_591785faa90742dfb12a3436c2dee0e2"
          }
        },
        "727b33b4d0ba48d3be3b76040bfb0ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8156f721d27246b5a5357fac577b7d82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 250/250 [00:03&lt;00:00, 79.47ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_415c082418fd4c3b94408e00bbcda82a"
          }
        },
        "ea44f4c51d334e3aa3ed03acbb795242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "591785faa90742dfb12a3436c2dee0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8156f721d27246b5a5357fac577b7d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "415c082418fd4c3b94408e00bbcda82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa16af7563554384be6e5513c6afe79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3eb2a25aab6141bfa1cf5a76bed6c5b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be015dcb6cef4b41911e69fbc5af1a59",
              "IPY_MODEL_a62d68c2e4914f0ba08ede851cf47280"
            ]
          }
        },
        "3eb2a25aab6141bfa1cf5a76bed6c5b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be015dcb6cef4b41911e69fbc5af1a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f2c219f0099d411caf4ff5e831673875",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e6d376e4a394a2aa025e2edc3898ec8"
          }
        },
        "a62d68c2e4914f0ba08ede851cf47280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_628aa2531c944e4e8efeecf6fca40993",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25/25 [00:00&lt;00:00, 50.60ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78482e48010a412385fbe008d146d728"
          }
        },
        "f2c219f0099d411caf4ff5e831673875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e6d376e4a394a2aa025e2edc3898ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "628aa2531c944e4e8efeecf6fca40993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78482e48010a412385fbe008d146d728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "642d8f4c64474cccbec943413240c027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2ad50bf2a3b4b77947a29b72ec2bc8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2dcf5d79460d4554940dd259ccca44a4",
              "IPY_MODEL_1960516a36a3417c8a3c345d2cc101ec"
            ]
          }
        },
        "d2ad50bf2a3b4b77947a29b72ec2bc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dcf5d79460d4554940dd259ccca44a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_519ced1cfc774ca2a671cfe3903623a8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 647693783,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 647693783,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2074f2221bef4523ba6f7364db39651c"
          }
        },
        "1960516a36a3417c8a3c345d2cc101ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b395732993124de480c2da0b8bb63032",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 648M/648M [00:12&lt;00:00, 52.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2e83c9cec9b4236afb6a7227ebf5db9"
          }
        },
        "519ced1cfc774ca2a671cfe3903623a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2074f2221bef4523ba6f7364db39651c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b395732993124de480c2da0b8bb63032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2e83c9cec9b4236afb6a7227ebf5db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e605dd890d741a5a89dd1fd04183e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b7bfd05045ef495bad3d30e6a2ad3f9b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dea7c32f1e71438d9e30d808be8ff585",
              "IPY_MODEL_8d22ef0d5edb458781eccf7e0037cbcd"
            ]
          }
        },
        "b7bfd05045ef495bad3d30e6a2ad3f9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dea7c32f1e71438d9e30d808be8ff585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_20c130370f4249a68c2a73fc4369f527",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1955,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1955,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d94589c558cf4f928bfdbfc1c182c806"
          }
        },
        "8d22ef0d5edb458781eccf7e0037cbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f177d781255f4db783b7394229d8f730",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.93k/? [00:00&lt;00:00, 11.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc1e239059a945139662bc1dd6b0b2b5"
          }
        },
        "20c130370f4249a68c2a73fc4369f527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d94589c558cf4f928bfdbfc1c182c806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f177d781255f4db783b7394229d8f730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc1e239059a945139662bc1dd6b0b2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hgw3GTXLLw0"
      },
      "source": [
        "## 🤗 Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-QHmRiAMB9"
      },
      "source": [
        "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
        "\n",
        "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
        "\n",
        "We will leverage 🤗`Seq2SeqTrainer`, gradient checkpointing and as usual 🤗`datasets`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B19PhgrCHM1"
      },
      "source": [
        "First, let's try to get a GPU with at least 15GB RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_yEG3X3TOtV",
        "outputId": "151d723b-e024-43fc-e4cf-b8e92625132b"
      },
      "source": [
        "# colab pro 더 빠른 GPU 사용\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  2 10:47:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpJjEpoKTaRi",
        "outputId": "9ab5e6b7-ad4e-431c-d753-2757d877dc47"
      },
      "source": [
        "# colab pro 추가 메모리\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COjxMSo-192Y"
      },
      "source": [
        "# crash colab to get more RAM\n",
        "# !kill -9 -1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z18zG2N192Z"
      },
      "source": [
        "To check that we are having enough RAM we can run the following command.\n",
        "If the randomely allocated GPU is too small, the above cells can be run \n",
        "to crash the notebook hoping to get a better GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMQWD3wL192Z",
        "outputId": "128c8b3d-8071-4938-dd69-890b0be8253f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  2 10:47:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APGpqABk192Z"
      },
      "source": [
        "Next, we install 🤗Transformers, 🤗Datasets, and `rouge_score`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59lSzY4192Z"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.2.1\n",
        "!pip install transformers==4.2.0\n",
        "!pip install rouge_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Me6n0o192Z"
      },
      "source": [
        "Let's start by loading and preprocessing the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65BXYIBs192Z"
      },
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzjKRBib192Z"
      },
      "source": [
        "Next, we download the pubmed train and validation dataset ([click to see on 🤗Datasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **☕** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daphuZnx192Z",
        "outputId": "75c90c66-cd87-4f79-ce65-56f37af6ad0b"
      },
      "source": [
        "train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkxYYnbkDpgO",
        "outputId": "e86387ad-869e-454b-a2ec-c7ecf2982f0a"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abstract': \" background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran . \\n the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) . \\n however , there were no significant changes among boys or total population . \\n the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population . \\n the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran . \\n community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children . \",\n",
              " 'article': \"a recent systematic analysis showed that in 2011 , 314 ( 296 - 331 ) million children younger than 5 years were mildly , moderately or severely stunted and 258 ( 240 - 274 ) million were mildly , moderately or severely underweight in the developing countries .\\nin iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 16.2% , 8.6% and 1.5% , for underweight , overweight and obesity , respectively .\\nthe prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% .\\nanthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight .\\nsnack should have 300 - 400 kcal energy and could provide 5 - 10 g of protein / day . nowadays , school nutrition programs are running as the national programs , world - wide . national school lunch program in the united states\\nthere are also some reports regarding school feeding programs in developing countries . in vietnam ,\\nschool base program showed an improvement in nutrient intakes . in iran a national free food program ( nffp )\\nis implemented in elementary schools of deprived areas to cover all poor students . however , this program is not conducted in slums and poor areas of the big cities so many malnourished children with low socio - economic situation are not covered by nffp . although the rate of poverty in areas known as deprived is higher than other areas , many students in deprived areas are not actually poor and can afford food .\\nhence , nutritional value of the nffp is lower than the scientific recommended snacks for this age group .\\nfurthermore , lack of variety of food packages has decreased the tendency of children toward nffp . on the other hand ,\\nthe most important one is ministry of education ( moe ) of iran , which is responsible for selecting and providing the packages for targeted schools .\\nthe ministry of health ( moh ) is supervising the health situation of students and their health needs .\\nwelfare organizations , along with charities , have the indirect effect on nutritional status of students by financial support of their family .\\nprovincial governors have also the role of coordinating and supervising all activities of these organizations .\\nparent - teacher association is a community - based institution that participates in school 's policy such as nffp .\\nin addition to these organizations , nutritional literacy of students , their parents and teachers , is a very important issue , which could affect nutritional status of school age children .\\ntherefore , the present study was conducted with the aim of improving the nffp , so that by its resources all poor children will be covered even in big cities .\\nmoreover , all food packages were replaced by nutritious and diverse packages that were accessible for non - poor children . according to the aim of this study and multiple factors that could affect the problem ,\\npublic health advocacy has been chosen as the best strategy to deal with this issue .\\ntherefore , the present study determines the effects of nutrition intervention in an advocacy process model on the prevalence of underweight in school aged children in the poor area of shiraz , iran .\\nthis interventional study has been carried out between 2009 and 2010 in shiraz , iran .\\nthis survey was approved by the research committee of shiraz university of medical sciences . in coordination with education organization of fars province\\ntwo elementary schools and one middle school in the third region of the urban area of shiraz were selected randomly . in those schools all\\nstudents ( 2897 , 7 - 13 years old ) were screened based on their body mass index ( bmi ) by nutritionists . according to convenience method all\\nstudents divided to two groups based on their economic situation ; family revenue and head of household 's job and nutrition situation ; the first group were poor and malnourished students and the other group were well nourished or well - off students .\\nfor this report , the children 's height and weight were entered into center for disease control and prevention ( cdc ) to calculate bmi and bmi - for - age z - scores based on cdc for diseases control and prevention and growth standards .\\nthe significance of the difference between proportions was calculated using two - tailed z - tests for independent proportions . for implementing the interventions ,\\nthe advocacy process model weight was to the nearest 0.1 kg on a balance scale ( model # seca scale ) .\\nstanding height was measured to the nearest 0.1 cm with a wall - mounted stadiometer .\\nadvocacy group formation : this step was started with stakeholder analysis and identifying the stakeholders .\\nthe team was formed with representatives of all stakeholders include ; education organization , welfare organization , deputy for health of shiraz university , food and cosmetic product supervisory office and several non - governmental organizations and charities . situation analysis : this was carried out by use of existing data such as formal report of organizations , literature review and focus group with experts .\\nthe prevalence of malnutrition and its related factors among students was determined and weaknesses and strengths of the nffp were analyzed .\\naccordingly , three sub - groups were established : research and evaluation , education and justification and executive group . designing the strategies :\\nthree strategies were identified ; education and justification campaign , nutritional intervention ( providing nutritious , safe and diverse snacks ) and networking . performing the interventions : interventions that were implementing in selected schools were providing a diverse and nutritious snack package along with nutrition education for both groups while the first group ( poor and malnourished students ) was utilized the package free of charge .\\neducation and justification intervention : regarding the literature review and expert opinion , an educational group affiliated with the advocacy team has prepared educational booklets about nutritional information for each level ( degree ) .\\naccordingly , education of these booklets has been integrated into regular education of students and they educated and justified for better nutrition life - style .\\nit leads the educational group to hold several meeting with the student 's parents to justify them about the project and its benefit for their children .\\nafter these meetings , parental desire for participation in the project illustrated the effectiveness of the justification meeting with them .\\nfor educate fifteen talk show programs in tv and radio , 12 published papers in the local newspaper , have implemented to mobilize the community and gain their support .\\nhealthy diet , the importance of breakfast and snack in adolescence , wrong food habits among school age children , role of the family to improve food habit of children were the main topics , in which media campaign has focused on .\\nnutritional intervention : the snack basket of the students was replaced with traditional , nutritious and diverse foods . in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron .\\nlow economic and malnourished children were supported by executive group affiliated with advocacy team and the rest of them prepare their snack by themselves .\\nresearch and evaluation : in this step , the literacy and anthropometric indices ( bmi ) of students were assessed before and after the interventions .\\nthe reference for anthropometric measures was the world health organization / national center for health statistics ( who / nchs ) standards and the cut - offs were - two standard deviations ( sd ) from the mean .\\neach student that was malnourished and poor has been taken into account for free food and nutritious snacks .\\ndemographic information , height , weight and knowledge of the students were measured by use of a validated and reliable ( cronbach 's alpha was 0.61 ) questionnaire .\\nthis project is granted by shiraz university of medical sciences , charities and welfare organization and education organization of fars province .\\nstatistical analyses were performed using the statistical package for the social sciences ( spss ) software , version 17.0 ( spss inc . ,\\nthe results are expressed as mean  sd and proportions as appropriated . in order to determine the effective variables on the malnutrition status\\npaired t test was used to compare the end values with baseline ones in each group .\\nin this project , the who z - score cut - offs used were as follow : using bmi - for - age z - scores ; overweight : > + 1 sd , i.e. , z - score > 1 ( equivalent to bmi 25 kg / m ) , obesity : > + 2 sd ( equivalent to bmi 30 kg / m ) , thinness : < 2 sd and severe thinness : < 3 sd .\\nthis interventional study has been carried out between 2009 and 2010 in shiraz , iran .\\nthis survey was approved by the research committee of shiraz university of medical sciences . in coordination with education organization of fars province\\ntwo elementary schools and one middle school in the third region of the urban area of shiraz were selected randomly . in those schools all\\nstudents ( 2897 , 7 - 13 years old ) were screened based on their body mass index ( bmi ) by nutritionists . according to convenience method all\\nstudents divided to two groups based on their economic situation ; family revenue and head of household 's job and nutrition situation ; the first group were poor and malnourished students and the other group were well nourished or well - off students .\\nfor this report , the children 's height and weight were entered into center for disease control and prevention ( cdc ) to calculate bmi and bmi - for - age z - scores based on cdc for diseases control and prevention and growth standards .\\nthe significance of the difference between proportions was calculated using two - tailed z - tests for independent proportions . for implementing the interventions ,\\nweight was to the nearest 0.1 kg on a balance scale ( model # seca scale ) .\\nstanding height was measured to the nearest 0.1 cm with a wall - mounted stadiometer .\\nadvocacy group formation : this step was started with stakeholder analysis and identifying the stakeholders .\\nthe team was formed with representatives of all stakeholders include ; education organization , welfare organization , deputy for health of shiraz university , food and cosmetic product supervisory office and several non - governmental organizations and charities . situation analysis : this was carried out by use of existing data such as formal report of organizations , literature review and focus group with experts .\\nthe prevalence of malnutrition and its related factors among students was determined and weaknesses and strengths of the nffp were analyzed .\\naccordingly , three sub - groups were established : research and evaluation , education and justification and executive group . designing the strategies :\\nthree strategies were identified ; education and justification campaign , nutritional intervention ( providing nutritious , safe and diverse snacks ) and networking . performing the interventions : interventions that were implementing in selected schools were providing a diverse and nutritious snack package along with nutrition education for both groups while the first group ( poor and malnourished students ) was utilized the package free of charge . duration of intervention was 6 months .\\neducation and justification intervention : regarding the literature review and expert opinion , an educational group affiliated with the advocacy team has prepared educational booklets about nutritional information for each level ( degree ) .\\naccordingly , education of these booklets has been integrated into regular education of students and they educated and justified for better nutrition life - style . obviously , student 's families had remarkable effect on children 's food habit .\\nit leads the educational group to hold several meeting with the student 's parents to justify them about the project and its benefit for their children .\\nafter these meetings , parental desire for participation in the project illustrated the effectiveness of the justification meeting with them .\\neducate fifteen talk show programs in tv and radio , 12 published papers in the local newspaper , have implemented to mobilize the community and gain their support .\\nhealthy diet , the importance of breakfast and snack in adolescence , wrong food habits among school age children , role of the family to improve food habit of children were the main topics , in which media campaign has focused on .\\nnutritional intervention : the snack basket of the students was replaced with traditional , nutritious and diverse foods . in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron .\\nlow economic and malnourished children were supported by executive group affiliated with advocacy team and the rest of them prepare their snack by themselves .\\nresearch and evaluation : in this step , the literacy and anthropometric indices ( bmi ) of students were assessed before and after the interventions .\\nthe reference for anthropometric measures was the world health organization / national center for health statistics ( who / nchs ) standards and the cut - offs were - two standard deviations ( sd ) from the mean .\\neach student that was malnourished and poor has been taken into account for free food and nutritious snacks .\\ndemographic information , height , weight and knowledge of the students were measured by use of a validated and reliable ( cronbach 's alpha was 0.61 ) questionnaire .\\nthis project is granted by shiraz university of medical sciences , charities and welfare organization and education organization of fars province .\\nadvocacy group formation : this step was started with stakeholder analysis and identifying the stakeholders .\\nthe team was formed with representatives of all stakeholders include ; education organization , welfare organization , deputy for health of shiraz university , food and cosmetic product supervisory office and several non - governmental organizations and charities .\\nsituation analysis : this was carried out by use of existing data such as formal report of organizations , literature review and focus group with experts .\\nthe prevalence of malnutrition and its related factors among students was determined and weaknesses and strengths of the nffp were analyzed .\\naccordingly , three sub - groups were established : research and evaluation , education and justification and executive group .\\ndesigning the strategies : three strategies were identified ; education and justification campaign , nutritional intervention ( providing nutritious , safe and diverse snacks ) and networking .\\nperforming the interventions : interventions that were implementing in selected schools were providing a diverse and nutritious snack package along with nutrition education for both groups while the first group ( poor and malnourished students ) was utilized the package free of charge .\\neducation and justification intervention : regarding the literature review and expert opinion , an educational group affiliated with the advocacy team has prepared educational booklets about nutritional information for each level ( degree ) .\\naccordingly , education of these booklets has been integrated into regular education of students and they educated and justified for better nutrition life - style . obviously , student 's families had remarkable effect on children 's food habit .\\nit leads the educational group to hold several meeting with the student 's parents to justify them about the project and its benefit for their children .\\nafter these meetings , parental desire for participation in the project illustrated the effectiveness of the justification meeting with them .\\neducate fifteen talk show programs in tv and radio , 12 published papers in the local newspaper , have implemented to mobilize the community and gain their support .\\nhealthy diet , the importance of breakfast and snack in adolescence , wrong food habits among school age children , role of the family to improve food habit of children were the main topics , in which media campaign has focused on . nutritional intervention : the snack basket of the students\\nwas replaced with traditional , nutritious and diverse foods . in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron .\\nlow economic and malnourished children were supported by executive group affiliated with advocacy team and the rest of them prepare their snack by themselves .\\nresearch and evaluation : in this step , the literacy and anthropometric indices ( bmi ) of students were assessed before and after the interventions .\\nthe reference for anthropometric measures was the world health organization / national center for health statistics ( who / nchs ) standards and the cut - offs were - two standard deviations ( sd ) from the mean .\\neach student that was malnourished and poor has been taken into account for free food and nutritious snacks .\\ndemographic information , height , weight and knowledge of the students were measured by use of a validated and reliable ( cronbach 's alpha was 0.61 ) questionnaire .\\nthis project is granted by shiraz university of medical sciences , charities and welfare organization and education organization of fars province .\\nstatistical analyses were performed using the statistical package for the social sciences ( spss ) software , version 17.0 ( spss inc . , chicago , il , usa ) .\\nthe results are expressed as mean  sd and proportions as appropriated . in order to determine the effective variables on the malnutrition status\\npaired t test was used to compare the end values with baseline ones in each group .\\ntwo - sided p < 0.05 was considered to be statistically significant . in this project ,\\nthe who z - score cut - offs used were as follow : using bmi - for - age z - scores ; overweight : > + 1 sd , i.e. , z - score > 1 ( equivalent to bmi 25 kg / m ) , obesity : > + 2 sd ( equivalent to bmi 30\\nkg / m ) , thinness : < 2 sd and severe thinness : < 3 sd .\\nstudy population contains 2897 children ; 70.8% were primary school students and 29.2% were secondary school students .\\n2336 ( 80.5% ) out of total students were well - off and 561 children ( 19.5% ) were indigent .\\n19.5% of subjects were in case group ( n = 561 ) and 80.5% were in the control group ( n = 2336 ) .\\nthe mean of age in welfare group was 10.0  2.3 and 10.5  2.5 in non - welfare group .\\ndemographic characteristics of school aged children in shiraz , iran table 2 shows the frequency of subjects in different categories of bmi for age in non - welfare and welfare groups of school aged children separately among boys and girls before and after a nutrition intervention based on advocacy process model in shiraz , iran .\\nthe frequency of subjects with bmi lower than < 2 sd decreased significantly after intervention among non - welfare girls ( p < 0.01 ) .\\nhowever , there were no significant decreases in the frequency of subjects with bmi lower than < 2 sd boys .\\nwhen we assess the effect of intervention in total population without separating by sex groups , we found no significant change in this population [ table 3 ] .\\nbmi for age for iranian students aged 7 - 14 years based on gender according to who growth standards 2007 bmi for age for iranian students aged 7 - 14 years according to who growth standards 2007 in non - welfare and welfare groups of total population table 4 has shown the prevalence of normal bmi , mild , moderate and severe malnutrition in non - welfare and welfare groups of school aged children separately among boys and girls before and after a nutrition intervention based on advocacy process model . according to this table\\nthere were no significant differences in the prevalence of mild , moderate and severe malnutrition among girls and boys .\\ntable 4 also shows the mean of all anthropometric indices changed significantly after intervention both among girls and boys .\\nthe pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001 ) .\\nbmi , height and weight in non - welfare and welfare groups of school aged children separately in males and females before and after a nutrition intervention based on advocacy process model in shiraz , iran according to study 's finding the odds ratio ( or ) of sever thinness and thinness in non - welfare compared with welfare is 3.5 ( or = 3.5 , confidence interval [ ci ] = 2.5 - 3.9 , p < 0.001 ) .\\nfurthermore , the finding showed or of overweight and obesity in welfare compared to non - welfare is 19.3 ( or = 19.3 , ci = 2.5 - 3.9 , p = 0.04 ) .\\nthe result of this community intervention study revealed that nutrition intervention based on advocacy program had been successful to reduce the prevalence of underweight among poor girls .\\nthis study shows determinant factor of nutritional status of school age children was their socio - economic level . according to our knowledge ,\\nthis is the first study , which determines the effect of a community intervention based on advocacy process on the malnutrition indices in a big city ( shiraz ) in iran .\\nthe other program in iran ( nffp ) is specified to deprived area and is not conducted in big cities .\\nallocating millions of dollars to nffp by government , selecting the malnourished students through an active screening system at primary and middle schools , paying attention of policy makers to student 's nutrition have provided the opportunity to combat the problem . however , negligence of under - poverty line , providing poor snacks in terms of nutritional value and lack of variety are the main defects of this program .\\nadvocacy by definition is a blending of science , ethics and politics for comprehensive approaching health issues . by using advocacy program in california among the high school students for improving their nutrition and physical activity\\nangeles unified school district participants emphasized on nutrition classes for families as well as students in addition to other interventions . in the present study\\nanother study revealed that evaluability assessment gave stakeholders the opportunity to reflect on the project and its implementation issues .\\nit seems that in iran , free food program among the students not only is needed in deprived areas , but also it should be performed in big cities such as shiraz . at baseline ,\\nno significant difference was founded among wealthy students between the pre- and post - nutritional status intervention .\\nin contrast , the numbers of students who have malnutrition decreased from 44% to 39.4% , which was identified as a significant among impecunious girls students .\\nthere was also a significant increase in the proportion of children with bmi that was normal for age ( 2 to + 1 sd ) most of the published community interventions showed better results among females compared with males .\\nthis difference in the impact of nutritional interventions between male and female might be related to the different age of puberty in the female population compared to the male population . in the age range of the present study female\\nalthough , there is no nffp in big cities of iran , there are some programs for improving the nutritional status such as providing free milk in schools .\\na recent publication has shown that school feeding programs focus on milk supplementation had beneficial effects on the physical function and school performances specifically among girls in iran .\\nthe results of the mentioned study showed an improvement in the weight of children , psychological test 's scores and the grade - point average following this school feeding program .\\nthe intervention in the present study had focused on the snack intake in the school time .\\nthere are some reports regarding the nutrition transition in iran , which shows the importance of nutrition intervention to provide more healthy eating dietary habits among welfare groups of adolescents .\\nhence , nutrition intervention especially in the form of nutrition education is needed in big cities and among welfare children and adolescents . although a study among iranian adolescents showed that dietary behavior of adolescents does not accord to their knowledge , which emphasize on the necessity of community intervention programs . a recent study regarding the major dietary pattern among iranian children showed the presence of four major dietary patterns , in which fast food pattern and sweet pattern as two major dietary patterns can be mentioned among iranian children . in advocacy program audience 's analysis\\naccordingly , one of the prominent strategies in this study was working with media and was meeting with parent - teacher association that both of them were secondary target audiences\\n. we also took into account policy makers in different levels , from national to local as primary audiences .\\nadvocacy team had several meetings with management and planning organization at national level and education organization of the fars province as well as principal of the targeted schools .\\nproviding nutritious snacks need contribution of private sector such as food industries or factories , but their benefits should be warranted .\\nanother choice was community involvement ; which can be achieved by female health volunteers who are working with the health system .\\nadvocacy team by using the support of charities and female health volunteers could establish a local factory that produced student 's snacks based on the new definition . however , there are some challenges on the way of expanding this program .\\nmass production of the proposed snacks according to different desires and cultures and getting involvement of food industries with respect to marketing issues is one of those challenges .\\nmoreover , providing a supportive environment in order to change the food habits of the students and their parents among the wide range of the population require a sustainable and continuous inter - sector collaboration .\\nalthough in a limited number of schools , in our study , interventions and advocacy program was successful , expanding this model to another areas around the country depends on convincing the policy makers at national level . in this\\nregard , advocacy team should prepare evidenced based profile and transitional planning to convince the policy makers for improving the rule and regulation of nffp .\\nthe same as this study in other studies have also emphasized that there must be efforts to strengthen the capacity within the schools to deal with the nutritional problems either overweight , obesity or malnutrition by using of educational and nutritional intervention .\\nassessing the dietary adherence is very important in nutrition intervention among population . as this population was children and adolescents we had a limitation in the blood sample collection to assess the subject 's dietary adherence .\\nfurthermore , this intervention was only focused on the intake of snack in school time and we did not have comprehensive information on the dietary intake of children and adolescents after school all over the day .\\nthe investigators propose further investigation in different areas of the country based on socio - cultural differences in order to make necessary modification and adapt this model to other areas .\\nregarding the nutritional needs of the school age children , provision of a good platform for implementing and expanding this efficient model to the whole country based upon the socio - economic situation of each region is advisable to the moh and the moe .\\ncommunity nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children .\",\n",
              " 'section_names': 'INTRODUCTION\\nMATERIALS AND METHODS\\nParticipants\\nInstruments\\nProcedure\\nFirst step\\nSecond step\\nThird step\\nForth step\\nInterventions\\nFifth step (assessment)\\nData analysis\\nRESULTS\\nDISCUSSION\\nCONCLUSION'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw1m538v192a",
        "outputId": "cfa05e1c-a9b5-4e26-ccbd-9ac84c4e2241"
      },
      "source": [
        "val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNIDzzAkDquo",
        "outputId": "891a6966-e7c3-4f03-efc2-215211a97267"
      },
      "source": [
        "val_dataset[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abstract': ' background and aim : there is lack of substantial indian data on venous thromboembolism ( vte ) . \\n the aim of this study was to provide real - world information on patient characteristics , management strategies , clinical outcomes , and temporal trends in vte.subjects and methods : multicentre retrospective registry involving 549 medical records of patients with confirmed diagnosis of vte ( deep vein thrombosis [ dvt ] confirmed by doppler ultrasonography ; pulmonary embolism [ pe ] by computed tomography , pulmonary angiography and/or v / q scan ) from 2006 to 2010 at three indian tertiary care hospitals.results:acute dvt without pe , acute dvt with pe , and pe alone were reported in 64% ( 352/549 ) , 23% ( 124/549 ) , and 13% ( 73/549 ) patients , respectively . \\n mean age was 47 ( 16 ) years , and 70% were males . \\n h / o dvt ( 34% ) , surgery including orthopedic surgery ( 28% ) , trauma ( 16% ) , and immobilization > 3 days ( 14% ) were the most common risk factors for vte . \\n hypertension ( 25% ) , diabetes ( 19% ) , and neurological disease ( other than stroke ) ( 8% ) were the most common co - morbidities . \\n most ( 94% ) were treated with heparin alone ( 82% ) or fondaparinux ( 2% ) for initial anticoagulation ; low molecular weight heparin alone ( 5% ) or warfarin / acenocoumarol ( 76% ) for long - term anticoagulation . \\n anticoagulant treatment was stopped because of bleeding in 2% ( 9/515 ) patients . \\n mortality was 7% among patients diagnosed with vte during hospital stay versus 1% in those hospitalized with diagnosed vte . \\n the annual incidence of dvt ( pe ) increased from 2006 to 2010.conclusion:acute dvt alone was responsible for the substantial burden of vte in indian patients . \\n bleeding was not the limiting factor for anticoagulant treatment in most patients . ',\n",
              " 'article': \"approximately , one - third of patients with symptomatic vte manifests pe , whereas two - thirds manifest dvt alone .\\nboth dvt and pe can be clinically silent ( asymptomatic ) and hence not suspected .\\nif undiagnosed , asymptomatic vte can lead to chronic venous disease or recurrent vte and long - term debilitating sequelae such as postthrombotic syndrome and chronic thromboembolic pulmonary hypertension .\\nvte is not only disabling but also prolongs hospital stay and increases the cost of treatment .\\nalong with myocardial infarction and arrhythmia ( due to electrolyte imbalance ) , pe is one of the commonest causes of sudden unexplained deaths in hospitalized patients .\\nit is estimated that 20 million cases of lower extremity dvt occur in the usa alone .\\nthe prevailing notion that the incidence of vte in asians is less than that in the western population has been disproved by recent studies .\\nthe incidence of postoperative dvt in indian patients undergoing major lower limb surgery is as high ( 43.2% and 60% patients in the groups with and without prophylaxis , respectively ) as seen in the western world .\\ngiven the growing burden of vte in india and lack of substantial indian data on characteristics of vte patients , use of diagnostics tools , prophylaxis , treatment options , and clinical outcomes in vte , there was a need to systematically collect such data .\\ndata on patient characteristics , clinical outcomes , predictors of mortality in acute dvt , management strategies and temporal trends in vte .\\nthe intent was to collect and provide data that would reflect actual day - to - day clinical practice , rather than results of highly controlled clinical trials with restricted study populations and imposed experimental intervention .\\nconsecutive medical records of inpatients and outpatients between january 2006 and december 2010 , meeting eligibility criteria ( confirmed diagnosis of acute or acute - on - chronic dvt by doppler ultrasound scan and/or pe by chest computed tomography scan , pulmonary angiography or v / q scan ) were identified and collected from the general medical records and/or radiology departments at each of the three participating hospitals .\\nhospital data were used to obtain the total number of patients who were annually registered at the hospital from 2006 to 2010 .\\ndata were processed and analyzed using sas ( version 9.1 , statistical analysis system ) . for the purpose of analysis ,  acute - on - chronic \\ndescriptive statistics were used to present patient characteristics , management strategies , and clinical outcomes of patients .\\nannual incidence rates ( 95% ci ) of vte per 100,000 hospital registrations over a period of 5 years were reported for each site .\\nfisher 's exact test was used to determine differences in the incidence of acute dvt ( pe ) over the years 20062010 .\\narmitage trend test was used to examine the direction ( positive or negative ) of the trend . as primary analyses\\nthe remaining 41% ( 393/949 ) medical records were not included because they did not satisfy the inclusion criteria .\\ndata from seven patients were excluded as there was no radiologically confirmed diagnosis of pe .\\na total of 64% ( 352/549 ) patients had acute dvt without pe , 23% ( 124/549 ) had acute dvt with pe , and 13% ( 73/549 ) had pe .\\neighty - seven percent ( 476/549 ) of patients had acute dvt ( pe ) , and 36% ( 197/549 ) had pe (  acute dvt ) [ figure 1 ] .\\noverall distribution of venous thromboembolism patients ( n = 549 ) a total of 21% ( 115/549 ) of patients visited the hospitals directly without being referred by a physician .\\nvenous thromboembolism patients referred from different medical specialties ( n=434 ) the demographic characteristics of the vte patients are mentioned in table 2 .\\ndemographic characteristics of venous thromboembolism patients ( n=549 ) a total of 182 patients had evidence of one risk factor , 126 had evidence of two risk factors , 70 had evidence of three risk factors and 31 had four or more risk factors recorded .\\npatients undergoing orthopedic surgery constituted 22% ( 33/152 ) of all surgical patients [ table 3 ] .\\nrisk factors for venous thromboembolism based on a review of the available records , 157 patients had a single co - morbidity , 81 had two co - morbidities , 23 had three co - morbidities , and 16 had four or more co - morbidities .\\n( myocardial infarction , heart failure , chronic obstructive pulmonary disease , ventilator dependency , sepsis , or pneumonia ) [ table 4 ] .\\nco - morbidities in venous thromboembolism patients of the 476 patients with dvt , 2% ( 9 ) had upper extremity dvt , 97% ( 462 ) had lower extremity dvt and the site of dvt was not known in 5 patients .\\na total of 31% ( 143/462 ) patients had dvt in the right limb , 54% ( 249/462 ) in the left limb and 9% ( 41/462 ) in both limbs ( site not known in 29 patients ) .\\nof the 462 patients with lower extremity dvt , 61% had proximal dvt , 13% had distal dvt , and 7% had proximal and distal dvt .\\na total of 39% ( 215/549 ) patients were diagnosed with vte during their hospital stay , 54% ( 296/549 ) were admitted to hospital with a diagnosis of vte , and 7% ( 38/549 ) were diagnosed and continued to be managed in the outpatient department [ figure 2 ] .\\nplace of detection of venous thromboembolism ( n = 549 ) duration of hospitalization after diagnosis of venous thromboembolism a smaller proportion of patients ( 15% ; 81/549 ) was diagnosed with vte during the postoperative period .\\nfigure 3 shows the proportion of patients with vte at different time points during the postoperative period .\\nof those diagnosed beyond 6 weeks , 21% ( 3/14 ) had orthopedic surgery ( hip fracture surgery ) .\\ndiagnosis of venous thromboembolism during the postoperative period ( n = 81 ) the most common ( 73% ) symptom was  swelling of the limb  among patients with vte [ table 6 ] .\\nsymptoms in venous thromboembolism patients in merely 4% of all the patients , dvt was also confirmed by venography .\\npe was confirmed by pulmonary angiography in 27% of all the patients [ table 7 ] .\\n. heparin ( low molecular weight heparin [ lmwh]/unfractionated heparin [ ufh ] ) alone , a combination of heparin ( lmwh / ufh ) and oral anticoagulant ( warfarin ) , and fondaparinux sodium alone were recommended to 82% ( 420/515 ) , 13% ( 66/515 ) , and 2% ( 12/515 ) patients , respectively as initial anticoagulation .\\nfive percent ( 25/515 ) of patients were recommended lmwh alone , and 76% ( 393/515 ) were recommended either warfarin or acenocoumarol alone for long - term anticoagulation .\\nthe median duration of initial anticoagulation was 5 days while that of long - term anticoagulation was 180 days ( 6 months ) .\\nanticoagulants were needed to be stopped because of bleeding in only 2% ( 9/515 ) patients .\\nclinical outcomes in patients diagnosed with venous thromboembolism during hospital stay clinical outcomes in patients admitted to hospital with a diagnosis of venous thromboembolism the annual incidence of acute dvt ( pe ) increased from 2006 to 2010 at all the three sites [ figure 4 ] .\\nhowever , a formal site - wise statistical analysis could not be performed to analyse trends in the incidence rates in acute dvt ( pe ) and pe alone as there were zero observations in some instances .\\nincidence of acute deep vein thrombosis ( with or without pulmonary embolism ) over a 5 years period ( 20062010 ) at three sites\\ndemographic characteristics of venous thromboembolism patients ( n=549 ) a total of 182 patients had evidence of one risk factor , 126 had evidence of two risk factors , 70 had evidence of three risk factors and 31 had four or more risk factors recorded .\\npatients undergoing orthopedic surgery constituted 22% ( 33/152 ) of all surgical patients [ table 3 ] .\\nrisk factors for venous thromboembolism based on a review of the available records , 157 patients had a single co - morbidity , 81 had two co - morbidities , 23 had three co - morbidities , and 16 had four or more co - morbidities .\\n( myocardial infarction , heart failure , chronic obstructive pulmonary disease , ventilator dependency , sepsis , or pneumonia ) [ table 4 ] .\\n2% ( 9 ) had upper extremity dvt , 97% ( 462 ) had lower extremity dvt and the site of dvt was not known in 5 patients .\\na total of 31% ( 143/462 ) patients had dvt in the right limb , 54% ( 249/462 ) in the left limb and 9% ( 41/462 ) in both limbs ( site not known in 29 patients ) . of the 462 patients with lower extremity dvt\\n, 61% had proximal dvt , 13% had distal dvt , and 7% had proximal and distal dvt .\\na total of 39% ( 215/549 ) patients were diagnosed with vte during their hospital stay , 54% ( 296/549 ) were admitted to hospital with a diagnosis of vte , and 7% ( 38/549 ) were diagnosed and continued to be managed in the outpatient department [ figure 2 ] .\\nplace of detection of venous thromboembolism ( n = 549 ) duration of hospitalization after diagnosis of venous thromboembolism a smaller proportion of patients ( 15% ; 81/549 ) was diagnosed with vte during the postoperative period .\\nfigure 3 shows the proportion of patients with vte at different time points during the postoperative period . of those diagnosed beyond 6 weeks\\ndiagnosis of venous thromboembolism during the postoperative period ( n = 81 ) the most common ( 73% ) symptom was  swelling of the limb  among patients with vte [ table 6 ] .\\na total of 182 patients had evidence of one risk factor , 126 had evidence of two risk factors , 70 had evidence of three risk factors and 31 had four or more risk factors recorded .\\npatients undergoing orthopedic surgery constituted 22% ( 33/152 ) of all surgical patients [ table 3 ] .\\nbased on a review of the available records , 157 patients had a single co - morbidity , 81 had two co - morbidities , 23 had three co - morbidities , and 16 had four or more co - morbidities .\\n( myocardial infarction , heart failure , chronic obstructive pulmonary disease , ventilator dependency , sepsis , or pneumonia ) [ table 4 ] .\\nof the 476 patients with dvt , 2% ( 9 ) had upper extremity dvt , 97% ( 462 ) had lower extremity dvt and the site of dvt was not known in 5 patients .\\na total of 31% ( 143/462 ) patients had dvt in the right limb , 54% ( 249/462 ) in the left limb and 9% ( 41/462 ) in both limbs ( site not known in 29 patients ) .\\nof the 462 patients with lower extremity dvt , 61% had proximal dvt , 13% had distal dvt , and 7% had proximal and distal dvt .\\na total of 39% ( 215/549 ) patients were diagnosed with vte during their hospital stay , 54% ( 296/549 ) were admitted to hospital with a diagnosis of vte , and 7% ( 38/549 ) were diagnosed and continued to be managed in the outpatient department [ figure 2 ] .\\nplace of detection of venous thromboembolism ( n = 549 ) duration of hospitalization after diagnosis of venous thromboembolism a smaller proportion of patients ( 15% ; 81/549 ) was diagnosed with vte during the postoperative period .\\nfigure 3 shows the proportion of patients with vte at different time points during the postoperative period . of those diagnosed beyond 6 weeks\\ndiagnosis of venous thromboembolism during the postoperative period ( n = 81 ) the most common ( 73% ) symptom was  swelling of the limb  among patients with vte [ table 6 ]\\npe was confirmed by pulmonary angiography in 27% of all the patients [ table 7 ] .\\nheparin ( low molecular weight heparin [ lmwh]/unfractionated heparin [ ufh ] ) alone , a combination of heparin ( lmwh / ufh ) and oral anticoagulant ( warfarin ) , and fondaparinux sodium alone were recommended to 82% ( 420/515 ) , 13% ( 66/515 ) , and 2% ( 12/515 ) patients , respectively as initial anticoagulation .\\nfive percent ( 25/515 ) of patients were recommended lmwh alone , and 76% ( 393/515 ) were recommended either warfarin or acenocoumarol alone for long - term anticoagulation .\\nthe median duration of initial anticoagulation was 5 days while that of long - term anticoagulation was 180 days ( 6 months ) .\\nanticoagulants were needed to be stopped because of bleeding in only 2% ( 9/515 ) patients .\\nclinical outcomes in patients diagnosed with venous thromboembolism during hospital stay clinical outcomes in patients admitted to hospital with a diagnosis of venous thromboembolism the annual incidence of acute dvt ( pe ) increased from 2006 to 2010 at all the three sites [ figure 4 ] .\\nhowever , a formal site - wise statistical analysis could not be performed to analyse trends in the incidence rates in acute dvt ( pe ) and pe alone as there were zero observations in some instances .\\nincidence of acute deep vein thrombosis ( with or without pulmonary embolism ) over a 5 years period ( 20062010 ) at three sites\\npe was confirmed by pulmonary angiography in 27% of all the patients [ table 7 ] .\\nheparin ( low molecular weight heparin [ lmwh]/unfractionated heparin [ ufh ] ) alone , a combination of heparin ( lmwh / ufh ) and oral anticoagulant ( warfarin ) , and fondaparinux sodium alone were recommended to 82% ( 420/515 ) , 13% ( 66/515 ) , and 2% ( 12/515 ) patients , respectively as initial anticoagulation .\\nfive percent ( 25/515 ) of patients were recommended lmwh alone , and 76% ( 393/515 ) were recommended either warfarin or acenocoumarol alone for long - term anticoagulation .\\nthe median duration of initial anticoagulation was 5 days while that of long - term anticoagulation was 180 days ( 6 months ) .\\nanticoagulants were needed to be stopped because of bleeding in only 2% ( 9/515 ) patients .\\nclinical outcomes in patients diagnosed with venous thromboembolism during hospital stay clinical outcomes in patients admitted to hospital with a diagnosis of venous thromboembolism\\nthe annual incidence of acute dvt ( pe ) increased from 2006 to 2010 at all the three sites [ figure 4 ] .\\nhowever , a formal site - wise statistical analysis could not be performed to analyse trends in the incidence rates in acute dvt ( pe ) and pe alone as there were zero observations in some instances .\\nincidence of acute deep vein thrombosis ( with or without pulmonary embolism ) over a 5 years period ( 20062010 ) at three sites\\nto our knowledge , this is the first multicenter , retrospective registry in india involving patients with vte that reflect real - world clinical practice . in contrast with the western data in which vte is predominantly a disease of older age , 44% patients in our study were between 40 and 59 years of age while 34% were below 40 years , particularly those with pe . in a study from north india ,\\nmen constituted 70% of our registry , more than those reported from vellore registry ( 48% ) , but similar to those reported in the endorse ( epidemiologic international day for the evaluation of patients at risk for vte in the acute hospital care setting ) study ( 69% ) .\\none of the reasons for this could be significantly high levels of homocysteine ( thrombophilia marker ) in males as compared to females as reported in an indian study .\\nfewer indian women use oral contraceptives and postmenopausal hormone replacement therapy , which are known to be risk factors for thrombosis .\\nthis is supported by the fact that only 1% of women in this registry reported the use of oral contraceptives , and none reported use of hormonal replacement therapy .\\na total of 28% of the overall referrals were from cardiologists . the majority ( 82% ) of the referrals were from medical rather than surgical ( 15% ) specialties as against a referral rate of 93% from surgeons at vellore .\\nour finding complements that from the endorse study in which 55% of the medical patients at risk of vte had cardiovascular disease . majority ( 53% ) of patients in our study had co - morbid cardiovascular disease including diabetes mellitus ; it is possible that these patients visited a cardiologist for their cardiovascular ailment ( s ) and were then referred by the cardiologist to vascular disease specialist ( investigator ) .\\nmost ( 89% ) of these patients had swelling of the ( lower ) limb .\\nit is possible that these patients may not have felt the need to visit a specialist for a symptom like  swelling of limb ,  instead visited their family physician .\\nit is very encouraging to know that family physicians suspected dvt in these situations and referred the patient to a specialist .\\npatients with a history of vte are about 8 times more likely to develop a new episode during a subsequent high - risk period compared with patients without a history of dvt or pe .\\nprior history of dvt was the most ( 34% ) common risk factor in patients who had only dvt , whereas past history of pe , trauma , and immobilization for more than 3 days were the most common risk factors in patients who had only pe .\\nour results ( major lower limb surgery as a risk factor in 3% patients ) appear to be consistent with those reported in the endorse study , which reported dvt in 4.4% patients undergoing major lower limb surgery .\\nother studies from india have reported a dvt incidence rate ranging from 8% to 20% in major lower limb surgery .\\nhowever , in our study , only 7% of patients had malignancy as a predisposing factor . among the malignancies , genitourinary cancer had the highest incidence ( 45% ) .\\nhypertension ( 25% ) was the most common co - morbidity followed by diabetes mellitus ( 19% ) in this patient population .\\nin addition , obesity ( 11% ) was a common risk factor in dvt complicated by pe .\\nour findings support an asian ( korean ) study that demonstrated prevalence of the metabolic syndrome in 48% patients with vte .\\nco - morbid neurological disease ( other than stroke ) and ventilator dependency were also commonly found in patients with dvt ( 10% ) and pe ( 11% ) respectively .\\nboth these conditions immobilize patients for prolonged periods of time , predisposing them to vte .\\nvenography and pulmonary angiography are the gold standard for diagnosis of dvt and pe respectively . in our study\\n, venography was used in just 4% patients and pulmonary angiography in less than one - third of the patients .\\nperhaps the relatively high cost of these tests and limited availability of such procedures may be the limiting factors .\\noverall , most ( 93% ) patients were managed as inpatients ( 39% diagnosed with vte during hospital stay and 54% admitted to hospital with a diagnosis of vte ) .\\na mean duration of hospitalization of 79 days after diagnosis of vte is supported by published data . in selected low - risk patients , outpatient treatment of dvt and pe may be considered .\\nthis approach was observed in a small proportion ( 7% ) of patients who were managed on an outpatient basis , nearly all ( 97% ) of whom had only dvt .\\nthe reported prevalence of postsurgical vte in our study ( 15% ) was half of that ( 30% ) reported in vellore registry .\\nthis could be explained by higher referral rate from surgeons at vellore compared to that of our sites .\\nmost ( 40% ; 32/81 ) dvt cases were diagnosed between 2 and 6 postoperative weeks , but pe in most cases ( 70% ; 7/10 ) was diagnosed during the first postoperative week .\\nwe notice that acute dvt complicated by pe was less ( 6% ; 7/124 ) frequently diagnosed during the postoperative period as against 18% ( 64/352 ) and 14% ( 10/73 ) of acute dvt alone and pe alone , respectively .\\nthe use and duration of anticoagulants in our registry appears to be consistent with the american college of chest physicians treatment guidelines , which recommend at least 5 days of initial anticoagulation with parenteral anticoagulation ( lmwh , fondaparinux , intravenous ufh , or subcutaneous ufh ) and at least 3 months of long - term anticoagulation treatment with vitamin k antagonist .\\nbleeding is the most serious complication of anticoagulation treatment and is a major concern for clinicians particularly as the patient 's age advances . in this registry ,\\nanticoagulant treatment was needed to be stopped because of bleeding in only 2% of the study population .\\nthe prospective reite registry has reported a rate of 3% for major / fatal bleeds .\\nthus , the fear of bleeding complications , which decreases the use of anticoagulant treatment , appears to be minimal .\\ndvt complicated by pe ( 60% ) and pe alone ( 75% ) were more frequently shifted to intensive care unit than those who had dvt alone ( 25% ) . similar to published data in which hospital readmission rate for vte was 5% for primary and 14% for secondary diagnosis , we report a hospital readmission rate of 6% ; however we do not know the cause for readmission .\\nthe death rate was 7% among those diagnosed with vte during hospital stay as against a rate of 1% among those who were hospitalized with a diagnosis of vte .\\nover 90% of patients treated on an outpatient basis obtained symptomatic relief with treatment . in our study , the hospital discharge rate ( 97% ) was more than triple and death rate was a quarter of that reported by pandey et al .\\n( hospital discharge rate 31% and death rate 16% ) at a university hospital in delhi .\\nour data show a significant increase in acute dvt ( pe ) from 2006 to 2010 .\\nthis can be explained by the increased awareness of vte in india as well as the advent of better diagnostic modalities , such as duplex ultrasonography becoming more readily available and accepted .\\nalthough there was no significant change in the number of pe cases from 2006 to 2010 , the burden of pe is almost double ( 13% of all vte ) of 7% , rate reported at christian medical college , vellore during a 10-year period from 1996 to 2005 .\\nour finding is consistent with a study from north india that reported a 16% incidence of pe in adult medical autopsies .\\nthis study has the expected limitations of any retrospective review including the availability of complete records for all patients , although a robust review of the data on medical charts was conducted .\\ncontrolling for bias and confounders is difficult as there is no randomization and no blinding .\\nfollow - up data of patients after hospital discharge were not available . in cases of death ,\\nfurther , the clinic charts reviewed in this study included a mix of those from vascular surgery and hematology departments , limiting the generalizability of the study results . despite these limitations ,\\nthis study provides large amount of useful information in a short span of time on patient characteristics , clinical outcomes , management strategies , and temporal trends in vte , based on  real world \\ndata that reflect actual day - to - day clinical practice over a period of 5 years across three sites in india .\\nwe believe that this information will serve as a guide in the optimal implementation of vte prophylaxis and treatment , to improve patient outcomes and to decrease the occurrence of vte in india .\\nreal world data reflecting actual day - to - day clinical practice in vte over a period of 5 years across three sites in india showed that vte is not uncommon in indian patients and that acute dvt was responsible for the substantial burden of vte .\\nwe believe that this information will serve as a guide in the optimal implementation of vte prophylaxis and treatment , to improve patient outcomes and to decrease the occurrence of vte in india .\\nliesel c. dsilva is and dr . sadhna j. joglekar was full - time employee of glaxosmithkline pharmaceuticals limited .\",\n",
              " 'section_names': 'Introduction\\nSubjects and Methods\\nResults\\nDemographics and characteristics of venous thromboembolism patients\\nRisk factors for venous thromboembolism patients\\nCo-morbidities in venous thromboembolism patients\\nClinical presentation of venous thromboembolism\\nManagement strategies for venous thromboembolism patients\\nDiagnostic tools for venous thromboembolism\\nAnticoagulant treatment in venous thromboembolism\\nAnnual incidence of acute deep venous thrombosis including the trend over a period of 5 years\\nDiscussion\\nConclusion\\nNone\\nFinancial support and sponsorship\\nConflicts of interest'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyn-TDeB192a"
      },
      "source": [
        "It's always a good idea to take a look at some data samples. Let's do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QkyLfIy192a"
      },
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=4):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3iiUgy192a"
      },
      "source": [
        "We can see that the input data is the `article` - a scientific report and the target data is the `abstract` - a concise summary of the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs7zKqy-192a"
      },
      "source": [
        "Cool! Having downloaded the dataset, let's tokenize it.\n",
        "We'll import the convenient `AutoTokenizer` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7wczXZP192a"
      },
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pi6Pr2N192a"
      },
      "source": [
        " and load the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "46f14117d9aa47d4ab43c0bd6a7fb5b7",
            "70f15365d468408a81c9f94ee6f6ef91",
            "330b166968d0438183c50585bc0e78ca",
            "f6b1ee2e31b74e1ca0a71c1b06d638c2",
            "8d798f71b32d4a629efd244d60b7b911",
            "b08e5513010d411796bfb4ef40a83b44",
            "c194a8d622054399bd42031ee2a76239",
            "625a11f84c554bdd82353f21fefa69e2",
            "f8f922195e7c46afb6478784c0f27bca",
            "26963febd3fe4af29aff6a24f3a54af8",
            "df441ff6d29b452d8931a292ab69f1e9",
            "332cf270926249d5844df57843e34e31",
            "c6f4fcec70894267a6cb638404db24b2",
            "ead94600ed8a45e1a7694a155e73ae02",
            "8b4ae3ecf9a44f62aa5c21e8338164cc",
            "548fe7ac70394b2e8642132dd0383056",
            "1075ee6bea4548cba349f2292015a899",
            "c5316d1c93dd4da5abfb60a28ac03018",
            "1aed6c3e1a41483c82b63c101860b43f",
            "bc6abdb1caa54f7d9dccfe2b34a873c3",
            "a5e7f73c371047ab9d246eb24a0c9eea",
            "1fac27eaebd04f74941f9ffa1e5a21ae",
            "030113e4392f48f89eabccd0af09ef1c",
            "f01b86c980844eb0ac6bd6e8b9be6a20"
          ]
        },
        "id": "XLYnI1xG192a",
        "outputId": "9313a322-2c54-4e9e-b08c-892c2abff3f3"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46f14117d9aa47d4ab43c0bd6a7fb5b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1092.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8f922195e7c46afb6478784c0f27bca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1075ee6bea4548cba349f2292015a899",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifg19ED9192a"
      },
      "source": [
        "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_s6Z_ni192a"
      },
      "source": [
        "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.${}^1$. \n",
        "\n",
        "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
        "\n",
        "In this notebook, we are only able to train on `batch_size=2` to prevent out-of-memory errors.\n",
        "\n",
        "---\n",
        "${}^1$ The data is taken from page 11 of [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk7tS_xN192b"
      },
      "source": [
        "max_input_length = 8192\n",
        "max_output_length = 512\n",
        "batch_size = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2493L-dt192b"
      },
      "source": [
        "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
        "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
        "\n",
        "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_UzG6Ek192b"
      },
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"abstract\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_output_length,\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-V7N0L-192b"
      },
      "source": [
        "For the sake of this notebook, we will reduce the training and validation data \n",
        "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP2EGXJl192b"
      },
      "source": [
        "train_dataset = train_dataset.select(range(250))\n",
        "val_dataset = val_dataset.select(range(25))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJ7dOpi192b"
      },
      "source": [
        "Great, having defined the mapping function, let's preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a7fe6f9ae61b4cfbadaba2c0bfc50e1a",
            "f3b8ce93ce314a4d80b6e7b9f935600c",
            "58609f6a301741c788d9e190e49601bc",
            "727b33b4d0ba48d3be3b76040bfb0ead",
            "ea44f4c51d334e3aa3ed03acbb795242",
            "591785faa90742dfb12a3436c2dee0e2",
            "8156f721d27246b5a5357fac577b7d82",
            "415c082418fd4c3b94408e00bbcda82a"
          ]
        },
        "id": "wcaN0IJD192b",
        "outputId": "952499e3-265a-4694-ec89-7f848d19f2a6"
      },
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7fe6f9ae61b4cfbadaba2c0bfc50e1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gunSLWj192b"
      },
      "source": [
        "and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "aa16af7563554384be6e5513c6afe79a",
            "3eb2a25aab6141bfa1cf5a76bed6c5b2",
            "be015dcb6cef4b41911e69fbc5af1a59",
            "a62d68c2e4914f0ba08ede851cf47280",
            "f2c219f0099d411caf4ff5e831673875",
            "0e6d376e4a394a2aa025e2edc3898ec8",
            "628aa2531c944e4e8efeecf6fca40993",
            "78482e48010a412385fbe008d146d728"
          ]
        },
        "id": "FkrEujTX192b",
        "outputId": "56576f0b-29df-4fde-84e5-c82f81de6b81"
      },
      "source": [
        "val_dataset = val_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa16af7563554384be6e5513c6afe79a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_UcMjygDwW5",
        "outputId": "c79353eb-b45f-4f9f-d78b-f419cc49373d"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  ...],\n",
              " 'global_attention_mask': [1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  ...],\n",
              " 'input_ids': [0,\n",
              "  102,\n",
              "  485,\n",
              "  20552,\n",
              "  1966,\n",
              "  969,\n",
              "  14,\n",
              "  11,\n",
              "  1466,\n",
              "  2156,\n",
              "  36835,\n",
              "  36,\n",
              "  40625,\n",
              "  111,\n",
              "  39507,\n",
              "  4839,\n",
              "  153,\n",
              "  408,\n",
              "  3240,\n",
              "  87,\n",
              "  195,\n",
              "  107,\n",
              "  58,\n",
              "  33019,\n",
              "  2156,\n",
              "  30389,\n",
              "  50,\n",
              "  11166,\n",
              "  16990,\n",
              "  5357,\n",
              "  8,\n",
              "  36586,\n",
              "  36,\n",
              "  15452,\n",
              "  111,\n",
              "  38415,\n",
              "  4839,\n",
              "  153,\n",
              "  58,\n",
              "  33019,\n",
              "  2156,\n",
              "  30389,\n",
              "  50,\n",
              "  11166,\n",
              "  223,\n",
              "  4301,\n",
              "  11,\n",
              "  5,\n",
              "  2623,\n",
              "  749,\n",
              "  479,\n",
              "  50118,\n",
              "  179,\n",
              "  10209,\n",
              "  260,\n",
              "  10,\n",
              "  892,\n",
              "  566,\n",
              "  262,\n",
              "  4429,\n",
              "  239,\n",
              "  334,\n",
              "  1972,\n",
              "  11,\n",
              "  579,\n",
              "  7566,\n",
              "  8,\n",
              "  17832,\n",
              "  3964,\n",
              "  20921,\n",
              "  260,\n",
              "  969,\n",
              "  21087,\n",
              "  9,\n",
              "  545,\n",
              "  4,\n",
              "  176,\n",
              "  207,\n",
              "  2156,\n",
              "  290,\n",
              "  4,\n",
              "  401,\n",
              "  207,\n",
              "  8,\n",
              "  112,\n",
              "  4,\n",
              "  245,\n",
              "  207,\n",
              "  2156,\n",
              "  13,\n",
              "  223,\n",
              "  4301,\n",
              "  2156,\n",
              "  19755,\n",
              "  8,\n",
              "  14057,\n",
              "  2156,\n",
              "  4067,\n",
              "  479,\n",
              "  50118,\n",
              "  627,\n",
              "  21087,\n",
              "  9,\n",
              "  29638,\n",
              "  566,\n",
              "  10891,\n",
              "  334,\n",
              "  5180,\n",
              "  408,\n",
              "  11,\n",
              "  3055,\n",
              "  43966,\n",
              "  14903,\n",
              "  31,\n",
              "  231,\n",
              "  207,\n",
              "  7,\n",
              "  545,\n",
              "  207,\n",
              "  479,\n",
              "  50118,\n",
              "  36046,\n",
              "  22356,\n",
              "  892,\n",
              "  9,\n",
              "  10891,\n",
              "  334,\n",
              "  521,\n",
              "  11,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  1487,\n",
              "  14,\n",
              "  545,\n",
              "  207,\n",
              "  9,\n",
              "  106,\n",
              "  6297,\n",
              "  31,\n",
              "  29638,\n",
              "  8,\n",
              "  614,\n",
              "  809,\n",
              "  2408,\n",
              "  479,\n",
              "  50118,\n",
              "  22617,\n",
              "  2990,\n",
              "  197,\n",
              "  33,\n",
              "  2993,\n",
              "  111,\n",
              "  3675,\n",
              "  48130,\n",
              "  1007,\n",
              "  8,\n",
              "  115,\n",
              "  694,\n",
              "  195,\n",
              "  111,\n",
              "  158,\n",
              "  821,\n",
              "  9,\n",
              "  8276,\n",
              "  1589,\n",
              "  183,\n",
              "  479,\n",
              "  25708,\n",
              "  2156,\n",
              "  334,\n",
              "  10894,\n",
              "  1767,\n",
              "  32,\n",
              "  878,\n",
              "  25,\n",
              "  5,\n",
              "  632,\n",
              "  1767,\n",
              "  2156,\n",
              "  232,\n",
              "  111,\n",
              "  1810,\n",
              "  479,\n",
              "  632,\n",
              "  334,\n",
              "  4592,\n",
              "  586,\n",
              "  11,\n",
              "  5,\n",
              "  10409,\n",
              "  982,\n",
              "  50118,\n",
              "  8585,\n",
              "  32,\n",
              "  67,\n",
              "  103,\n",
              "  690,\n",
              "  2624,\n",
              "  334,\n",
              "  10943,\n",
              "  1767,\n",
              "  11,\n",
              "  2623,\n",
              "  749,\n",
              "  479,\n",
              "  11,\n",
              "  748,\n",
              "  5810,\n",
              "  8697,\n",
              "  2156,\n",
              "  50118,\n",
              "  8813,\n",
              "  1542,\n",
              "  586,\n",
              "  969,\n",
              "  41,\n",
              "  3855,\n",
              "  11,\n",
              "  30417,\n",
              "  43954,\n",
              "  479,\n",
              "  11,\n",
              "  10209,\n",
              "  260,\n",
              "  10,\n",
              "  632,\n",
              "  481,\n",
              "  689,\n",
              "  586,\n",
              "  36,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  4839,\n",
              "  50118,\n",
              "  354,\n",
              "  6264,\n",
              "  11,\n",
              "  10891,\n",
              "  1304,\n",
              "  9,\n",
              "  22632,\n",
              "  911,\n",
              "  7,\n",
              "  1719,\n",
              "  70,\n",
              "  2129,\n",
              "  521,\n",
              "  479,\n",
              "  959,\n",
              "  2156,\n",
              "  42,\n",
              "  586,\n",
              "  16,\n",
              "  45,\n",
              "  2964,\n",
              "  11,\n",
              "  3369,\n",
              "  8014,\n",
              "  8,\n",
              "  2129,\n",
              "  911,\n",
              "  9,\n",
              "  5,\n",
              "  380,\n",
              "  1947,\n",
              "  98,\n",
              "  171,\n",
              "  8196,\n",
              "  282,\n",
              "  2126,\n",
              "  6555,\n",
              "  408,\n",
              "  19,\n",
              "  614,\n",
              "  20182,\n",
              "  111,\n",
              "  776,\n",
              "  1068,\n",
              "  32,\n",
              "  45,\n",
              "  2913,\n",
              "  30,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  479,\n",
              "  1712,\n",
              "  5,\n",
              "  731,\n",
              "  9,\n",
              "  5263,\n",
              "  11,\n",
              "  911,\n",
              "  684,\n",
              "  25,\n",
              "  22632,\n",
              "  16,\n",
              "  723,\n",
              "  87,\n",
              "  97,\n",
              "  911,\n",
              "  2156,\n",
              "  171,\n",
              "  521,\n",
              "  11,\n",
              "  22632,\n",
              "  911,\n",
              "  32,\n",
              "  45,\n",
              "  888,\n",
              "  2129,\n",
              "  8,\n",
              "  64,\n",
              "  4960,\n",
              "  689,\n",
              "  479,\n",
              "  50118,\n",
              "  2457,\n",
              "  1755,\n",
              "  2156,\n",
              "  22813,\n",
              "  923,\n",
              "  9,\n",
              "  5,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  16,\n",
              "  795,\n",
              "  87,\n",
              "  5,\n",
              "  6441,\n",
              "  5131,\n",
              "  14967,\n",
              "  13,\n",
              "  42,\n",
              "  1046,\n",
              "  333,\n",
              "  479,\n",
              "  50118,\n",
              "  506,\n",
              "  39212,\n",
              "  4321,\n",
              "  2156,\n",
              "  1762,\n",
              "  9,\n",
              "  3143,\n",
              "  9,\n",
              "  689,\n",
              "  8368,\n",
              "  34,\n",
              "  8065,\n",
              "  5,\n",
              "  16699,\n",
              "  9,\n",
              "  408,\n",
              "  1706,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  479,\n",
              "  15,\n",
              "  5,\n",
              "  97,\n",
              "  865,\n",
              "  2156,\n",
              "  50118,\n",
              "  627,\n",
              "  144,\n",
              "  505,\n",
              "  65,\n",
              "  16,\n",
              "  3158,\n",
              "  9,\n",
              "  1265,\n",
              "  36,\n",
              "  475,\n",
              "  3540,\n",
              "  4839,\n",
              "  9,\n",
              "  10209,\n",
              "  260,\n",
              "  2156,\n",
              "  61,\n",
              "  16,\n",
              "  2149,\n",
              "  13,\n",
              "  18099,\n",
              "  8,\n",
              "  1976,\n",
              "  5,\n",
              "  8368,\n",
              "  13,\n",
              "  3656,\n",
              "  1304,\n",
              "  479,\n",
              "  50118,\n",
              "  627,\n",
              "  3158,\n",
              "  9,\n",
              "  474,\n",
              "  36,\n",
              "  475,\n",
              "  2678,\n",
              "  4839,\n",
              "  16,\n",
              "  21291,\n",
              "  3009,\n",
              "  5,\n",
              "  474,\n",
              "  1068,\n",
              "  9,\n",
              "  521,\n",
              "  8,\n",
              "  49,\n",
              "  474,\n",
              "  782,\n",
              "  479,\n",
              "  50118,\n",
              "  605,\n",
              "  43528,\n",
              "  2665,\n",
              "  2156,\n",
              "  552,\n",
              "  19,\n",
              "  12237,\n",
              "  2156,\n",
              "  33,\n",
              "  5,\n",
              "  18677,\n",
              "  1683,\n",
              "  15,\n",
              "  22813,\n",
              "  2194,\n",
              "  9,\n",
              "  521,\n",
              "  30,\n",
              "  613,\n",
              "  323,\n",
              "  9,\n",
              "  49,\n",
              "  284,\n",
              "  479,\n",
              "  50118,\n",
              "  13138,\n",
              "  42726,\n",
              "  12066,\n",
              "  33,\n",
              "  67,\n",
              "  5,\n",
              "  774,\n",
              "  9,\n",
              "  22220,\n",
              "  8,\n",
              "  21291,\n",
              "  3009,\n",
              "  70,\n",
              "  1713,\n",
              "  9,\n",
              "  209,\n",
              "  2665,\n",
              "  479,\n",
              "  50118,\n",
              "  20985,\n",
              "  111,\n",
              "  3254,\n",
              "  5259,\n",
              "  16,\n",
              "  10,\n",
              "  435,\n",
              "  111,\n",
              "  716,\n",
              "  6786,\n",
              "  14,\n",
              "  33439,\n",
              "  11,\n",
              "  334,\n",
              "  128,\n",
              "  29,\n",
              "  714,\n",
              "  215,\n",
              "  25,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  479,\n",
              "  50118,\n",
              "  179,\n",
              "  1285,\n",
              "  7,\n",
              "  209,\n",
              "  2665,\n",
              "  2156,\n",
              "  22813,\n",
              "  18218,\n",
              "  9,\n",
              "  521,\n",
              "  2156,\n",
              "  49,\n",
              "  1041,\n",
              "  8,\n",
              "  2948,\n",
              "  2156,\n",
              "  16,\n",
              "  10,\n",
              "  182,\n",
              "  505,\n",
              "  696,\n",
              "  2156,\n",
              "  61,\n",
              "  115,\n",
              "  3327,\n",
              "  22813,\n",
              "  2194,\n",
              "  9,\n",
              "  334,\n",
              "  1046,\n",
              "  408,\n",
              "  479,\n",
              "  50118,\n",
              "  8585,\n",
              "  13364,\n",
              "  2156,\n",
              "  5,\n",
              "  1455,\n",
              "  892,\n",
              "  21,\n",
              "  2964,\n",
              "  19,\n",
              "  5,\n",
              "  4374,\n",
              "  9,\n",
              "  3927,\n",
              "  5,\n",
              "  295,\n",
              "  3145,\n",
              "  642,\n",
              "  2156,\n",
              "  98,\n",
              "  14,\n",
              "  30,\n",
              "  63,\n",
              "  1915,\n",
              "  70,\n",
              "  2129,\n",
              "  408,\n",
              "  40,\n",
              "  28,\n",
              "  2913,\n",
              "  190,\n",
              "  11,\n",
              "  380,\n",
              "  1947,\n",
              "  479,\n",
              "  50118,\n",
              "  4321,\n",
              "  2137,\n",
              "  2156,\n",
              "  70,\n",
              "  689,\n",
              "  8368,\n",
              "  58,\n",
              "  4209,\n",
              "  30,\n",
              "  30426,\n",
              "  8,\n",
              "  5544,\n",
              "  8368,\n",
              "  14,\n",
              "  58,\n",
              "  6500,\n",
              "  13,\n",
              "  786,\n",
              "  111,\n",
              "  2129,\n",
              "  408,\n",
              "  479,\n",
              "  309,\n",
              "  7,\n",
              "  5,\n",
              "  4374,\n",
              "  9,\n",
              "  42,\n",
              "  892,\n",
              "  8,\n",
              "  1533,\n",
              "  2433,\n",
              "  14,\n",
              "  115,\n",
              "  3327,\n",
              "  5,\n",
              "  936,\n",
              "  2156,\n",
              "  50118,\n",
              "  15110,\n",
              "  474,\n",
              "  8973,\n",
              "  34,\n",
              "  57,\n",
              "  4986,\n",
              "  25,\n",
              "  5,\n",
              "  275,\n",
              "  1860,\n",
              "  7,\n",
              "  432,\n",
              "  19,\n",
              "  42,\n",
              "  696,\n",
              "  479,\n",
              "  50118,\n",
              "  8585,\n",
              "  13364,\n",
              "  2156,\n",
              "  5,\n",
              "  1455,\n",
              "  892,\n",
              "  23483,\n",
              "  5,\n",
              "  3038,\n",
              "  9,\n",
              "  10894,\n",
              "  6530,\n",
              "  11,\n",
              "  41,\n",
              "  8973,\n",
              "  609,\n",
              "  1421,\n",
              "  15,\n",
              "  5,\n",
              "  21087,\n",
              "  9,\n",
              "  223,\n",
              "  4301,\n",
              "  11,\n",
              "  334,\n",
              "  5180,\n",
              "  408,\n",
              "  11,\n",
              "  5,\n",
              "  2129,\n",
              "  443,\n",
              "  9,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2156,\n",
              "  10209,\n",
              "  260,\n",
              "  479,\n",
              "  50118,\n",
              "  9226,\n",
              "  3222,\n",
              "  31370,\n",
              "  892,\n",
              "  34,\n",
              "  57,\n",
              "  2584,\n",
              "  66,\n",
              "  227,\n",
              "  2338,\n",
              "  8,\n",
              "  1824,\n",
              "  11,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2156,\n",
              "  10209,\n",
              "  260,\n",
              "  479,\n",
              "  50118,\n",
              "  9226,\n",
              "  2658,\n",
              "  21,\n",
              "  2033,\n",
              "  30,\n",
              "  5,\n",
              "  557,\n",
              "  1540,\n",
              "  9,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2737,\n",
              "  9,\n",
              "  1131,\n",
              "  17874,\n",
              "  479,\n",
              "  11,\n",
              "  13141,\n",
              "  19,\n",
              "  1265,\n",
              "  1651,\n",
              "  9,\n",
              "  856,\n",
              "  2726,\n",
              "  2791,\n",
              "  50118,\n",
              "  7109,\n",
              "  10891,\n",
              "  1304,\n",
              "  8,\n",
              "  65,\n",
              "  1692,\n",
              "  334,\n",
              "  11,\n",
              "  5,\n",
              "  371,\n",
              "  976,\n",
              "  9,\n",
              "  5,\n",
              "  4879,\n",
              "  443,\n",
              "  9,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  58,\n",
              "  3919,\n",
              "  22422,\n",
              "  479,\n",
              "  11,\n",
              "  167,\n",
              "  1304,\n",
              "  70,\n",
              "  50118,\n",
              "  26302,\n",
              "  4189,\n",
              "  36,\n",
              "  971,\n",
              "  6750,\n",
              "  2156,\n",
              "  262,\n",
              "  111,\n",
              "  508,\n",
              "  107,\n",
              "  793,\n",
              "  4839,\n",
              "  58,\n",
              "  19756,\n",
              "  716,\n",
              "  15,\n",
              "  49,\n",
              "  809,\n",
              "  2862,\n",
              "  1965,\n",
              "  36,\n",
              "  741,\n",
              "  5408,\n",
              "  4839,\n",
              "  30,\n",
              "  10894,\n",
              "  1952,\n",
              "  479,\n",
              "  309,\n",
              "  7,\n",
              "  9742,\n",
              "  5448,\n",
              "  70,\n",
              "  50118,\n",
              "  26302,\n",
              "  4189,\n",
              "  6408,\n",
              "  7,\n",
              "  80,\n",
              "  1134,\n",
              "  716,\n",
              "  15,\n",
              "  49,\n",
              "  776,\n",
              "  1068,\n",
              "  25606,\n",
              "  284,\n",
              "  903,\n",
              "  8,\n",
              "  471,\n",
              "  9,\n",
              "  6028,\n",
              "  128,\n",
              "  29,\n",
              "  633,\n",
              "  8,\n",
              "  10894,\n",
              "  1068,\n",
              "  25606,\n",
              "  5,\n",
              "  78,\n",
              "  333,\n",
              "  58,\n",
              "  2129,\n",
              "  8,\n",
              "  8196,\n",
              "  282,\n",
              "  2126,\n",
              "  6555,\n",
              "  521,\n",
              "  8,\n",
              "  5,\n",
              "  97,\n",
              "  333,\n",
              "  58,\n",
              "  157,\n",
              "  29697,\n",
              "  6555,\n",
              "  50,\n",
              "  157,\n",
              "  111,\n",
              "  160,\n",
              "  521,\n",
              "  479,\n",
              "  50118,\n",
              "  1990,\n",
              "  42,\n",
              "  266,\n",
              "  2156,\n",
              "  5,\n",
              "  408,\n",
              "  128,\n",
              "  29,\n",
              "  6958,\n",
              "  8,\n",
              "  2408,\n",
              "  58,\n",
              "  2867,\n",
              "  88,\n",
              "  1312,\n",
              "  13,\n",
              "  2199,\n",
              "  797,\n",
              "  8,\n",
              "  8555,\n",
              "  36,\n",
              "  740,\n",
              "  34836,\n",
              "  4839,\n",
              "  7,\n",
              "  15756,\n",
              "  741,\n",
              "  5408,\n",
              "  8,\n",
              "  741,\n",
              "  5408,\n",
              "  111,\n",
              "  13,\n",
              "  111,\n",
              "  1046,\n",
              "  992,\n",
              "  111,\n",
              "  4391,\n",
              "  716,\n",
              "  15,\n",
              "  740,\n",
              "  34836,\n",
              "  13,\n",
              "  6357,\n",
              "  797,\n",
              "  8,\n",
              "  8555,\n",
              "  8,\n",
              "  434,\n",
              "  2820,\n",
              "  479,\n",
              "  50118,\n",
              "  627,\n",
              "  11382,\n",
              "  9,\n",
              "  5,\n",
              "  2249,\n",
              "  227,\n",
              "  27311,\n",
              "  21,\n",
              "  9658,\n",
              "  634,\n",
              "  80,\n",
              "  111,\n",
              "  326,\n",
              "  13355,\n",
              "  992,\n",
              "  111,\n",
              "  3457,\n",
              "  13,\n",
              "  2222,\n",
              "  27311,\n",
              "  479,\n",
              "  13,\n",
              "  9704,\n",
              "  5,\n",
              "  15985,\n",
              "  2156,\n",
              "  50118,\n",
              "  627,\n",
              "  8973,\n",
              "  609,\n",
              "  1421,\n",
              "  2408,\n",
              "  21,\n",
              "  7,\n",
              "  5,\n",
              "  14712,\n",
              "  321,\n",
              "  4,\n",
              "  134,\n",
              "  14091,\n",
              "  15,\n",
              "  10,\n",
              "  2394,\n",
              "  3189,\n",
              "  36,\n",
              "  1421,\n",
              "  849,\n",
              "  15636,\n",
              "  102,\n",
              "  3189,\n",
              "  4839,\n",
              "  479,\n",
              "  50118,\n",
              "  8190,\n",
              "  6958,\n",
              "  21,\n",
              "  9550,\n",
              "  7,\n",
              "  5,\n",
              "  14712,\n",
              "  321,\n",
              "  4,\n",
              "  134,\n",
              "  25434,\n",
              "  19,\n",
              "  10,\n",
              "  2204,\n",
              "  111,\n",
              "  13296,\n",
              "  1690,\n",
              "  5416,\n",
              "  12687,\n",
              "  479,\n",
              "  50118,\n",
              "  625,\n",
              "  31375,\n",
              "  5073,\n",
              "  333,\n",
              "  9285,\n",
              "  4832,\n",
              "  42,\n",
              "  1149,\n",
              "  21,\n",
              "  554,\n",
              "  19,\n",
              "  1968,\n",
              "  14074,\n",
              "  1966,\n",
              "  8,\n",
              "  9397,\n",
              "  5,\n",
              "  7193,\n",
              "  479,\n",
              "  50118,\n",
              "  627,\n",
              "  165,\n",
              "  21,\n",
              "  4829,\n",
              "  19,\n",
              "  4844,\n",
              "  9,\n",
              "  70,\n",
              "  7193,\n",
              "  680,\n",
              "  25606,\n",
              "  1265,\n",
              "  1651,\n",
              "  2156,\n",
              "  6642,\n",
              "  1651,\n",
              "  2156,\n",
              "  3193,\n",
              "  13,\n",
              "  474,\n",
              "  9,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2737,\n",
              "  2156,\n",
              "  689,\n",
              "  8,\n",
              "  22411,\n",
              "  1152,\n",
              "  2422,\n",
              "  20946,\n",
              "  558,\n",
              "  8,\n",
              "  484,\n",
              "  786,\n",
              "  111,\n",
              "  22463,\n",
              "  2665,\n",
              "  8,\n",
              "  ...],\n",
              " 'labels': [0,\n",
              "  3618,\n",
              "  4832,\n",
              "  5,\n",
              "  1455,\n",
              "  892,\n",
              "  21,\n",
              "  2584,\n",
              "  66,\n",
              "  7,\n",
              "  7118,\n",
              "  5,\n",
              "  3038,\n",
              "  9,\n",
              "  435,\n",
              "  10894,\n",
              "  6530,\n",
              "  716,\n",
              "  15,\n",
              "  8973,\n",
              "  1548,\n",
              "  15,\n",
              "  29638,\n",
              "  2194,\n",
              "  566,\n",
              "  334,\n",
              "  111,\n",
              "  5180,\n",
              "  408,\n",
              "  11,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2156,\n",
              "  10209,\n",
              "  260,\n",
              "  4,\n",
              "  36739,\n",
              "  29,\n",
              "  8,\n",
              "  6448,\n",
              "  4832,\n",
              "  42,\n",
              "  403,\n",
              "  111,\n",
              "  797,\n",
              "  22813,\n",
              "  6530,\n",
              "  34,\n",
              "  57,\n",
              "  626,\n",
              "  227,\n",
              "  2266,\n",
              "  8,\n",
              "  2338,\n",
              "  15,\n",
              "  971,\n",
              "  6750,\n",
              "  2270,\n",
              "  8,\n",
              "  5929,\n",
              "  334,\n",
              "  2786,\n",
              "  8,\n",
              "  1972,\n",
              "  36,\n",
              "  262,\n",
              "  111,\n",
              "  508,\n",
              "  107,\n",
              "  793,\n",
              "  4839,\n",
              "  716,\n",
              "  15,\n",
              "  8973,\n",
              "  1548,\n",
              "  11,\n",
              "  1481,\n",
              "  853,\n",
              "  1222,\n",
              "  2156,\n",
              "  10209,\n",
              "  260,\n",
              "  479,\n",
              "  1437,\n",
              "  50118,\n",
              "  5,\n",
              "  695,\n",
              "  1286,\n",
              "  30426,\n",
              "  14967,\n",
              "  11,\n",
              "  285,\n",
              "  1304,\n",
              "  81,\n",
              "  10,\n",
              "  132,\n",
              "  12,\n",
              "  180,\n",
              "  675,\n",
              "  552,\n",
              "  19,\n",
              "  8973,\n",
              "  28094,\n",
              "  2163,\n",
              "  11,\n",
              "  645,\n",
              "  7,\n",
              "  5731,\n",
              "  8,\n",
              "  3720,\n",
              "  22813,\n",
              "  6530,\n",
              "  479,\n",
              "  13,\n",
              "  10437,\n",
              "  9,\n",
              "  12833,\n",
              "  9,\n",
              "  5,\n",
              "  6530,\n",
              "  434,\n",
              "  4872,\n",
              "  21383,\n",
              "  9,\n",
              "  1198,\n",
              "  12,\n",
              "  8,\n",
              "  618,\n",
              "  111,\n",
              "  6530,\n",
              "  58,\n",
              "  27697,\n",
              "  1118,\n",
              "  4,\n",
              "  38547,\n",
              "  35,\n",
              "  627,\n",
              "  13135,\n",
              "  9,\n",
              "  9352,\n",
              "  19,\n",
              "  809,\n",
              "  2862,\n",
              "  1965,\n",
              "  795,\n",
              "  87,\n",
              "  195,\n",
              "  207,\n",
              "  8065,\n",
              "  3625,\n",
              "  71,\n",
              "  6530,\n",
              "  566,\n",
              "  1972,\n",
              "  36,\n",
              "  181,\n",
              "  5457,\n",
              "  321,\n",
              "  4,\n",
              "  4197,\n",
              "  4839,\n",
              "  479,\n",
              "  1437,\n",
              "  50118,\n",
              "  959,\n",
              "  2156,\n",
              "  89,\n",
              "  58,\n",
              "  117,\n",
              "  1233,\n",
              "  1022,\n",
              "  566,\n",
              "  2786,\n",
              "  50,\n",
              "  746,\n",
              "  1956,\n",
              "  479,\n",
              "  1437,\n",
              "  50118,\n",
              "  5,\n",
              "  1266,\n",
              "  9,\n",
              "  70,\n",
              "  32117,\n",
              "  22356,\n",
              "  21383,\n",
              "  1714,\n",
              "  3625,\n",
              "  71,\n",
              "  6530,\n",
              "  258,\n",
              "  566,\n",
              "  1972,\n",
              "  8,\n",
              "  2786,\n",
              "  25,\n",
              "  157,\n",
              "  25,\n",
              "  11,\n",
              "  746,\n",
              "  1956,\n",
              "  479,\n",
              "  1437,\n",
              "  50118,\n",
              "  5,\n",
              "  1198,\n",
              "  12,\n",
              "  8,\n",
              "  618,\n",
              "  111,\n",
              "  1296,\n",
              "  1265,\n",
              "  4990,\n",
              "  11,\n",
              "  258,\n",
              "  1134,\n",
              "  969,\n",
              "  14,\n",
              "  5,\n",
              "  1294,\n",
              "  128,\n",
              "  29,\n",
              "  674,\n",
              "  2655,\n",
              "  1471,\n",
              "  34,\n",
              "  57,\n",
              "  3625,\n",
              "  1130,\n",
              "  31,\n",
              "  316,\n",
              "  4,\n",
              "  245,\n",
              "  1437,\n",
              "  155,\n",
              "  4,\n",
              "  176,\n",
              "  7,\n",
              "  545,\n",
              "  4,\n",
              "  398,\n",
              "  1437,\n",
              "  204,\n",
              "  4,\n",
              "  246,\n",
              "  36,\n",
              "  181,\n",
              "  28696,\n",
              "  321,\n",
              "  4,\n",
              "  27623,\n",
              "  322,\n",
              "  3865,\n",
              "  27953,\n",
              "  4832,\n",
              "  42,\n",
              "  892,\n",
              "  13905,\n",
              "  5,\n",
              "  801,\n",
              "  1282,\n",
              "  8,\n",
              "  27949,\n",
              "  4484,\n",
              "  9,\n",
              "  334,\n",
              "  10943,\n",
              "  1767,\n",
              "  11,\n",
              "  10209,\n",
              "  260,\n",
              "  479,\n",
              "  1437,\n",
              "  50118,\n",
              "  435,\n",
              "  10894,\n",
              "  6530,\n",
              "  716,\n",
              "  15,\n",
              "  5,\n",
              "  8973,\n",
              "  609,\n",
              "  1421,\n",
              "  16,\n",
              "  2375,\n",
              "  15,\n",
              "  4881,\n",
              "  5,\n",
              "  21087,\n",
              "  9,\n",
              "  223,\n",
              "  4301,\n",
              "  4010,\n",
              "  566,\n",
              "  2182,\n",
              "  334,\n",
              "  5180,\n",
              "  408,\n",
              "  479,\n",
              "  1437,\n",
              "  2,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I_QUpgAyOJ7"
      },
      "source": [
        "**train_dataset**  \n",
        "attention mask  \n",
        "global_attention_mask  \n",
        "input_ids  \n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abdbZ4eW192b"
      },
      "source": [
        "Finally, the datasets should be converted into the PyTorch format as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC9arodU192b"
      },
      "source": [
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "val_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "HoBH1whVwb7u",
        "outputId": "47f6fbc0-3a8a-42e3-87a2-af8f2ff99f43"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
              " 'global_attention_mask': tensor([1, 0, 0,  ..., 0, 0, 0]),\n",
              " 'input_ids': tensor([  0, 102, 485,  ...,   1,   1,   1]),\n",
              " 'labels': tensor([    0,  3618,  4832,     5,  1455,   892,    21,  2584,    66,     7,\n",
              "          7118,     5,  3038,     9,   435, 10894,  6530,   716,    15,  8973,\n",
              "          1548,    15, 29638,  2194,   566,   334,   111,  5180,   408,    11,\n",
              "          1481,   853,  1222,  2156, 10209,   260,     4, 36739,    29,     8,\n",
              "          6448,  4832,    42,   403,   111,   797, 22813,  6530,    34,    57,\n",
              "           626,   227,  2266,     8,  2338,    15,   971,  6750,  2270,     8,\n",
              "          5929,   334,  2786,     8,  1972,    36,   262,   111,   508,   107,\n",
              "           793,  4839,   716,    15,  8973,  1548,    11,  1481,   853,  1222,\n",
              "          2156, 10209,   260,   479,  1437, 50118,     5,   695,  1286, 30426,\n",
              "         14967,    11,   285,  1304,    81,    10,   132,    12,   180,   675,\n",
              "           552,    19,  8973, 28094,  2163,    11,   645,     7,  5731,     8,\n",
              "          3720, 22813,  6530,   479,    13, 10437,     9, 12833,     9,     5,\n",
              "          6530,   434,  4872, 21383,     9,  1198,    12,     8,   618,   111,\n",
              "          6530,    58, 27697,  1118,     4, 38547,    35,   627, 13135,     9,\n",
              "          9352,    19,   809,  2862,  1965,   795,    87,   195,   207,  8065,\n",
              "          3625,    71,  6530,   566,  1972,    36,   181,  5457,   321,     4,\n",
              "          4197,  4839,   479,  1437, 50118,   959,  2156,    89,    58,   117,\n",
              "          1233,  1022,   566,  2786,    50,   746,  1956,   479,  1437, 50118,\n",
              "             5,  1266,     9,    70, 32117, 22356, 21383,  1714,  3625,    71,\n",
              "          6530,   258,   566,  1972,     8,  2786,    25,   157,    25,    11,\n",
              "           746,  1956,   479,  1437, 50118,     5,  1198,    12,     8,   618,\n",
              "           111,  1296,  1265,  4990,    11,   258,  1134,   969,    14,     5,\n",
              "          1294,   128,    29,   674,  2655,  1471,    34,    57,  3625,  1130,\n",
              "            31,   316,     4,   245,  1437,   155,     4,   176,     7,   545,\n",
              "             4,   398,  1437,   204,     4,   246,    36,   181, 28696,   321,\n",
              "             4, 27623,   322,  3865, 27953,  4832,    42,   892, 13905,     5,\n",
              "           801,  1282,     8, 27949,  4484,     9,   334, 10943,  1767,    11,\n",
              "         10209,   260,   479,  1437, 50118,   435, 10894,  6530,   716,    15,\n",
              "             5,  8973,   609,  1421,    16,  2375,    15,  4881,     5, 21087,\n",
              "             9,   223,  4301,  4010,   566,  2182,   334,  5180,   408,   479,\n",
              "          1437,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100])}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4j2pUM_192c"
      },
      "source": [
        "Alright, we're almost ready to start training. Let's load the model via the `AutoModelForSeq2SeqLM` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6wz7S7b192c"
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpOKd662192c"
      },
      "source": [
        "We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "642d8f4c64474cccbec943413240c027",
            "d2ad50bf2a3b4b77947a29b72ec2bc8c",
            "2dcf5d79460d4554940dd259ccca44a4",
            "1960516a36a3417c8a3c345d2cc101ec",
            "519ced1cfc774ca2a671cfe3903623a8",
            "2074f2221bef4523ba6f7364db39651c",
            "b395732993124de480c2da0b8bb63032",
            "f2e83c9cec9b4236afb6a7227ebf5db9"
          ]
        },
        "id": "vHw7_nMQ192c",
        "outputId": "38090c2b-bfad-4469-cf87-613f75634229"
      },
      "source": [
        "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "642d8f4c64474cccbec943413240c027",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=647693783.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTDTLQV9192c"
      },
      "source": [
        "During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSt8x6mu192c"
      },
      "source": [
        "# set generate hyperparameters\n",
        "led.config.num_beams = 2\n",
        "led.config.max_length = 512\n",
        "led.config.min_length = 100\n",
        "led.config.length_penalty = 2.0\n",
        "led.config.early_stopping = True\n",
        "led.config.no_repeat_ngram_size = 3"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqqcuezU192c"
      },
      "source": [
        "Next, we also have to define the function the will compute the `\"rouge\"` score during evalution.\n",
        "\n",
        "Let's load the `\"rouge\"` metric from 🤗datasets and define the `compute_metrics(...)` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5e605dd890d741a5a89dd1fd04183e9c",
            "b7bfd05045ef495bad3d30e6a2ad3f9b",
            "dea7c32f1e71438d9e30d808be8ff585",
            "8d22ef0d5edb458781eccf7e0037cbcd",
            "20c130370f4249a68c2a73fc4369f527",
            "d94589c558cf4f928bfdbfc1c182c806",
            "f177d781255f4db783b7394229d8f730",
            "fc1e239059a945139662bc1dd6b0b2b5"
          ]
        },
        "id": "B1gU2BXJ192c",
        "outputId": "eade9532-4546-4359-b29f-599916726da5"
      },
      "source": [
        "rouge = load_metric(\"rouge\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e605dd890d741a5a89dd1fd04183e9c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1955.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG0mPNbp192c"
      },
      "source": [
        "The compute metrics function expects the generation output, called `pred.predictions` as well as the gold label, called `pred.label_ids`.\n",
        "\n",
        "Those tokens are decoded and consequently, the rouge score can be computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o3v3O9192c"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77JgIa9o192c"
      },
      "source": [
        "Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfV29f1L192d"
      },
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51TY_eN-192d"
      },
      "source": [
        "In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n",
        "\n",
        "Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9TBR1Fa192d"
      },
      "source": [
        "# # enable fp16 apex training\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     predict_with_generate=True,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     fp16=True,\n",
        "#     output_dir=\"./\",\n",
        "#     logging_steps=5,\n",
        "#     eval_steps=10,\n",
        "#     save_steps=10,\n",
        "#     save_total_limit=2,\n",
        "#     gradient_accumulation_steps=4, ## 줄일 수 있는 parameter인듯. default=4\n",
        "#     num_train_epochs=1,\n",
        "# )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHgxs9s6192d"
      },
      "source": [
        "The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVddmiYv192d"
      },
      "source": [
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=led,\n",
        "#     tokenizer=tokenizer,\n",
        "#     args=training_args,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "# )"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSa9ed3l192d"
      },
      "source": [
        "and we can start training. This will take about ~35min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t5Jkoai2OOY",
        "outputId": "4d3bb483-edb4-46a1-9bbb-f7bae0026a13"
      },
      "source": [
        "# ! pip install apex"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apex\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "Collecting cryptacular\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy\n",
            "  Downloading zope.sqlalchemy-1.6-py2.py3-none-any.whl (22 kB)\n",
            "Collecting velruse>=1.0.3\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 16.9 MB/s \n",
            "\u001b[?25hCollecting pyramid>1.1.2\n",
            "  Downloading pyramid-2.0-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 71.6 MB/s \n",
            "\u001b[?25hCollecting pyramid_mailer\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from apex) (2.23.0)\n",
            "Collecting wtforms\n",
            "  Downloading WTForms-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting wtforms-recaptcha\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting plaster\n",
            "  Downloading plaster-1.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting webob>=1.8.3\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 77.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex) (57.4.0)\n",
            "Collecting hupper>=1.5\n",
            "  Downloading hupper-1.10.3-py2.py3-none-any.whl (26 kB)\n",
            "Collecting plaster-pastedeploy\n",
            "  Downloading plaster_pastedeploy-0.7-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting zope.deprecation>=3.5.0\n",
            "  Downloading zope.deprecation-4.4.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting venusian>=1.0\n",
            "  Downloading venusian-3.0.0-py3-none-any.whl (13 kB)\n",
            "Collecting zope.interface>=3.8.0\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 80.6 MB/s \n",
            "\u001b[?25hCollecting translationstring>=0.4\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex) (1.3.0)\n",
            "Collecting anykeystore\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "Collecting python3-openid\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 67.2 MB/s \n",
            "\u001b[?25hCollecting pbkdf2\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "Collecting PasteDeploy>=2.0\n",
            "  Downloading PasteDeploy-2.1.1-py2.py3-none-any.whl (17 kB)\n",
            "Collecting repoze.sendmail>=4.1\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 46 kB/s \n",
            "\u001b[?25hCollecting transaction\n",
            "  Downloading transaction-3.0.1-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.7/dist-packages (from wtforms->apex) (2.0.1)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9 in /usr/local/lib/python3.7/dist-packages (from zope.sqlalchemy->apex) (1.4.27)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (3.6.0)\n",
            "Building wheels for collected packages: apex, velruse, anykeystore, cryptacular, pbkdf2\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46466 sha256=e0878b4b718bd898bec0514927b97c5129f4c3cb56ddbfe6b63907cbc20c50b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/00/2b/37b6028388b451bbd30230c62f5238aef3b11fdff9503138bc\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50937 sha256=b4a228300e9d47aba4b5ca9fd8abe80425635f85a2cd92b14276ae99b5947e33\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f0/95/7f8b3bb1cce5c78ca7a7922cf72383f886f70e358f0a18d60b\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=17042 sha256=f21d9b9ced3309ec9186ed7da443d69ca7876b8bd9ca93a396d6d4a5ef290e03\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/14/12/afad2dc2b7ea0884e12f260b0723b49b98f39f08adb7b0414f\n",
            "  Building wheel for cryptacular (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp37-cp37m-linux_x86_64.whl size=54544 sha256=bba4d71c896661ac3aab803c573a297611c30c0ea66480a6d6080c82deec6a30\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/e8/c2/4b71f45f434136d31df930960b8a916bb36ebe2144479a37a1\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5103 sha256=06e36f6c060232f952854aced1f5eb1ec861b61068144d2d633b274c30eaef16\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/16/ea/daca297d70ee0782ac6e16e83b2c55b2ca42a2113750bc0489\n",
            "Successfully built apex velruse anykeystore cryptacular pbkdf2\n",
            "Installing collected packages: zope.interface, plaster, PasteDeploy, zope.deprecation, webob, venusian, translationstring, transaction, plaster-pastedeploy, hupper, wtforms, repoze.sendmail, python3-openid, pyramid, pbkdf2, anykeystore, zope.sqlalchemy, wtforms-recaptcha, velruse, pyramid-mailer, cryptacular, apex\n",
            "Successfully installed PasteDeploy-2.1.1 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.6.2 hupper-1.10.3 pbkdf2-1.3 plaster-1.0 plaster-pastedeploy-0.7 pyramid-2.0 pyramid-mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 transaction-3.0.1 translationstring-1.4 velruse-1.1.1 venusian-3.0.0 webob-1.8.7 wtforms-3.0.0 wtforms-recaptcha-0.3.2 zope.deprecation-4.4.0 zope.interface-5.4.0 zope.sqlalchemy-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CigrFIc4192d"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQfzdvO192d"
      },
      "source": [
        "This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n",
        "\n",
        "In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWerWoc3192d"
      },
      "source": [
        "# **Appendix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSUZxqMX192d"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmnWQn0dtz6C",
        "outputId": "8d8b73db-997e-4074-c2aa-23186d5a8beb"
      },
      "source": [
        "# ! pip3 install amp-atomistics"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: amp-atomistics in /usr/local/lib/python3.7/dist-packages (0.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from amp-atomistics) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from amp-atomistics) (1.19.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from amp-atomistics) (4.8.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.7/dist-packages (from amp-atomistics) (3.22.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from amp-atomistics) (22.3.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from ase->amp-atomistics) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->amp-atomistics) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->amp-atomistics) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->amp-atomistics) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->amp-atomistics) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->amp-atomistics) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->amp-atomistics) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfe1ztPu5fZT",
        "outputId": "e1e2325f-c4f3-47ad-883e-205744cc09cd"
      },
      "source": [
        "# # 시간 오래 걸림. 30분 정도 apex나 amp 사용 안하면 설치할 필요 없음\n",
        "\n",
        "# !git clone https://github.com/NVIDIA/apex.git\n",
        "# %cd apex\n",
        "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8639, done.\u001b[K\n",
            "remote: Counting objects: 100% (726/726), done.\u001b[K\n",
            "remote: Compressing objects: 100% (450/450), done.\u001b[K\n",
            "remote: Total 8639 (delta 430), reused 471 (delta 264), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8639/8639), 14.46 MiB | 14.13 MiB/s, done.\n",
            "Resolving deltas: 100% (5829/5829), done.\n",
            "/content/apex\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-a5ffjmhf\n",
            "Created temporary directory: /tmp/pip-req-tracker-gu_ihea4\n",
            "Initialized build tracking at /tmp/pip-req-tracker-gu_ihea4\n",
            "Created build tracker: /tmp/pip-req-tracker-gu_ihea4\n",
            "Entered build tracker: /tmp/pip-req-tracker-gu_ihea4\n",
            "Created temporary directory: /tmp/pip-install-f0h85vuz\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-7l0wu3re\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-gu_ihea4'\n",
            "    Running setup.py (path:/tmp/pip-req-build-7l0wu3re/setup.py) egg_info for package from file:///content/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-mfehxt4y\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.10.0+cu111\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-mfehxt4y/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-7l0wu3re/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-7l0wu3re has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-gu_ihea4'\n",
            "Created temporary directory: /tmp/pip-unpack-sht177m2\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Attempting uninstall: apex\n",
            "    Found existing installation: apex 0.9.10.dev0\n",
            "    Uninstalling apex-0.9.10.dev0:\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex-0.9.10.dev0.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex-0.9.10.dev0.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex/\n",
            "      Successfully uninstalled apex-0.9.10.dev0\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "  Created temporary directory: /tmp/pip-record-uwndeih8\n",
            "    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-7l0wu3re/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-7l0wu3re/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-uwndeih8/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.10.0+cu111\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-7l0wu3re/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "    Cuda compilation tools, release 11.1, V11.1.105\n",
            "    Build cuda_11.1.TC455_06.29190527_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.7\n",
            "    creating build/lib.linux-x86_64-3.7/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.7/apex\n",
            "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.7/apex\n",
            "    creating build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    creating build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    creating build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:381: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.7\n",
            "    creating build/temp.linux-x86_64-3.7/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.7/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/welford.cu -o build/temp.linux-x86_64-3.7/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/syncbn.o build/temp.linux-x86_64-3.7/csrc/welford.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:152:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:174:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:216:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:217:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:242:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:243:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.7/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "                                                               ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/mlp.o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'fused_dense_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.7/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:30:63: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                   ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:33:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:35:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:64:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "                                                                         ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:68:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias = at::empty({out_features}, input.type());\n",
            "                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:70:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:73:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:75:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:106:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:107:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:108:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:111:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:113:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:149:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "                                                                             ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:150:74: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "                                                                              ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:151:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "                                                              ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:152:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias2 = at::empty({out_features}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:153:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:154:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                            ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:157:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:159:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/fused_dense.o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.7/csrc/megatron\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-7l0wu3re/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-7l0wu3re/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-7l0wu3re/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-7l0wu3re/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/normalization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/commons.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/standalone_bert.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/microbatches.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/parallel_state.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/log_util.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/_data/_batchsampler.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/_data/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/amp/grad_scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/amp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/enums.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/output.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/base.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/data.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/db.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/mlp/mlp.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/mlp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/cells.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/models.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck_module_test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fused_dense/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/multiproc.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/distributed.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/LARC.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/handle.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/__version__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_amp_state.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/opt.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/amp.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/wrap.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/rnn_compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/frontend.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_initialize.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/_autocast_utils.py -> /usr/local/lib/python3.7/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/commons.py to commons.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/log_util.py to log_util.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/_data/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/amp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/enums.py to enums.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/output.py to output.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/base.py to base.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/data.py to data.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/db.py to db.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/mlp.py to mlp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/cells.py to cells.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/models.py to models.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/distributed.py to distributed.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/LARC.py to LARC.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/handle.py to handle.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__version__.py to __version__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/scaler.py to scaler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/opt.py to opt.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/amp.py to amp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/wrap.py to wrap.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/compat.py to compat.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/frontend.py to frontend.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py to _initialize.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-37.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-uwndeih8/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-gu_ihea4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO-ro4dXuBcC"
      },
      "source": [
        "# import amp\n",
        "# from apex import amp"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "msB7BC1d192d",
        "outputId": "06060f81-6742-4124-9f4c-d8900a1cf808"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "\n",
        "# load rouge\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "# load pubmed\n",
        "pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n",
        "pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n",
        "\n",
        "# comment out following lines for a test run\n",
        "# pubmed_train = pubmed_train.select(range(32))\n",
        "# pubmed_val = pubmed_val.select(range(32))\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384\")\n",
        "\n",
        "\n",
        "# max encoder length is 8192 for PubMed\n",
        "encoder_max_length = 8192\n",
        "decoder_max_length = 512\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=encoder_max_length,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"abstract\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=decoder_max_length,\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "# map train data\n",
        "pubmed_train = pubmed_train.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")\n",
        "\n",
        "# map val data\n",
        "pubmed_val = pubmed_val.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")\n",
        "\n",
        "# set Python list to PyTorch tensor\n",
        "pubmed_train.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "# set Python list to PyTorch tensor\n",
        "pubmed_val.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "# enable fp16 apex training  ## ## name 'amp' is not defined 문제로 주석처리\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=False, ## default = True\n",
        "    fp16_backend=\"auto\", ## default = apex\n",
        "    output_dir=\"./\",\n",
        "    logging_steps=250,\n",
        "    eval_steps=5000,\n",
        "    save_steps=500,\n",
        "    warmup_steps=1500,\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=4,\n",
        ")\n",
        "\n",
        "\n",
        "# compute Rouge score during validation\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "# load model + enable gradient checkpointing & disable cache for checkpointing\n",
        "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
        "\n",
        "# set generate hyperparameters\n",
        "led.config.num_beams = 4\n",
        "led.config.max_length = 512\n",
        "led.config.min_length = 100\n",
        "led.config.length_penalty = 2.0\n",
        "led.config.early_stopping = True\n",
        "led.config.no_repeat_ngram_size = 3\n",
        "\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=led,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args, ## optional\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=pubmed_train,\n",
        "    eval_dataset=pubmed_val,\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer.train()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc)\n",
            "Reusing dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc/cache-d5e11f198483e4b7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc/cache-8776a54252755b17.arrow\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-0e2e19763af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         )\n\u001b[1;32m   2205\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2052\u001b[0m             )\n\u001b[1;32m   2053\u001b[0m         \u001b[0;31m# If the user passed a tuple for encoder_outputs, we wrap it in a LEDEncoderBaseModelOutput when return_dict=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0;31m# [bsz, seq_len] -> [bsz, seq_len]; 1 -> 0.0; 0 -> \"-inf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1671\u001b[0;31m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# get masking tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36m_expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0minverted_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexpanded_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mexpanded_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverted_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# make sure that global_attn_mask is positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.90 GiB total capacity; 14.46 GiB already allocated; 181.75 MiB free; 14.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loQcxgV-NZzT"
      },
      "source": [
        "![20211202_171420.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWkuw7sX192e"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOHvozyv192e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
        "\n",
        "# load pubmed\n",
        "pubmed_test = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"test\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = LEDTokenizer.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\")\n",
        "model = LEDForConditionalGeneration.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\").to(\"cuda\").half()\n",
        "\n",
        "\n",
        "def generate_answer(batch):\n",
        "  inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=8192, return_tensors=\"pt\", truncation=True)\n",
        "  input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
        "  attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
        "  global_attention_mask = torch.zeros_like(attention_mask)\n",
        "  # put global attention on <s> token\n",
        "  global_attention_mask[:, 0] = 1\n",
        "\n",
        "  predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
        "  batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
        "  return batch\n",
        "\n",
        "\n",
        "result = pubmed_test.map(generate_answer, batched=True, batch_size=1)\n",
        "\n",
        "# load rouge\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p-KtOd1y-Nu"
      },
      "source": [
        "Out Of Memory 문제가 발생해서 GCP 상에서 다시 돌려봐야겠다."
      ]
    }
  ]
}