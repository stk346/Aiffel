{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f980b26b",
   "metadata": {
    "id": "6Hgw3GTXLLw0"
   },
   "source": [
    "## 🤗 Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32de77",
   "metadata": {
    "id": "W7-QHmRiAMB9"
   },
   "source": [
    "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
    "\n",
    "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
    "\n",
    "We will leverage 🤗`Seq2SeqTrainer`, gradient checkpointing and as usual 🤗`datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd59a64",
   "metadata": {
    "id": "0B19PhgrCHM1"
   },
   "source": [
    "First, let's try to get a GPU with at least 15GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661029",
   "metadata": {
    "id": "6z18zG2N192Z"
   },
   "source": [
    "To check that we are having enough RAM we can run the following command.\n",
    "If the randomely allocated GPU is too small, the above cells can be run \n",
    "to crash the notebook hoping to get a better GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd0b19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32163c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ded7d85",
   "metadata": {
    "id": "P59lSzY4192Z"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install datasets==1.2.1\n",
    "# !pip install transformers==4.2.0\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a533c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface datasets load\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_train_df/\")\n",
    "val_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_val_df/\")\n",
    "test_dataset = load_from_disk(\"/home/aiffelsummabot/LED/HF_test_df/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1583374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': ' from outstanding performance in 2011 12, burberry began the year cautiously optimistic , our long-range objectives ensuring clarity of the luxury brand message , enabling sustainable growth and being a great company firmly in sight . angela ahrendts chief executive officer this combination of optimism and determination , fuelled by the brand’s wealth of opportunity , suggested continued pursuit of the investment-oriented strategic agenda in the year ahead . at the same time , this pre-disposition was tempered by uncertainties in the macro environment and the goal to deliver near-term financial performance . in the final analysis , the result was a balance of dynamic management , core execution and strategic investment . challenging context following standout growth in 2011 relative to the range of consumer sectors , luxury slowed dramatically in 2012. the ongoing economic crisis in the eurozone and a continued sluggish us weighed on all areas of consumer spending . although most of asia remained relatively healthy , the chinese consumer which accounts for a majority of luxury consumption growth was subdued by a secularly decelerating economy complicated by government transition . industry experts estimate that luxury sector growth declined from 13 percent in 2011 to 5 percent in 2012. within that , ready-to-wear brands and businesses were disproportionately affected . for burberry , this climate manifested itself in a decline in store traffic and greater weekly sales volatility . dynamic management internally , we talk about managing the business dynamically and focusing on the things we can control . 2012 13’s external environment tested the team on these dimensions . supported by investments in information systems and business intelligence expertise during the past few years which enable monitoring , analysis and implementation we set out to do exactly that . heightened conversion the traffic decline placed greater emphasis on converting consumers entering the stores to customers . this effort included service initiatives that maximised time on the selling floor of our most skilled associates , increased inventory availability and improved selling skills and product knowledge . in terms of product , we expedited fashion assortments targeted at core luxury customers and refined monthly floorset execution to enhance the flow of fresh merchandise . 8 chief executive officer’s letter striking the balance targeted marketing aided by deeper consumer insight , we retargeted marketing activities in keeping with changing consumer spending patterns . the team increased the brand’s presence in high-profile outdoor and travel-oriented locations and experimented with new digital venues some of which were contracted on a real-time basis . for festive periods , a cross-functional group refined programmes across product , visual and advertising to better highlight specific gift-giving opportunities . tactical efficiency the group also acted to enhance near-term efficiency . discretionary expenses were tightly controlled and inventory was closely managed at all stages of the process in keeping with softer sales . core execution articulated by the five strategic themes , burberry’s core strategy has been consistently executed over the past seven years , and actions to navigate immediate conditions did not sway us from this course . blurring the physical and digital given a world of increasingly ubiquitous mobile internet access , we expect dissolution of the boundaries separating physical and digital channels . consumers will see a single , continuous space in which to interact with a brand . through a range of activities , we are working to integrate the benefits of the physical and digital spheres . ultimately , the vision is to serve completely any consumer on any platform in any geography . in this regard , the opening of the london flagship at 121 regent street in september is our most ambitious effort to date . housed in a period building restored in partnership with traditional british craftsmen , the store expresses burberry .com in tangible space complete product assortment , rfid technology to trigger targeted multimedia content , omnipresent digital screens continuously projecting brand imagery . burberry world live , a store of the future , blending heritage and innovation , online with offline . additional integration initiatives included the expanded use of ipads to enhance inventory availability , continued upgrading of retail theatre throughout the store base to ensure synchronised delivery of brand content to consumers , and experimentation with new payment systems to streamline the purchase experience . enhancing the product proposition at the heart of the burberry brand , product was a key area of strategic activity . elevation of the product offering is an ongoing process . in the year , we exited selected opening price points in heritage rainwear and leather goods categories . exacting a cost in terms of sales , this is consistent with the brand’s positioning within the current luxury context . similarly responding to consumer demand , we continued to invest in the upper tiers of our product pyramid , the prorsum and london labels . these labels increased their share of retail sales during the year . although founded as a mens brand , burberry is underpenetrated in mens . during the year , outerwear benefited from greater emphasis on innovation and design . tailoring developed with broader assortments and expanded distribution . closer attention to in-store timing of seasonal merchandise enhanced relevance of the offering . and the first fully dedicated burberry mens store was opened in london in october 2012. achieving a 13 percent revenue increase , mens was the fastest growing product division in the year . with mens integration only two years old , burberry is in the early stages of capitalising on this heritage . october saw the launch of the britain watch for women and men . the britain , featuring an advanced swiss-made mechanical movement and more sophisticated design , is an important step in realigning burberry’s watch business with the brand’s luxury positioning . engaging the chinese consumer globally given their importance , efforts to better understand and serve chinese consumers are an ongoing priority . during the year , burberry conducted proprietary research , leveraging the results across functions to extend product sizing and fit , to train sales associates in high-travel markets , to formulate occasion-specific marketing campaigns citing a few examples . in the year , greater china represented burberry’s fastest growing major market and this consumer was prominent throughout the retail network . 9 chief executive officer’s letter developing growth markets outside asia , the group continued to develop other growth markets . in the directly operated markets of india , latin america and the middle east , the group opened net six mainline stores during the year . while some offer below average profitability today , we believe these markets represent important components of future growth . in regions operated through franchise partners , including turkey , russia and eastern europe , eight stores were opened with expansion to five new markets , including georgia and jordan . new franchise agreements were signed for colombia and chile . expanding the retail presence in addition to london , the year included flagship store openings in chicago , hong kong and milan . while contributing to sales , these brand statements in gateway cities present the complete burberry to diverse groups of relevant consumers , many of them new to the brand . in total , the group opened net 14 mainline stores , six concessions and five outlets during the year , and completed seven major renovations . average selling space increased 13 percent . refining the wholesale presence efforts to align the quality of the brand’s wholesale presence with that of retail are ongoing . in both the americas and europe , we continued to concentrate on luxury-oriented department and specialty stores with emphasis on dedicated real estate while exiting legacy doors inconsistent with the brand’s positioning . this activity , in combination with the channel’s exposure to soft geographies , resulted in 1 percent underlying wholesale revenue growth for the year . as part of the brand proposition , burberry looks to be a leader in consumers’ digital interaction with brands , in both innovation and capability . in marketing , the s s13 campaign generated record awareness through social media . total burberry youtube video views reached over ten million during the year . rfid-enabled personalised content was introduced with the a w13 runway show . experimenting with emerging digital platforms , burberry streamed live images of london weather to prominent outdoor sites in london , paris , hong kong , los angeles and new york during the olympic period . in commerce , burberry .com added spanish and korean languages and tested new fulfilment options . strategic investment while executing currently , the team invested in strategic initiatives with longer-term horizons . integrating beauty among the most exciting strategic investments in recent years , the transition of burberry’s fragrance and make-up business from a licensed to a directly operated business began in the year . offering luxury’s opening price point and broadest distribution , fragrance is the most widely encountered expression of the burberry brand . the category also accounts for a large percentage of global brand media spend . as a result , direct operation will assist in optimising brand presence in every market , further enable burberry to capitalise on the synergistic relationship with fashion , and better align the product offering with brand architecture . integration elevates this business to true core activity , allowing burberry to capture the full opportunity . in terms of opportunity , despite burberry’s position among the largest luxury apparel and accessories brands globally , it is undersized in fragrance . growth has been slow , with fragrance significantly underperforming the rest of the group over the past five years . in make-up , the brand has only just started . from the decision to integrate in october , the team moved quickly across functions , leveraging existing skills and resources while adding external category-specific talent and capability . as of 1 april , beauty had been successfully integrated and commenced operating . burberry’s fifth product division , beauty is a growth platform of the future . evolving customer dialogue in a landscape of big data and continuous communication , we believe information-intensive , deep customer relationships which allow an individualised customer dialogue will be critical to future success in luxury . as part of this , the group began developing new tools to provide an integrated view of a customer’s interaction with the brand across all burberry platforms , with initial piloting of a clienteling application commencing at year end . development work to enhance consumers’ ability to engage the brand through mobile devices also progressed in the year . 10 chief executive officer’s letter transitioning the japan legacy in japan , transition from the legacy licensed business to global integration continued . as part of this , burberry’s early stage retail operation achieved strong growth at existing stores and concessions , opened a concession , added a third store and planned additional openings in 2013 14 while the effect of licence terminations continued to reduce legacy royalty income . reinforcing the supply chain to accommodate future growth objectives , the group reinforced the supply chain . in logistics , burberry added distribution capacity , upgraded existing facilities and increased network efficiency . in sourcing , additional resources were committed to further develop in-house outerwear manufacturing capability and improve raw material management . strong financial results this balance of activity delivered record financial results in 2012 13 while positioning burberry well for years ahead . total revenue increased 8 percent underlying to pound 2bn . retail revenue grew 12 percent driven by new space and a 5 percent comparable store gain . soft european markets particularly weighed on wholesale , resulting in a 1 percent underlying revenue increase . the 1 percent underlying decline in licensing revenue was a product of double-digit growth among global licences more than offset by declining legacy japan royalties . adjusted operating profit increased 14 percent to pound 428m , with the core retail wholesale segment increasing 17 percent on 8 percent revenue growth retail wholesale operating margin also reached a record 17.8 percent . capital expenditure totalled pound 176m and the group ended the year with pound 297m in net cash . powerful culture burberry’s culture is a key ingredient to this success . rooted in the brand’s core values and fuelled by a creative-thinking , entrepreneurial spirit , our connected , united culture creates an energy that enables innovation , coordination and agility . these characteristics are evident in the year’s accomplishments . this distinctive culture is also expressed externally through ethical trade and sustainability efforts , employee engagement with local communities and the burberry foundation , which contributes both human and capital resources to encourage youth to realise their dreams through the power of creativity . we continued to invest in this powerful culture throughout the organisation communication initiatives , operating structures , reward programmes and celebrations . the bigger the business becomes , the more connected we will need to be . a great community through the efforts of this great team we largely realised our objectives . so i thank them , as well as burberry’s extended community of franchise and licensing partners , customers and suppliers , for their passion , commitment and hard work during the year . looking forward , this team provides confidence for markets favourable or not . kpi growth in adjusted diluted eps year to 31 march is a key valuation metric for burberry’s shareholders . 70.0p 14 percent 2013 2012 2011 2010 2009 70.0 14 percent 61.6 26 percent 48.9 39 percent 35.1 16 percent 30.2 -4 percent adjusted diluted eps is stated before exceptional items . reported diluted eps 57.0p 2012 59.3p . 11 chief executive officer’s letter ',\n",
       " 'abstract': 'from outstanding performance in 2011 12, burberry began the year cautiously optimistic , our long-range objectives ensuring clarity of the luxury brand message , enabling sustainable growth and being a great company firmly in sight .angela ahrendts chief executive officer this combination of optimism and determination , fuelled by the brand’s wealth of opportunity , suggested continued pursuit of the investment-oriented strategic agenda in the year ahead .at the same time , this pre-disposition was tempered by uncertainties in the macro environment and the goal to deliver near-term financial performance .in the final analysis , the result was a balance of dynamic management , core execution and strategic investment .challenging context following standout growth in 2011 relative to the range of consumer sectors , luxury slowed dramatically in 2012. the ongoing economic crisis in the eurozone and a continued sluggish us weighed on all areas of consumer spending .although most of asia remained relatively healthy , the chinese consumer which accounts for a majority of luxury consumption growth was subdued by a secularly decelerating economy complicated by government transition .industry experts estimate that luxury sector growth declined from 13 percent in 2011 to 5 percent in 2012. within that , ready-to-wear brands and businesses were disproportionately affected .additional integration initiatives included the expanded use of ipads to enhance inventory availability , continued upgrading of retail theatre throughout the store base to ensure synchronised delivery of brand content to consumers , and experimentation with new payment systems to streamline the purchase experience .and the first fully dedicated burberry mens store was opened in london in october 2012. achieving a 13 percent revenue increase , mens was the fastest growing product division in the year .strategic investment while executing currently , the team invested in strategic initiatives with longer-term horizons .soft european markets particularly weighed on wholesale , resulting in a 1 percent underlying revenue increase .capital expenditure totalled pound 176m and the group ended the year with pound 297m in net cash .',\n",
       " '__index_level_0__': 1592}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fe607ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2445\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19146d6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMQWD3wL192Z",
    "outputId": "80096c0e-b26b-488f-ac55-d194e2579d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 12 01:30:11 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    32W / 250W |   1703MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      2240      C   ...abot/anaconda3/envs/summabot/bin/python  1693MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2e40",
   "metadata": {
    "id": "APGpqABk192Z"
   },
   "source": [
    "Next, we install 🤗Transformers, 🤗Datasets, and `rouge_score`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfb3f8",
   "metadata": {
    "id": "p3Me6n0o192Z"
   },
   "source": [
    "Let's start by loading and preprocessing the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b551ed",
   "metadata": {
    "id": "CzjKRBib192Z"
   },
   "source": [
    "Next, we download the pubmed train and validation dataset ([click to see on 🤗Datasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **☕** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5b249",
   "metadata": {
    "id": "Uyn-TDeB192a"
   },
   "source": [
    "It's always a good idea to take a look at some data samples. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "febc7268",
   "metadata": {
    "id": "5QkyLfIy192a"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=4):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4813ec",
   "metadata": {
    "id": "wx3iiUgy192a"
   },
   "source": [
    "We can see that the input data is the `article` - a scientific report and the target data is the `abstract` - a concise summary of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bcd4a",
   "metadata": {
    "id": "xs7zKqy-192a"
   },
   "source": [
    "Cool! Having downloaded the dataset, let's tokenize it.\n",
    "We'll import the convenient `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d0e1ef7",
   "metadata": {
    "id": "k7wczXZP192a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a2d36",
   "metadata": {
    "id": "Ifg19ED9192a"
   },
   "source": [
    "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a786b30",
   "metadata": {
    "id": "w_s6Z_ni192a"
   },
   "source": [
    "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.${}^1$. \n",
    "\n",
    "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
    "\n",
    "In this notebook, we are only able to train on `batch_size=2` to prevent out-of-memory errors.\n",
    "\n",
    "---\n",
    "${}^1$ The data is taken from page 11 of [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62abf2ee",
   "metadata": {
    "id": "jk7tS_xN192b"
   },
   "outputs": [],
   "source": [
    "max_input_length = 4096\n",
    "max_output_length = 512\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fd76",
   "metadata": {
    "id": "2493L-dt192b"
   },
   "source": [
    "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
    "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
    "\n",
    "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70b28121",
   "metadata": {
    "id": "2_UzG6Ek192b"
   },
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8da054",
   "metadata": {
    "id": "q-V7N0L-192b"
   },
   "source": [
    "For the sake of this notebook, we will reduce the training and validation data \n",
    "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b172c6",
   "metadata": {
    "id": "OeJ7dOpi192b"
   },
   "source": [
    "Great, having defined the mapping function, let's preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3971ec4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a7fe6f9ae61b4cfbadaba2c0bfc50e1a",
      "f3b8ce93ce314a4d80b6e7b9f935600c",
      "58609f6a301741c788d9e190e49601bc",
      "727b33b4d0ba48d3be3b76040bfb0ead",
      "ea44f4c51d334e3aa3ed03acbb795242",
      "591785faa90742dfb12a3436c2dee0e2",
      "8156f721d27246b5a5357fac577b7d82",
      "415c082418fd4c3b94408e00bbcda82a"
     ]
    },
    "id": "wcaN0IJD192b",
    "outputId": "952499e3-265a-4694-ec89-7f848d19f2a6"
   },
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da93a5",
   "metadata": {
    "id": "8gunSLWj192b"
   },
   "source": [
    "and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6f650bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "aa16af7563554384be6e5513c6afe79a",
      "3eb2a25aab6141bfa1cf5a76bed6c5b2",
      "be015dcb6cef4b41911e69fbc5af1a59",
      "a62d68c2e4914f0ba08ede851cf47280",
      "f2c219f0099d411caf4ff5e831673875",
      "0e6d376e4a394a2aa025e2edc3898ec8",
      "628aa2531c944e4e8efeecf6fca40993",
      "78482e48010a412385fbe008d146d728"
     ]
    },
    "id": "FkrEujTX192b",
    "outputId": "56576f0b-29df-4fde-84e5-c82f81de6b81"
   },
   "outputs": [],
   "source": [
    "# val_dataset = val_dataset.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd6638",
   "metadata": {
    "id": "2I_QUpgAyOJ7"
   },
   "source": [
    "**train_dataset**  \n",
    "attention mask  \n",
    "global_attention_mask  \n",
    "input_ids  \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e3997",
   "metadata": {
    "id": "abdbZ4eW192b"
   },
   "source": [
    "Finally, the datasets should be converted into the PyTorch format as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a15a2e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2445\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cd6b651",
   "metadata": {
    "id": "fC9arodU192b"
   },
   "outputs": [],
   "source": [
    "# train_dataset.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )\n",
    "# val_dataset.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b245fe",
   "metadata": {
    "id": "A4j2pUM_192c"
   },
   "source": [
    "Alright, we're almost ready to start training. Let's load the model via the `AutoModelForSeq2SeqLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c263a05",
   "metadata": {
    "id": "V6wz7S7b192c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e567e",
   "metadata": {
    "id": "EpOKd662192c"
   },
   "source": [
    "We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09e5d609",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "642d8f4c64474cccbec943413240c027",
      "d2ad50bf2a3b4b77947a29b72ec2bc8c",
      "2dcf5d79460d4554940dd259ccca44a4",
      "1960516a36a3417c8a3c345d2cc101ec",
      "519ced1cfc774ca2a671cfe3903623a8",
      "2074f2221bef4523ba6f7364db39651c",
      "b395732993124de480c2da0b8bb63032",
      "f2e83c9cec9b4236afb6a7227ebf5db9"
     ]
    },
    "id": "vHw7_nMQ192c",
    "outputId": "38090c2b-bfad-4469-cf87-613f75634229"
   },
   "outputs": [],
   "source": [
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6df40",
   "metadata": {
    "id": "sTDTLQV9192c"
   },
   "source": [
    "During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d0a0009",
   "metadata": {
    "id": "vSt8x6mu192c"
   },
   "outputs": [],
   "source": [
    "# set generate hyperparameters\n",
    "led.config.num_beams = 2\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d21319",
   "metadata": {
    "id": "gqqcuezU192c"
   },
   "source": [
    "Next, we also have to define the function the will compute the `\"rouge\"` score during evalution.\n",
    "\n",
    "Let's load the `\"rouge\"` metric from 🤗datasets and define the `compute_metrics(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ac749d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5e605dd890d741a5a89dd1fd04183e9c",
      "b7bfd05045ef495bad3d30e6a2ad3f9b",
      "dea7c32f1e71438d9e30d808be8ff585",
      "8d22ef0d5edb458781eccf7e0037cbcd",
      "20c130370f4249a68c2a73fc4369f527",
      "d94589c558cf4f928bfdbfc1c182c806",
      "f177d781255f4db783b7394229d8f730",
      "fc1e239059a945139662bc1dd6b0b2b5"
     ]
    },
    "id": "B1gU2BXJ192c",
    "outputId": "eade9532-4546-4359-b29f-599916726da5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1aa0a",
   "metadata": {
    "id": "TG0mPNbp192c"
   },
   "source": [
    "The compute metrics function expects the generation output, called `pred.predictions` as well as the gold label, called `pred.label_ids`.\n",
    "\n",
    "Those tokens are decoded and consequently, the rouge score can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3bf8c935",
   "metadata": {
    "id": "z9o3v3O9192c"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ee710",
   "metadata": {
    "id": "77JgIa9o192c"
   },
   "source": [
    "Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b403e79e",
   "metadata": {
    "id": "tfV29f1L192d"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca01ff",
   "metadata": {
    "id": "51TY_eN-192d"
   },
   "source": [
    "In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n",
    "\n",
    "Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8236db2f",
   "metadata": {
    "id": "s9TBR1Fa192d"
   },
   "outputs": [],
   "source": [
    "# # enable fp16 apex training\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     fp16=True,\n",
    "#     output_dir=\"./\",\n",
    "#     logging_steps=5,\n",
    "#     eval_steps=10,\n",
    "#     save_steps=10,\n",
    "#     save_total_limit=2,\n",
    "#     gradient_accumulation_steps=4, ## 줄일 수 있는 parameter인듯. default=4\n",
    "#     num_train_epochs=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1f466",
   "metadata": {
    "id": "nHgxs9s6192d"
   },
   "source": [
    "The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5aba0e45",
   "metadata": {
    "id": "BVddmiYv192d"
   },
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=led,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6f5e0",
   "metadata": {
    "id": "PSa9ed3l192d"
   },
   "source": [
    "and we can start training. This will take about ~35min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03c42e21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t5Jkoai2OOY",
    "outputId": "4d3bb483-edb4-46a1-9bbb-f7bae0026a13"
   },
   "outputs": [],
   "source": [
    "# ! pip install apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a156a574",
   "metadata": {
    "id": "CigrFIc4192d"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbb384",
   "metadata": {
    "id": "omQfzdvO192d"
   },
   "source": [
    "This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n",
    "\n",
    "In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9c5b8",
   "metadata": {
    "id": "WSUZxqMX192d"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "863eaf02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmnWQn0dtz6C",
    "outputId": "8d8b73db-997e-4074-c2aa-23186d5a8beb"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install amp-atomistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df9efa68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfe1ztPu5fZT",
    "outputId": "e1e2325f-c4f3-47ad-883e-205744cc09cd"
   },
   "outputs": [],
   "source": [
    "# # 시간 오래 걸림. 30분 정도 apex나 amp 사용 안하면 설치할 필요 없음\n",
    "\n",
    "# !git clone https://github.com/NVIDIA/apex.git\n",
    "# %cd apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2230e4df",
   "metadata": {
    "id": "oO-ro4dXuBcC"
   },
   "outputs": [],
   "source": [
    "# import apex\n",
    "# from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a1a6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2643f706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2871ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bff5f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb969c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('학습을 진행하는 기기:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36ecf870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "print(torch.__version__) # torch version 출력\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efbab5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be5d18da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "msB7BC1d192d",
    "outputId": "06060f81-6742-4124-9f4c-d8900a1cf808",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# from datasets import load_dataset, load_metric\n",
    "# from transformers import (\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "# )\n",
    "\n",
    "# # load rouge\n",
    "# rouge = load_metric(\"rouge\")\n",
    "\n",
    "# # load pubmed\n",
    "# # pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n",
    "# # pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n",
    "# pubmed_train = train_dataset\n",
    "# pubmed_val = val_dataset\n",
    "\n",
    "# # comment out following lines for a test run\n",
    "# # pubmed_train = pubmed_train.select(range(32))\n",
    "# # pubmed_val = pubmed_val.select(range(32))\n",
    "\n",
    "# # load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "\n",
    "# # max encoder length is 8192 for PubMed\n",
    "# encoder_max_length = 4096\n",
    "# decoder_max_length = 512\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# def process_data_to_model_inputs(batch):\n",
    "#     # tokenize the inputs and labels\n",
    "#     inputs = tokenizer(\n",
    "#         batch[\"article\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=encoder_max_length,\n",
    "#     )\n",
    "#     outputs = tokenizer(\n",
    "#         batch[\"abstract\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=decoder_max_length,\n",
    "#     )\n",
    "\n",
    "#     batch[\"input_ids\"] = inputs.input_ids\n",
    "#     batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "#     # create 0 global_attention_mask lists\n",
    "#     batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "#         [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "#     ]\n",
    "\n",
    "#     # since above lists are references, the following line changes the 0 index for all samples\n",
    "#     batch[\"global_attention_mask\"][0][0] = 1\n",
    "#     batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "#     # We have to make sure that the PAD token is ignored\n",
    "#     batch[\"labels\"] = [\n",
    "#         [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "#         for labels in batch[\"labels\"]\n",
    "#     ]\n",
    "\n",
    "#     return batch\n",
    "\n",
    "\n",
    "# # # map train data\n",
    "# # pubmed_train = pubmed_train.map(\n",
    "# #     process_data_to_model_inputs,\n",
    "# #     batched=True,\n",
    "# #     batch_size=batch_size,\n",
    "# #     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# # )\n",
    "\n",
    "# # # map val data\n",
    "# # pubmed_val = pubmed_val.map(\n",
    "# #     process_data_to_model_inputs,\n",
    "# #     batched=True,\n",
    "# #     batch_size=batch_size,\n",
    "# #     remove_columns=[\"article\", \"abstract\", \"__index_level_0__\"],\n",
    "# # )\n",
    "\n",
    "# # # set Python list to PyTorch tensor\n",
    "# # pubmed_train.set_format(\n",
    "# #     type=\"torch\",\n",
    "# #     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# # )\n",
    "\n",
    "# # # set Python list to PyTorch tensor\n",
    "# # pubmed_val.set_format(\n",
    "# #     type=\"torch\",\n",
    "# #     columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    "# # )\n",
    "\n",
    "# # enable fp16 apex training  ## ## name 'amp' is not defined 문제로 주석처리\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     fp16=False, ## default = True\n",
    "#     fp16_backend=\"auto\", ## default = apex\n",
    "#     output_dir=\"./LED_test\",\n",
    "#     logging_steps=250,\n",
    "#     eval_steps=5000,\n",
    "#     save_steps=500,\n",
    "#     warmup_steps=1500,\n",
    "#     save_total_limit=2,\n",
    "#     gradient_accumulation_steps=4,\n",
    "# )\n",
    "\n",
    "\n",
    "# # compute Rouge score during validation\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(\n",
    "#         predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "#     )[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }\n",
    "\n",
    "\n",
    "# # load model + enable gradient checkpointing & disable cache for checkpointing\n",
    "# led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# # set generate hyperparameters\n",
    "# led.config.num_beams = 4\n",
    "# led.config.max_length = 512\n",
    "# led.config.min_length = 100\n",
    "# led.config.length_penalty = 2.0\n",
    "# led.config.early_stopping = True\n",
    "# led.config.no_repeat_ngram_size = 3\n",
    "\n",
    "\n",
    "# # instantiate trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=led,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args, ## optional\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=pubmed_train,\n",
    "#     eval_dataset=pubmed_val,\n",
    "# )\n",
    "\n",
    "# # start training\n",
    "# with torch.autograd.set_detect_anomaly(True): ## RuntimeError 때문에 추가\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a057d4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84381dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['__index_level_0__', 'abstract', 'article'],\n",
       "    num_rows: 2445\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e618c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = test_dataset.map(\n",
    "#     process_data_to_model_inputs,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     remove_columns=[\"article\", \"abstract\" ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "507c1950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abstract', 'article'],\n",
       "    num_rows: 328\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "286ddc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a84155",
   "metadata": {},
   "source": [
    "main이 실행되는데 main은 다시 main_worker 들을 multi-processing으로 실행합니다. GPU 4개를 하나의 노드로 보고 world_size를 설정합니다. 그러면 mp.spawn 함수가 2개의 GPU에서 따로 따로 main_worker를 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99b07ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffelsummabot/anaconda3/envs/summabot/lib/python3.7/site-packages/transformers/generation_utils.py:1632: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c5e1d7370149fdb7506932b1c324c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=82.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: Score(precision=0.48448750906971416, recall=0.3208196580430346, fmeasure=0.3733898039246645)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "# load pubmed\n",
    "pubmed_test = test_dataset\n",
    "\n",
    "# load tokenizer\n",
    "model_path = \"/home/aiffelsummabot/LED/checkpoint-500/\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_path)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_path).to(\"cuda\").half()\n",
    "\n",
    "\n",
    "def generate_answer(batch):\n",
    "    inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=4096, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    # put global attention on <s> token\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "    batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "\n",
    "result = pubmed_test.map(generate_answer, batched=True, batch_size=4)\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3134a980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abstract', 'article', 'predicted_abstract'],\n",
       "    num_rows: 328\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4cfee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predicted_result = pd.DataFrame({'abstract': result['abstract'],\n",
    "                                                    'article': result['article'],\n",
    "                                                    'predicted_abstract': result['predicted_abstract']})\n",
    "\n",
    "predicted_result.to_csv('./predicted_document.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
